{"meta":{"title":"Binge.van","subtitle":null,"description":"点滴积累","author":"范斌","url":"http://github.com/Bingevan","root":"/Bingevan/"},"pages":[{"title":"标签","date":"2019-03-12T04:06:44.000Z","updated":"2019-03-12T04:11:05.079Z","comments":true,"path":"tags/index.html","permalink":"http://github.com/Bingevan/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2019-03-12T04:12:00.000Z","updated":"2019-03-12T04:13:17.141Z","comments":true,"path":"categories/index.html","permalink":"http://github.com/Bingevan/categories/index.html","excerpt":"","text":""},{"title":"Me","date":"2019-03-15T10:50:36.000Z","updated":"2019-03-15T13:25:26.514Z","comments":true,"path":"about/index.html","permalink":"http://github.com/Bingevan/about/index.html","excerpt":"","text":"水滴石穿，积少成多！"}],"posts":[{"title":"","slug":"A机器学习总结/集成学习","date":"2019-03-17T08:32:23.123Z","updated":"2019-03-17T08:32:23.123Z","comments":true,"path":"2019/03/17/A机器学习总结/集成学习/","link":"","permalink":"http://github.com/Bingevan/2019/03/17/A机器学习总结/集成学习/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"A机器学习总结/数据清洗","date":"2019-03-17T08:12:40.000Z","updated":"2019-03-17T08:31:16.156Z","comments":true,"path":"2019/03/17/A机器学习总结/数据清洗/","link":"","permalink":"http://github.com/Bingevan/2019/03/17/A机器学习总结/数据清洗/","excerpt":"","text":"数据清洗预处理工具 关系型数据库或者Python 元数据与数据的特征查看 对数据有直观的了解，位置后的数据处理做准备 缺省值清洗 确定缺省值的范围 去除不需要的字段 填充缺省值内容（重要 ） 重新获取数据","categories":[{"name":"面试","slug":"面试","permalink":"http://github.com/Bingevan/categories/面试/"},{"name":"机器学习","slug":"面试/机器学习","permalink":"http://github.com/Bingevan/categories/面试/机器学习/"}],"tags":[]},{"title":"快速排序","slug":"E数据结构与算法/新建文本文档 - 副本 (3)","date":"2019-03-17T02:00:51.000Z","updated":"2019-03-18T12:24:56.602Z","comments":true,"path":"2019/03/17/E数据结构与算法/新建文本文档 - 副本 (3)/","link":"","permalink":"http://github.com/Bingevan/2019/03/17/E数据结构与算法/新建文本文档 - 副本 (3)/","excerpt":"","text":"","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://github.com/Bingevan/categories/数据结构/"}],"tags":[]},{"title":"改进快速排序","slug":"E数据结构与算法/01快速排序","date":"2019-03-17T02:00:51.000Z","updated":"2019-03-18T12:26:56.541Z","comments":true,"path":"2019/03/17/E数据结构与算法/01快速排序/","link":"","permalink":"http://github.com/Bingevan/2019/03/17/E数据结构与算法/01快速排序/","excerpt":"","text":"##partition算法 （荷兰国旗问题）给定一个数组arr，和一个数num，请把小于num的数放在数组的左边，等于num的数放在数组的中间，大于num的数放在数组的右边。要求额外空间复杂度O(1)，时间复杂度O(N) 思想： 大于num的数放在数组的左边，小于num的数放在数组的右边，设当前指针为cur，小于区域的最后一个数指针为less，大于区域的第一个数为more。 解题： 要比较的数x=num，cur+1； 要比较的数x&lt;num,x与小于区域的下一个数交换，并且cur+1、less+1 要比较的数x&gt;num,x与大于区域的前一个数交换，并且more+1 代码： 123456789101112131415161718192021public static int[] pattena(int[] arr,int L,int R,int num)&#123; int less = L-1; int more = R+1; int cur = L; while (cur&lt;more)&#123; if(arr[cur]&lt;num)&#123; awap(arr,++less,cur++); &#125;else if(arr[cur]&gt;num)&#123; awap(arr,cur,--more); &#125;else &#123; cur++; &#125; &#125; //return new int[]&#123;less+1,more-1&#125;; return arr; &#125;public static void awap(int[] arr,int i ,int j)&#123; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp;&#125; 基于partition改进后的快速排序 随机快速排序的细节和复杂度分析可以用荷兰国旗问题来改进快速排序时间复杂度O(N*logN)，额外空间复杂度O(logN) 解题： 将数组的最后一个数最为num 小于num的放在左边，—&gt; 递归 大于num的放在右边，—&gt; 递归 代码实现： 12345678910111213141516171819202122232425262728293031public static int[] quickSort(int arr[],int L, int R) &#123; if(L&lt;R) &#123; int num = arr[arr.length - 1]; int[] idnex = partition(arr, L, R,num); quickSort(arr, L, idnex[0]-1); quickSort(arr, idnex[1]+1, R); &#125; return arr; &#125;public static int[] partition(int[] arr,int L,int R,int num)&#123; int less = L-1; int more = R; int cur = L; while (cur &lt; more) &#123; if (arr[cur]&lt;num) &#123; exchange(arr,++less,cur++); &#125; else if (arr[cur]&gt;num) &#123; exchange(arr,--more, cur); &#125;else &#123; cur++; &#125; &#125; exchange(arr, more, R); return new int[]&#123;less + 1, more &#125;;&#125;public static void exchange(int[] arr,int i, int j) &#123; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp;&#125;","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://github.com/Bingevan/categories/数据结构/"}],"tags":[]},{"title":"堆排序","slug":"E数据结构与算法/02堆排序","date":"2019-03-17T02:00:51.000Z","updated":"2019-03-18T13:06:44.620Z","comments":true,"path":"2019/03/17/E数据结构与算法/02堆排序/","link":"","permalink":"http://github.com/Bingevan/2019/03/17/E数据结构与算法/02堆排序/","excerpt":"","text":"","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://github.com/Bingevan/categories/数据结构/"}],"tags":[]},{"title":"快速排序","slug":"E数据结构与算法/新建文本文档 - 副本 (2)","date":"2019-03-17T02:00:51.000Z","updated":"2019-03-18T12:24:56.602Z","comments":true,"path":"2019/03/17/E数据结构与算法/新建文本文档 - 副本 (2)/","link":"","permalink":"http://github.com/Bingevan/2019/03/17/E数据结构与算法/新建文本文档 - 副本 (2)/","excerpt":"","text":"","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://github.com/Bingevan/categories/数据结构/"}],"tags":[]},{"title":"快速排序","slug":"Fcaffe/新建文本文档 - 副本 (4)","date":"2019-03-17T02:00:51.000Z","updated":"2019-03-26T07:53:18.934Z","comments":true,"path":"2019/03/17/Fcaffe/新建文本文档 - 副本 (4)/","link":"","permalink":"http://github.com/Bingevan/2019/03/17/Fcaffe/新建文本文档 - 副本 (4)/","excerpt":"","text":"","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://github.com/Bingevan/categories/数据结构/"}],"tags":[]},{"title":"快速排序","slug":"E数据结构与算法/新建文本文档 - 副本 (6)","date":"2019-03-17T02:00:51.000Z","updated":"2019-03-18T12:24:56.602Z","comments":true,"path":"2019/03/17/E数据结构与算法/新建文本文档 - 副本 (6)/","link":"","permalink":"http://github.com/Bingevan/2019/03/17/E数据结构与算法/新建文本文档 - 副本 (6)/","excerpt":"","text":"","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://github.com/Bingevan/categories/数据结构/"}],"tags":[]},{"title":"深度学习基础","slug":"B深度学习总结/深度学习","date":"2019-03-17T02:00:51.000Z","updated":"2019-03-18T11:43:23.621Z","comments":true,"path":"2019/03/17/B深度学习总结/深度学习/","link":"","permalink":"http://github.com/Bingevan/2019/03/17/B深度学习总结/深度学习/","excerpt":"","text":"过拟合与欠拟合 欠拟合：指模型不能在训练数据集上获得足够低的训练误差 过拟合：指模型的训练误差与测试误差（泛化误差）上的误差过大 反映在评价指标上就是模型在训练集上表现良好，但是在测试集或者在新数据集上的表现一般（泛化能力差） 降低过拟合风险的方法 所有为了减小测试误差的策略统称为正则化方法，这些方法可能会议增大训练误差为代价。 数据增强 图像：平移、旋转、缩放 利用生成对抗网络生成新数据 NLP：利用机器翻译生成新数据 降低模型复杂度 神经网络：减少网络层、神经元个数 决策树：降低树的深度、剪枝 权值约束（添加正则化项） L1、L2正则化 集成学习 神经网络：Dropout 决策树：随机森林、GDBT 提前终止 降低欠拟合风险的方法 加入新的特征 交叉特征、多项式特征 升读学习：因子分解机、Deep-Crossing 增加模型复杂度 线性模型：添加高次项 神经网络：增加网络层数、神经元个数 减小正则化项的系数 添加正则化项是为了限制模型的学习能力，减小正则化项可以放宽这个限制 模型通常更加倾向于更大的权重，更大的权重可以使模型更好的拟合 反向传播算法反向传播的作用、目的、本质 概述：梯度下降法中需要利用损失函数对所有参数的梯度来寻找局部最小点，而反向传播算法就是用于计算梯度的算法，其本质就是利用链式求导法则对每个参数求偏导。 公式推导https://blog.csdn.net/lien0906/article/details/79193103 激活函数激活函数的作用——为什么使用非线性激活函数使用非线性激活函数的目的是为了像网络中加入非线性因素；增强网络的表示能力，解决线性网络存在的问题。 为什么加入非线性网络可以增强网络的表达能力呢——神经网络的万能近似定理 神经网络的万能近似定理认为震惊网络具有至少一个非线性隐含层，那么只要给网络足够数量的隐藏单元，他就可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的函数。 如果不使用非线性激活函数，那么每一层输出都是上一层输入的线性组合；此时无论网络有多少层，其整体也将是线性的，这会导致失去万能近似的性质。 但是部分层是纯线性是可以接受的，有助于减少网络的参数。 常见的激活函数整流线性单元ReLU ReLU通常是激活函数最好的默认选择 =\\max(0,z)) ReLU的拓展 ReLU及其扩展都是基于以下公式： 绝对值整流（absolute value rectification） 当a=-1，此时函数就是g(z)=|z| 渗漏整流线性单元 当a是一个很小的值，如a=0.001 参数化线性整流 将a作为一个可学习的参数 maxout单元 maxout单元进一步扩展了ReLU，他是一个可学习的K段函数，参数数量是普通全连接层的 k 倍 。 ####sigmoid与tanh sigmoid(z)，常记作 σ(z)； tanh(z) 的图像与 sigmoid(z) 大致相同，区别是值域为 (-1, 1) =\\frac{1}{1+\\exp(-z)}) 其他激活函数ReLU（优势）与sigmoid比较1.避免梯度消失 sigmoid函数在输入取绝对值非常大的正值或者负值的时候会出现饱和现象——在图像上表现的很平，此时函数会对输入的微小变化不敏感——从而造成梯度消失 ReLU的导数始终是一个常数——负半区为0，正半区为1，——所以不会发生梯度消失 2.减缓过拟合 ReLU在负半区输出为0，一旦神经元的激活值进入负半轴，那么该机或值就不会产生梯度/不会被训练，造成了网络的稀疏性——稀疏激活。 有利于减少参数的相互依赖，缓解过拟合问题的发生。 3.加速计算 ReLU的求导不涉及浮点运算 为什么 ReLU 不是全程可微/可导也能用于基于梯度的学习？ 在实现过程中通常返回左导数或者右导数的其中一个。 正则化Batch Normalizaton(批标准化) BN是一种正则化方法（较少泛化误差），主要的作用有： 加速网络的训练（缓解梯度消失，支持更大的学习率） 防止过拟合 降低参数初始化的要求 动机： 训练的本质就是学习数据的分布。如果训练数据与测试数据分布不同会降低模型的泛化能力，因此应该在训练前对所有输入数据做归一化处理。 在神经网络中，因为每个隐层的参数不同，会使下一层的输入发生变化，从而导致每一批数据的分布也发生变化，从而导致网络在每次迭代中需要拟合不同的数据分布，增加了网络的训练难度与过拟合的风险。 基本原理 BN方法会正对每一批数据，在网络的每一层输入之前增加归一化处理，是输入的均值为0，标准差为1，目的是将数据限制在统一的分布下。 具体就是，针对每一层的K个神经元，计算这一批数据在低K个神经元的均值与标准差，然后将归一化的值作为该神经元的激活函值。 BN可以看作在各层之间加入了一个新的计算层，对数据进行额外的约束，从而增加模型的泛化能力 但是BN也降低了模型的拟合能力，破坏了之前学习到的特征分布。 为了恢复原式的数据，BN引入了一个重构变化来还原最优的输入数据的分布 其中 γ 和 β 为可训练参数。 小结： 以上过程可归纳为一个 BN(x) 函数： ) 其中 &amp;=\\gamma\\boldsymbol{\\hat{x}_i}+\\beta\\&space;&amp;=\\gamma\\frac{\\boldsymbol{x_i}-\\boldsymbol{\\mathrm{E}[x_i]}}{\\sqrt{\\boldsymbol{\\mathrm{Var}[x_i]}+\\epsilon}}+\\beta&space;\\end{aligned}) 完整算法： L1/L2 范数正则化L1/L2范数的作用、异同相同点：限制了模型的学习能力——通过限制模型参数的规模，使模型偏好于权值较小的目标函数，防止过拟合。 不同点： L1正则化可以产生稀疏的权值举证，可以用于特征选择，同时一定程度上防止过拟合；L2正则化用于防止模型过拟合 L1正则化适用于特征之间有关联的情况；L2正则化适用于特征之间没有关联的情况。 为什么L1和L2正则化可以防止过拟合 L1、L2正则化回事模型偏好于更小的权重 更小的权重意味着更低的模型复杂度，添加L1和L2正则化相当于为模型添加了某种先验，限制参数的分布，从而降低了模型的复杂度。 模型的复杂度低意味着模型对于噪声与异常点的抗干扰能力增强，从而提高了模型的泛化能力——直观来说就是对训练数据的拟合刚刚号，不会过分拟合数据——奥卡姆剃刀定理 为什么L1正则化可以产生稀疏的权值，L2不会？ 对于目标函数添加范数正则化，训练时相当于在范数的约束下求目标函数J的最小值 带有L1 范数（左）和L2 范数（右）约束的二维图示 图中 J 与 L1 首次相交的点即是最优解。L1 在和每个坐标轴相交的地方都会有“顶点”出现，多维的情况下，这些顶点会更多；在顶点的位置就会产生稀疏的解。而 J 与这些“顶点”相交的机会远大于其他点，因此 L1 正则化会产生稀疏的解。 L2 不会产生“顶点”，因此 J 与 L2 相交的点具有稀疏性的概率就会变得非常小。 DropoutBagging集成学习 集成方法的主要想法是分别训练不同的模型，然后让所有的模型表决最终的输出。 集成学习凑效的原因是不同模型通常不会在测试集上产生相同的误差。 集成模型能至少与它的任何一成员 表现的一样好。如果成员的误差是独立的，集成将显著提升模型的性能。 Bagging是一种集成学习的策略——具体来说，Bagging涉及构造K个不同的数据集 每个数据集从原始数据集中重复采样构成，和原始数据集具有相同的样例，——这就意味着每个数据集有大概率缺少来自原始数据集的例子，但是也包含若干重复的例子 更具体的就是，如果采样所得到的训练集与原式的数据集大小相同，那么所得到的数据集中大概有原始数据集2/3的实例。 集成算法与神经网络 神经网络能够找到足够多的不同的解，意味着他们可以从模型平均中受益——即使所有的模型都在同一数据集上训练。 神经网络中随机初始化的差异、批训练数据的随机选择、超参数的差异等非确定性实现往往足以使得继承中的不同成员有部分独立的误差。 Dropout策略 简单来说，Dropout是通过共享参数提供了一种廉价的Bagging集成近似——Drpout相当于继承了从基础网络去除部分单元后形成的子网络。 通常隐层的采样率为0.5,输入的采样率为0.8，超参数也可以这样，但其采样率一般为1 权重比例推断规则 权重比例推断规则的目的是确保在测试时一个单元的期望总输入与在训练时该单元的期望总输入大致相同。 实践时，如果使用 0.5 的采样概率，权重比例规则相当于在训练结束后将权重乘 0.5，然后像平常一样使用模型；等价的，另一种方法是在训练时将单元的状态乘 2。 Dropout 与 Bagging 的不同 在 Bagging 的情况下，所有模型都是独立的；而在 Dropout 的情况下，所有模型共享参数，其中每个模型继承父神经网络参数的不同子集。 在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。","categories":[{"name":"面试","slug":"面试","permalink":"http://github.com/Bingevan/categories/面试/"}],"tags":[]},{"title":"快速排序","slug":"E数据结构与算法/新建文本文档 - 副本 (7)","date":"2019-03-17T02:00:51.000Z","updated":"2019-03-18T12:24:56.602Z","comments":true,"path":"2019/03/17/E数据结构与算法/新建文本文档 - 副本 (7)/","link":"","permalink":"http://github.com/Bingevan/2019/03/17/E数据结构与算法/新建文本文档 - 副本 (7)/","excerpt":"","text":"","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://github.com/Bingevan/categories/数据结构/"}],"tags":[]},{"title":"快速排序","slug":"E数据结构与算法/新建文本文档 - 副本 (8)","date":"2019-03-17T02:00:51.000Z","updated":"2019-03-18T12:24:56.602Z","comments":true,"path":"2019/03/17/E数据结构与算法/新建文本文档 - 副本 (8)/","link":"","permalink":"http://github.com/Bingevan/2019/03/17/E数据结构与算法/新建文本文档 - 副本 (8)/","excerpt":"","text":"","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://github.com/Bingevan/categories/数据结构/"}],"tags":[]},{"title":"快速排序","slug":"E数据结构与算法/新建文本文档 - 副本","date":"2019-03-17T02:00:51.000Z","updated":"2019-03-18T12:24:56.602Z","comments":true,"path":"2019/03/17/E数据结构与算法/新建文本文档 - 副本/","link":"","permalink":"http://github.com/Bingevan/2019/03/17/E数据结构与算法/新建文本文档 - 副本/","excerpt":"","text":"","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://github.com/Bingevan/categories/数据结构/"}],"tags":[]},{"title":"快速排序","slug":"E数据结构与算法/新建文本文档 - 副本 (5)","date":"2019-03-17T02:00:51.000Z","updated":"2019-03-18T12:24:56.602Z","comments":true,"path":"2019/03/17/E数据结构与算法/新建文本文档 - 副本 (5)/","link":"","permalink":"http://github.com/Bingevan/2019/03/17/E数据结构与算法/新建文本文档 - 副本 (5)/","excerpt":"","text":"","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://github.com/Bingevan/categories/数据结构/"}],"tags":[]},{"title":"Faster-Rcnn原理","slug":"D计算机视觉/Faste-Rcnn","date":"2019-03-16T06:10:18.000Z","updated":"2019-03-17T08:39:03.821Z","comments":true,"path":"2019/03/16/D计算机视觉/Faste-Rcnn/","link":"","permalink":"http://github.com/Bingevan/2019/03/16/D计算机视觉/Faste-Rcnn/","excerpt":"","text":"","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://github.com/Bingevan/categories/计算机视觉/"}],"tags":[]},{"title":"CBAM","slug":"C论文/CBAM网络","date":"2019-03-16T06:10:18.000Z","updated":"2019-03-17T07:51:05.506Z","comments":true,"path":"2019/03/16/C论文/CBAM网络/","link":"","permalink":"http://github.com/Bingevan/2019/03/16/C论文/CBAM网络/","excerpt":"摘要 CBAM特点就是轻量级通用模块 可以很好的加入到CNN结构中 端到端可训练的CNNs","text":"摘要 CBAM特点就是轻量级通用模块 可以很好的加入到CNN结构中 端到端可训练的CNNs 作者贡献 提出了一个高效的attention模块—-CBAM，该模块能够嵌入到目前的主流CNN网络结构中。 通过额外的分离实验证明了CBAM中attention的有效性。 在多个平台上（ImageNet-1K，MS COCO和VOC 2007）上证明了CBAM的性能提升。 Convolutional Block Attention Module给定输入特征为 对于一个中间层的feature map：，CBAM将会顺序推理出1维的channel attention map 以及2维的spatial attention map ，整个过程如下所示： %20%5Cotimes%20F) %20%5Cotimes%20F%5E%7B%27%7D) 其中为element-wise multiplication，首先将channel attention map与输入的feature map相乘得到，之后计算的spatial attention map，并将两者相乘得到最终的输出。下图为CBAM的示意图： Channel attention moduleeature map 的每个channel都被视为一个feature detector，channel attention主要关注于输入图片中什么(what)是有意义的。为了高效地计算channel attention，论文使用最大池化和平均池化对feature map在空间维度上进行压缩，得到两个不同的空间背景描述：和。使用由MLP组成的共享网络对这两个不同的空间背景描述进行计算得到channel attention map：。计算过程如下： 其中，，后使用了Relu作为激活函数。 Spatial attention module. 与channel attention不同，spatial attention主要关注于位置信息(where)。为了计算spatial attention，论文首先在channel的维度上使用最大池化和平均池化得到两个不同的特征描述和，然后使用concatenation将两个特征描述合并，并使用卷积操作生成spatial attention map %20%5Cin%20%5Cmathbb%20R_%7BH*W%7D)。计算过程如下： 其中，表示7*7的卷积层 下图为channel attention和spatial attention的示意图： 部分代码： 12345678910111213141516171819202122232425262728293031323334class ChannelAttention(nn.Module): def __init__(self, in_planes, ratio=16): super(ChannelAttention, self).__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) self.max_pool = nn.AdaptiveMaxPool2d(1) self.fc1 = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False) self.relu1 = nn.ReLU() self.fc2 = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False) self.sigmoid = nn.Sigmoid() def forward(self, x): avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x)))) max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x)))) out = avg_out + max_out return self.sigmoid(out)class SpatialAttention(nn.Module): def __init__(self, kernel_size=7): super(SpatialAttention, self).__init__() assert kernel_size in (3, 7), 'kernel size must be 3 or 7' padding = 3 if kernel_size == 7 else 1 self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False) self.sigmoid = nn.Sigmoid() def forward(self, x): avg_out = torch.mean(x, dim=1, keepdim=True) max_out, _ = torch.max(x, dim=1, keepdim=True) x = torch.cat([avg_out, max_out], dim=1) x = self.conv1(x) return self.sigmoid(x)","categories":[{"name":"每日论文","slug":"每日论文","permalink":"http://github.com/Bingevan/categories/每日论文/"}],"tags":[]},{"title":"hexo基本使用","slug":"hexo基本使用","date":"2019-03-14T11:26:33.000Z","updated":"2019-03-14T14:31:34.424Z","comments":true,"path":"2019/03/14/hexo基本使用/","link":"","permalink":"http://github.com/Bingevan/2019/03/14/hexo基本使用/","excerpt":"","text":"创建新的文章1hexo n 文章的名字 服务器的开启123hexo clearhexo ghexo s 服务器的关闭1ctrl + c 新建分类页与标签页 分类页 1hexo n page categories 标签页 1hexo n page tags 修改完章连接的样式 打开文件：..\\themes\\source\\css_common\\components\\post\\post.styl 12345678//输入代码.post-body p a &#123; color: #345; border-bottom: none; &amp;:hover&#123; color: red; &#125;&#125; 修改文章底部带#号的标签如： 打开文件 ：..\\themes\\next\\layout_macro\\post.swig 修改代码 123456789&lt;footer class=&quot;post-footer&quot;&gt; &#123;% if post.tags and post.tags.length and not is_index %&#125; &lt;div class=&quot;post-tags&quot;&gt; &#123;% for tag in post.tags %&#125; &lt;a href=&quot;&#123;&#123; url_for(tag.path) &#125;&#125;&quot; rel=&quot;tag&quot;&gt;&lt;i class=&quot;fa fa-google&quot;&gt;&lt;/i&gt; &#123;&#123; tag.name &#125;&#125;&lt;/a&gt; &#123;% endfor %&#125; &lt;/div&gt; &#123;% endif %&#125; ... 修改后的样式： 添加Valine评论功能增加搜索功能 打开主题、站点的配置文件 打开官网，找到第三方服务集成，找到搜索服务选择搜索样式： http://theme-next.iissnan.com/third-party-services.html#local-search 在git输入npm install hexo-generator-searchdb --save 编辑主题配置文件 123# Local searchlocal_search: enable: true 添加站点配置文件 12345search: path: search.xml field: post format: html limit: 10000 效果如图： 添加不蒜子统计功能 打开主题的配置文件 打开官网，找到第三方服务集成，找到统计服务经行修改 http://theme-next.iissnan.com/third-party-services.html#analytics-busuanzi 增加分享功能 打开主题的配置文件 找到baidushare修改代码 123baidushare: type: button baidushare: true 隐藏底部的强力驱动 注释掉代码段 1234567891011121314151617&lt;!--&#123;% if theme.footer.powered.enable %&#125; &lt;div class=\"powered-by\"&gt;&#123;# #&#125;&#123;&#123; __('footer.powered', next_url('https://hexo.io', 'Hexo', &#123;class: 'theme-link'&#125;)) &#125;&#125;&#123;# #&#125;&#123;% if theme.footer.powered.version %&#125; v&#123;&#123; hexo_env('version') &#125;&#125;&#123;% endif %&#125;&#123;# #&#125;&lt;/div&gt;&#123;% endif %&#125;&#123;% if theme.footer.powered.enable and theme.footer.theme.enable %&#125; &lt;span class=\"post-meta-divider\"&gt;|&lt;/span&gt;&#123;% endif %&#125;&#123;% if theme.footer.theme.enable %&#125; &lt;div class=\"theme-info\"&gt;&#123;# #&#125;&#123;&#123; __('footer.theme') &#125;&#125; – &#123;&#123; next_url('https://theme-next.org', 'NexT.' + theme.scheme, &#123;class: 'theme-link'&#125;) &#125;&#125;&#123;# #&#125;&#123;% if theme.footer.theme.version %&#125; v&#123;&#123; version &#125;&#125;&#123;% endif %&#125;&#123;##&#125;&lt;/div&gt;&#123;% endif %&#125;--&gt; 实现字数统计与阅读时长功能","categories":[{"name":"工具","slug":"工具","permalink":"http://github.com/Bingevan/categories/工具/"}],"tags":[{"name":"导航","slug":"导航","permalink":"http://github.com/Bingevan/tags/导航/"},{"name":"分享","slug":"分享","permalink":"http://github.com/Bingevan/tags/分享/"}]}]}