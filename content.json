{"meta":{"title":"一个专注DL的小白","subtitle":null,"description":"专注=完美","author":"范斌","url":"https://dreamercv.github.io/dreamercv.github.io","root":"/dreamercv.github.io/"},"pages":[{"title":"分类","date":"2019-03-12T04:12:00.000Z","updated":"2019-03-12T04:13:17.141Z","comments":true,"path":"categories/index.html","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-03-12T04:06:44.000Z","updated":"2019-03-12T04:11:05.079Z","comments":true,"path":"tags/index.html","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/index.html","excerpt":"","text":""},{"title":"Me","date":"2019-03-15T10:50:36.000Z","updated":"2019-03-15T13:25:26.514Z","comments":true,"path":"about/index.html","permalink":"https://dreamercv.github.io/dreamercv.github.io/about/index.html","excerpt":"","text":"水滴石穿，积少成多！"}],"posts":[{"title":"DL中的Top","slug":"DL中的Top","date":"2019-05-18T04:43:32.000Z","updated":"2019-05-18T04:46:48.263Z","comments":true,"path":"2019/05/18/DL中的Top/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/18/DL中的Top/","excerpt":"","text":"top1—–就是你预测的label取最后概率向量里面最大的那一个作为预测结果，如过你的预测结果中概率最大的那个分类正确，则预测正确。否则预测错误 top5—–就是最后概率向量最大的前五名中，只要出现了正确概率即为预测正确。否则预测错误。","categories":[{"name":"DL基础","slug":"DL基础","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/DL基础/"}],"tags":[]},{"title":"Markdown点滴","slug":"markdown点滴","date":"2019-05-18T04:19:15.000Z","updated":"2019-05-18T04:26:32.906Z","comments":true,"path":"2019/05/18/markdown点滴/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/18/markdown点滴/","excerpt":"搭建博客时，需要设置页面布局等功能，一些Markdown的隐藏语法与hexo功能介绍。","text":"搭建博客时，需要设置页面布局等功能，一些Markdown的隐藏语法与hexo功能介绍。 导入图片 导入图片设置图片大小与位置 1&lt;img src=\"http://prjlqbksp.bkt.clouddn.com/blog/20190517/xLPq64JApYSM.png?imageslim\" width=\"400\" hegiht=\"30\" align=center/&gt; 显示设置 博客显示部分内容 123&lt;meta name=\"referrer\" content=\"no-referrer\" /&gt; &lt;!--more--&gt; 文字居中1&lt;center&gt;图1：原始论文中的模型&lt;/center&gt;","categories":[{"name":"工具","slug":"工具","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/工具/"}],"tags":[{"name":"博客","slug":"博客","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/博客/"}]},{"title":"__call__方法","slug":"call-方法","date":"2019-05-18T04:08:50.000Z","updated":"2019-05-18T04:15:23.714Z","comments":true,"path":"2019/05/18/call-方法/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/18/call-方法/","excerpt":"在Python的class中有一些函数往往具有特殊的意义。__init__()和__call__()就是class很有用的两类特殊的函数。","text":"在Python的class中有一些函数往往具有特殊的意义。__init__()和__call__()就是class很有用的两类特殊的函数。 __init__()在Python中，__init__()函数的意义等同于类的构造器（同理，__del__()等同于类的析构函数）。因此，__init__()方法的作用是创建一个类的实例。 __call__()Python中的函数是一级对象。这意味着Python中的函数的引用可以作为输入传递到其他的函数/方法中，并在其中被执行。而Python中类的实例（对象）可以被当做函数对待。也就是说，我们可以将它们作为输入传递到其他的函数/方法中并调用他们，正如我们调用一个正常的函数那样。而类中__call__()函数的意义正在于此。为了将一个类实例当做函数调用，我们需要在类中实现__call__()方法。也就是我们要在类中实现如下方法：def __call__(self, *args)。这个方法接受一定数量的变量作为输入。假设x是X类的一个实例。那么调用x.__call__(1,2)等同于调用x(1,2)。这个实例本身在这里相当于一个函数。 总结那么，__init__()和__call__()的区别如下： __init__()的作用是初始化某个类的一个实例。 __call__()的作用是使实例能够像函数一样被调用，同时不影响实例本身的生命周期（__call__()不影响一个实例的构造和析构）。但是__call__()可以用来改变实例的内部成员的值。即，若函数中有__call__()方法可以直接将实例当作函数调用。模糊了实例与对象的关系。 12345678910111213class X(object): def __init__(self, a, b, range): self.a = a self.b = b self.range = range def __call__(self, a, b): self.a = a self.b = b print('__call__ with （&#123;&#125;, &#123;&#125;）'.format(self.a, self.b)) def __del__(self, a, b, range): del self.a del self.b del self.range 123xInstance = X(1, 2, 3)xInstance(1,2)#两种效果一致#__call__ with (1, 2)","categories":[{"name":"广东工业-物体检测","slug":"广东工业-物体检测","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/"},{"name":"函数分析","slug":"广东工业-物体检测/函数分析","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/函数分析/"}],"tags":[{"name":"python","slug":"python","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/python/"}]},{"title":"transforms的二十二个方法","slug":"transforms的二十二个方法","date":"2019-05-18T02:34:02.000Z","updated":"2019-05-18T02:53:50.461Z","comments":true,"path":"2019/05/18/transforms的二十二个方法/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/18/transforms的二十二个方法/","excerpt":"预处理方法 ：裁剪—Crop、翻转和旋转—Flip and Rotation、图像变换、对transforms操作，使数据增强更灵活","text":"预处理方法 ：裁剪—Crop、翻转和旋转—Flip and Rotation、图像变换、对transforms操作，使数据增强更灵活 裁剪——Crop中心裁剪：transforms.CenterCrop随机裁剪：transforms.RandomCrop随机长宽比裁剪：transforms.RandomResizedCrop上下左右中心裁剪：transforms.FiveCrop上下左右中心裁剪后翻转：transforms.TenCrop 翻转和旋转——Flip and Rotation依概率p水平翻转：transforms.RandomHorizontalFlip(p=0.5)依概率p垂直翻转：transforms.RandomVerticalFlip(p=0.5)随机旋转：transforms.RandomRotation 图像变换resize：transforms.Resize标准化：transforms.Normalize转为tensor，并归一化至[0-1]：transforms.ToTensor填充：transforms.Pad修改亮度、对比度和饱和度：transforms.ColorJitter转灰度图：transforms.Grayscale线性变换：transforms.LinearTransformation()仿射变换：transforms.RandomAffine依概率p转为灰度图：transforms.RandomGrayscale将数据转换为PILImage：transforms.ToPILImagetransforms.Lambda：Apply a user-defined lambda as a transform. 对transforms操作，使数据增强更灵活transforms.RandomChoice(transforms)， 从给定的一系列transforms中选一个进行操作transforms.RandomApply(transforms, p=0.5)，给一个transform加上概率，依概率进行操作transforms.RandomOrder，将transforms中的操作随机打乱 裁剪—Crop随机裁剪：transforms.RandomCropclass torchvision.transforms.RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode=‘constant’)功能：依据给定的size随机裁剪参数：size- (sequence or int)，若为sequence,则为(h,w)，若为int，则(size,size)padding-(sequence or int, optional)，此参数是设置填充多少个pixel。 当为int时，图像上下左右均填充int个，例如padding=4，则上下左右均填充4个pixel，若为32*32，则会变成40*40。 当为sequence时，若有2个数，则第一个数表示左右扩充多少，第二个数表示上下的。当有4个数时，则为左，上，右，下。 fill- (int or tuple) 填充的值是什么（仅当填充模式为constant时有用）。int时，各通道均填充该值，当长度为3的tuple时，表示RGB通道需要填充的值。padding_mode- 填充模式，这里提供了4种填充模式，1.constant，常量。2.edge 按照图片边缘的像素值来填充。3.reflect，暂不了解。 4. symmetric，暂不了解。 中心裁剪：transforms.CenterCropclass torchvision.transforms.CenterCrop(size)功能：依据给定的size从中心裁剪参数：size- (sequence or int)，若为sequence,则为(h,w)，若为int，则(size,size) 随机长宽比裁剪 transforms.RandomResizedCropclass torchvision.transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)功能：随机大小，随机长宽比裁剪原始图片，最后将图片resize到设定好的size参数：size- 输出的分辨率scale- 随机crop的大小区间，如scale=(0.08, 1.0)，表示随机crop出来的图片会在的0.08倍至1倍之间。ratio- 随机长宽比设置interpolation- 插值的方法，默认为双线性插值(PIL.Image.BILINEAR) 上下左右中心裁剪：transforms.FiveCropclass torchvision.transforms.FiveCrop(size)功能：对图片进行上下左右以及中心裁剪，获得5张图片，返回一个4D-tensor参数：size- (sequence or int)，若为sequence,则为(h,w)，若为int，则(size,size) 上下左右中心裁剪后翻转: transforms.TenCropclass torchvision.transforms.TenCrop(size, vertical_flip=False)功能：对图片进行上下左右以及中心裁剪，然后全部翻转（水平或者垂直），获得10张图片，返回一个4D-tensor。参数：size- (sequence or int)，若为sequence,则为(h,w)，若为int，则(size,size)vertical_flip (bool) - 是否垂直翻转，默认为flase，即默认为水平翻转 翻转和旋转——Flip and Rotation依概率p水平翻转transforms.RandomHorizontalFlipclass torchvision.transforms.RandomHorizontalFlip(p=0.5)功能：依据概率p对PIL图片进行水平翻转参数：p- 概率，默认值为0.5 依概率p垂直翻转transforms.RandomVerticalFlipclass torchvision.transforms.RandomVerticalFlip(p=0.5)功能：依据概率p对PIL图片进行垂直翻转参数：p- 概率，默认值为0.5 随机旋转：transforms.RandomRotationclass torchvision.transforms.RandomRotation(degrees, resample=False, expand=False, center=None)功能：依degrees随机旋转一定角度参数：degress- (sequence or float or int)，若为单个数，如 30，则表示在（-30，+30）之间随机旋转若为sequence，如(30，60)，则表示在30-60度之间随机旋转resample- 重采样方法选择，可选 PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC，默认为最近邻expand- ?center- 可选为中心旋转还是左上角旋转 图像变换resize：transforms.Resizeclass torchvision.transforms.Resize(size, interpolation=2)功能：重置图像分辨率参数：size- If size is an int, if height &gt; width, then image will be rescaled to (size height / width, size)，所以建议size设定为hwinterpolation- 插值方法选择，默认为PIL.Image.BILINEAR 标准化：transforms.Normalizeclass torchvision.transforms.Normalize(mean, std)功能：对数据按通道进行标准化，即先减均值，再除以标准差，注意是h*w*c 转为tensor：transforms.ToTensorclass torchvision.transforms.ToTensor功能：将PIL Image或者 ndarray 转换为tensor，并且归一化至[0-1]注意事项：归一化至[0-1]是直接除以255，若自己的ndarray数据尺度有变化，则需要自行修改。 填充：transforms.Padclass torchvision.transforms.Pad(padding, fill=0, padding_mode=‘constant’)功能：对图像进行填充参数：padding-(sequence or int, optional)，此参数是设置填充多少个pixel。 当为int时，图像上下左右均填充int个，例如padding=4，则上下左右均填充4个pixel，若为32*32，则会变成40*40。 当为sequence时，若有2个数，则第一个数表示左右扩充多少，第二个数表示上下的。当有4个数时，则为左，上，右，下。 fill- (int or tuple)填充的值是什么（仅当填充模式为constant时有用）。int时，各通道均填充该值，当长度为3的tuple时，表示RGB通道需要填充的值。padding_mode- 填充模式，这里提供了4种填充模式，1.constant，常量。2.edge 按照图片边缘的像素值来填充。3.reflect，？ 4. symmetric，？ 修改亮度、对比度和饱和度：transforms.ColorJitterclass torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)功能：修改修改亮度、对比度和饱和度 转灰度图：transforms.Grayscaleclass torchvision.transforms.Grayscale(num_output_channels=1)功能：将图片转换为灰度图参数：num_output_channels- (int) ，当为1时，正常的灰度图，当为3时， 3 channel with r == g == b 线性变换：transforms.LinearTransformation()class torchvision.transforms.LinearTransformation(transformation_matrix)功能：对矩阵做线性变化，可用于白化处理！ whitening: zero-center the data, compute the data covariance matrix参数：transformation_matrix(Tensor) – tensor [D x D], D = C x H x W 仿射变换：transforms.RandomAffineclass torchvision.transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0)功能：仿射变换 依概率p转为灰度图：transforms.RandomGrayscaleclass torchvision.transforms.RandomGrayscale(p=0.1)功能：依概率p将图片转换为灰度图，若通道数为3，则3 channel with r == g == b 将数据转换为PILImage：transforms.ToPILImageclass torchvision.transforms.ToPILImage(mode=None)功能：将tensor 或者 ndarray的数据转换为 PIL Image 类型数据参数：mode- 为None时，为1通道， mode=3通道默认转换为RGB，4通道默认转换为RGBA transforms.LambdaApply a user-defined lambda as a transform.暂不了解，待补充。 对transforms操作，使数据增强更灵活PyTorch不仅可设置对图片的操作，还可以对这些操作进行随机选择、组合 transforms.RandomChoice(transforms)功能：从给定的一系列transforms中选一个进行操作，randomly picked from a list transforms.RandomApply(transforms, p=0.5)功能：给一个transform加上概率，以一定的概率执行该操作 transforms.RandomOrder`功能：将transforms中的操作顺序随机打乱 转载：https://blog.csdn.net/weixin_38533896/article/details/86028509#_Crop_36","categories":[{"name":"广东工业-物体检测","slug":"广东工业-物体检测","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/"},{"name":"函数分析","slug":"广东工业-物体检测/函数分析","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/函数分析/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/pytorch/"}]},{"title":"数据加载与处理","slug":"pytorch-数据加载与处理","date":"2019-05-18T02:06:08.000Z","updated":"2019-05-18T02:19:30.834Z","comments":true,"path":"2019/05/18/pytorch-数据加载与处理/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/18/pytorch-数据加载与处理/","excerpt":"一种是整个数据集都在一个文件夹下，内部再另附一个label文件，说明每个文件夹的状态，如这个数据库。这种存放数据的方式可能更适合在非分类问题上得到应用。 一种则是更适合使用在分类问题上，即把不同种类的数据分为不同的文件夹存放起来。","text":"一种是整个数据集都在一个文件夹下，内部再另附一个label文件，说明每个文件夹的状态，如这个数据库。这种存放数据的方式可能更适合在非分类问题上得到应用。 一种则是更适合使用在分类问题上，即把不同种类的数据分为不同的文件夹存放起来。 本文首先结合官方turorials介绍第一种方法，以了解其数据加载的原理；然后以代码形式简单介绍第二种方法。其中第二种方法和第一种方法的原理相同，其差别在于第二种方法运用了trochvision中提供的已写好的工具ImageFolder，因此实现起来更为简单。 第一种torch.utils.data.Dataset是一个抽象类，用户想要加载自定义的数据只需要继承这个类，并且覆写其中的两个方法即可： __len__: 覆写这个方法使得len(dataset)可以返回整个数据集的大小 __getitem__: 覆写这个方法使得dataset[i]可以返回数据集中第i个样本 不覆写这两个方法会直接返回错误，其源码如下： 12345def __getitem__(self, index): raise NotImplementedErrordef __len__(self): raise NotImplementedError 这里我随便从网上下载了20张图像，10张小猫，10张小狗。为了省事儿(只是想验证下继承Dataset类是否好用)，我没有给数据集增加标签文件，而是直接把1-10号定义为小猫，11-20号定义为小狗，这样会给__len__和__getitem__减小麻烦，其目录结构如下： 建立的自定义类如下： 123456789101112131415161718192021222324252627282930from torch.utils.data import DataLoader, Datasetfrom skimage import io, transformimport matplotlib.pyplot as plt import os import torchfrom torchvision import transformsimport numpy as np class AnimalData(Dataset): def __init__(self, root_dir, transform=None): self.root_dir = root_dir self.transform = transform def __len__(self): return 20 def __getitem__(self, idx): filenames = os.listdir(self.root_dir) filename = filenames[idx] img = io.imread(os.path.join(self.root_dir, filename)) # print filename[:-5] if (int(filename[:-5]) &gt; 10): lable = np.array([0]) else: lable = np.array([1]) sample = &#123;'image': img, 'lable':lable&#125; if self.transform: sample = self.transform(sample) return sample Transforms &amp; Compose transforms可以注意到上一节中AnimalData类中__init__中有个transform参数，这也是这一节中要讲清楚的问题。 从网上随便下载的图片必然大小不一，而cnn的结构却要求输入图像要有固定的大小；numpy中的图像通道定义为H, W, C，而pytorch中的通道定义为C, H, W; pytorch中输入数据需要将numpy array改为tensor类型；输入数据往往需要归一化，等等。 基于以上考虑，我们可以自定义一些Callable的类，然后作为trasform参数传递给上一节定义的dataset类。为了更加方便，torchvision.transforms.Compose提供了Compose类，可以一次性将我们自定义的callable类传递给dataset类，直接得到转换后的数据。 这里我直接copy了教程上的三个类：Rescale, RandomCrop, ToTensor,稍作改动，适应我的数据库。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980class Rescale(object): \"\"\"Rescale the image in a sample to a given size. Args: output_size (tuple or int): Desired output size. If tuple, output is matched to output_size. If int, smaller of image edges is matched to output_size keeping aspect ratio the same. \"\"\" def __init__(self, output_size): assert isinstance(output_size, (int, tuple)) self.output_size = output_size def __call__(self, sample): image, lable = sample['image'], sample['lable'] h, w = image.shape[:2] if isinstance(self.output_size, int): if h &gt; w: new_h, new_w = self.output_size * h / w, self.output_size else: new_h, new_w = self.output_size, self.output_size * w / h else: new_h, new_w = self.output_size new_h, new_w = int(new_h), int(new_w) img = transform.resize(image, (new_h, new_w)) # h and w are swapped for lable because for images, # x and y axes are axis 1 and 0 respectively # lable = lable * [new_w / w, new_h / h] return &#123;'image': img, 'lable': lable&#125;class RandomCrop(object): \"\"\"Crop randomly the image in a sample. Args: output_size (tuple or int): Desired output size. If int, square crop is made. \"\"\" def __init__(self, output_size): assert isinstance(output_size, (int, tuple)) if isinstance(output_size, int): self.output_size = (output_size, output_size) else: assert len(output_size) == 2 self.output_size = output_size def __call__(self, sample): image, lable = sample['image'], sample['lable'] h, w = image.shape[:2] new_h, new_w = self.output_size top = np.random.randint(0, h - new_h) left = np.random.randint(0, w - new_w) image = image[top: top + new_h, left: left + new_w] # lable = lable - [left, top] return &#123;'image': image, 'lable': lable&#125;class ToTensor(object): \"\"\"Convert ndarrays in sample to Tensors.\"\"\" def __call__(self, sample): image, lable = sample['image'], sample['lable'] # print lable # swap color axis because # numpy image: H x W x C # torch image: C X H X W image = image.transpose((2, 0, 1)) return &#123;'image': torch.from_numpy(image), 'lable': torch.from_numpy(lable)&#125; 定义好callable类之后，通过torchvision.transforms.Compose将上述三个类结合在一起，传递给AnimalData类中的transform参数即可。 1234trsm = transforms.Compose([Rescale(256), RandomCrop(224), ToTensor()])data = AnimalData('./all', transform=trsm) Iterating through the dataset上一节中得到data实例之后可以通过for循环来一个一个读取数据，现在这是效率低下的。torch.utils.data.DadaLoader类解决了上述问题。其主要有如下特点： Batching the data Shuffling the data Load the data in parallel using multiprocessing workers. 实现起来也很简单： 12345dataloader = DataLoader(data, batch_size=4, shuffle=True, num_workers=4)for i_batch, bach_data in enumerate(dataloader): print i_batch print bach_data['image'].size() print bach_data['lable'] 第二种torchvisionpytorch几乎将上述所有工作都封装起来供我们使用，其中一个工具就是torchvision.datasets.ImageFolder,用于加载用户自定义的数据，要求我们的数据要有如下结构： root/ants/xxx.png root/ants/xxy.jpeg root/ants/xxz.png . . . root/bees/123.jpg root/bees/nsdf3.png root/bees/asd932_.png torchvision.transforms中也封装了各种各样的数据处理的工具，如Resize, ToTensor等等功能供我们使用。 加载数据代码如下： 12345678910111213141516171819202122232425from torchvision import transforms, utilsfrom torchvision import datasetsimport torchimport matplotlib.pyplot as plt train_data = datasets.ImageFolder('./data1', transform=transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor()]))train_loader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True, ) print len(train_loader)for i_batch, img in enumerate(train_loader): if i_batch == 0: print(img[1]) fig = plt.figure() grid = utils.make_grid(img[0]) plt.imshow(grid.numpy().transpose((1, 2, 0))) plt.show() break 附录最后欣赏一段torchvision源码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136# vision/torchvision/datasets/folder.pyimport torch.utils.data as datafrom PIL import Imageimport osimport os.pathIMG_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm']def is_image_file(filename): \"\"\"Checks if a file is an image. Args: filename (string): path to a file Returns: bool: True if the filename ends with a known image extension \"\"\" filename_lower = filename.lower() return any(filename_lower.endswith(ext) for ext in IMG_EXTENSIONS)def find_classes(dir): classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))] classes.sort() class_to_idx = &#123;classes[i]: i for i in range(len(classes))&#125; return classes, class_to_idxdef make_dataset(dir, class_to_idx): images = [] dir = os.path.expanduser(dir) for target in sorted(os.listdir(dir)): d = os.path.join(dir, target) if not os.path.isdir(d): continue for root, _, fnames in sorted(os.walk(d)): for fname in sorted(fnames): if is_image_file(fname): path = os.path.join(root, fname) item = (path, class_to_idx[target]) images.append(item) return imagesdef pil_loader(path): # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835) with open(path, 'rb') as f: img = Image.open(f) return img.convert('RGB')def accimage_loader(path): import accimage try: return accimage.Image(path) except IOError: # Potentially a decoding problem, fall back to PIL.Image return pil_loader(path)def default_loader(path): from torchvision import get_image_backend if get_image_backend() == 'accimage': return accimage_loader(path) else: return pil_loader(path)class ImageFolder(data.Dataset): \"\"\"A generic data loader where the images are arranged in this way: :: root/dog/xxx.png root/dog/xxy.png root/dog/xxz.png root/cat/123.png root/cat/nsdf3.png root/cat/asd932_.png Args: root (string): Root directory path. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. loader (callable, optional): A function to load an image given its path. Attributes: classes (list): List of the class names. class_to_idx (dict): Dict with items (class_name, class_index). imgs (list): List of (image path, class_index) tuples \"\"\" def __init__(self, root, transform=None, target_transform=None, loader=default_loader): classes, class_to_idx = find_classes(root) imgs = make_dataset(root, class_to_idx) if len(imgs) == 0: raise(RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\" \"Supported image extensions are: \" + \",\".join(IMG_EXTENSIONS))) self.root = root self.imgs = imgs self.classes = classes self.class_to_idx = class_to_idx self.transform = transform self.target_transform = target_transform self.loader = loader def __getitem__(self, index): \"\"\" Args: index (int): Index Returns: tuple: (image, target) where target is class_index of the target class. \"\"\" path, target = self.imgs[index] img = self.loader(path) if self.transform is not None: img = self.transform(img) if self.target_transform is not None: target = self.target_transform(target) return img, target def __len__(self): return len(self.imgs) def __repr__(self): fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n' fmt_str += ' Number of datapoints: &#123;&#125;\\n'.format(self.__len__()) fmt_str += ' Root Location: &#123;&#125;\\n'.format(self.root) tmp = ' Transforms (if any): ' fmt_str += '&#123;0&#125;&#123;1&#125;\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp))) tmp = ' Target Transforms (if any): ' fmt_str += '&#123;0&#125;&#123;1&#125;'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp))) return fmt_str 参考博客：https://www.jianshu.com/p/220357ca3342","categories":[{"name":"广东工业-物体检测","slug":"广东工业-物体检测","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/"},{"name":"函数分析","slug":"广东工业-物体检测/函数分析","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/函数分析/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/pytorch/"}]},{"title":"数据加载与处理","slug":"数据加载与处理","date":"2019-05-18T02:06:08.000Z","updated":"2019-05-18T02:18:29.336Z","comments":true,"path":"2019/05/18/数据加载与处理/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/18/数据加载与处理/","excerpt":"一种是整个数据集都在一个文件夹下，内部再另附一个label文件，说明每个文件夹的状态，如这个数据库。这种存放数据的方式可能更适合在非分类问题上得到应用。 一种则是更适合使用在分类问题上，即把不同种类的数据分为不同的文件夹存放起来。","text":"一种是整个数据集都在一个文件夹下，内部再另附一个label文件，说明每个文件夹的状态，如这个数据库。这种存放数据的方式可能更适合在非分类问题上得到应用。 一种则是更适合使用在分类问题上，即把不同种类的数据分为不同的文件夹存放起来。 本文首先结合官方turorials介绍第一种方法，以了解其数据加载的原理；然后以代码形式简单介绍第二种方法。其中第二种方法和第一种方法的原理相同，其差别在于第二种方法运用了trochvision中提供的已写好的工具ImageFolder，因此实现起来更为简单。 第一种torch.utils.data.Dataset是一个抽象类，用户想要加载自定义的数据只需要继承这个类，并且覆写其中的两个方法即可： __len__: 覆写这个方法使得len(dataset)可以返回整个数据集的大小 __getitem__: 覆写这个方法使得dataset[i]可以返回数据集中第i个样本 不覆写这两个方法会直接返回错误，其源码如下： 12345def __getitem__(self, index): raise NotImplementedErrordef __len__(self): raise NotImplementedError 这里我随便从网上下载了20张图像，10张小猫，10张小狗。为了省事儿(只是想验证下继承Dataset类是否好用)，我没有给数据集增加标签文件，而是直接把1-10号定义为小猫，11-20号定义为小狗，这样会给__len__和__getitem__减小麻烦，其目录结构如下： 建立的自定义类如下： 123456789101112131415161718192021222324252627282930from torch.utils.data import DataLoader, Datasetfrom skimage import io, transformimport matplotlib.pyplot as plt import os import torchfrom torchvision import transformsimport numpy as np class AnimalData(Dataset): def __init__(self, root_dir, transform=None): self.root_dir = root_dir self.transform = transform def __len__(self): return 20 def __getitem__(self, idx): filenames = os.listdir(self.root_dir) filename = filenames[idx] img = io.imread(os.path.join(self.root_dir, filename)) # print filename[:-5] if (int(filename[:-5]) &gt; 10): lable = np.array([0]) else: lable = np.array([1]) sample = &#123;'image': img, 'lable':lable&#125; if self.transform: sample = self.transform(sample) return sample Transforms &amp; Compose transforms可以注意到上一节中AnimalData类中__init__中有个transform参数，这也是这一节中要讲清楚的问题。 从网上随便下载的图片必然大小不一，而cnn的结构却要求输入图像要有固定的大小；numpy中的图像通道定义为H, W, C，而pytorch中的通道定义为C, H, W; pytorch中输入数据需要将numpy array改为tensor类型；输入数据往往需要归一化，等等。 基于以上考虑，我们可以自定义一些Callable的类，然后作为trasform参数传递给上一节定义的dataset类。为了更加方便，torchvision.transforms.Compose提供了Compose类，可以一次性将我们自定义的callable类传递给dataset类，直接得到转换后的数据。 这里我直接copy了教程上的三个类：Rescale, RandomCrop, ToTensor,稍作改动，适应我的数据库。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980class Rescale(object): \"\"\"Rescale the image in a sample to a given size. Args: output_size (tuple or int): Desired output size. If tuple, output is matched to output_size. If int, smaller of image edges is matched to output_size keeping aspect ratio the same. \"\"\" def __init__(self, output_size): assert isinstance(output_size, (int, tuple)) self.output_size = output_size def __call__(self, sample): image, lable = sample['image'], sample['lable'] h, w = image.shape[:2] if isinstance(self.output_size, int): if h &gt; w: new_h, new_w = self.output_size * h / w, self.output_size else: new_h, new_w = self.output_size, self.output_size * w / h else: new_h, new_w = self.output_size new_h, new_w = int(new_h), int(new_w) img = transform.resize(image, (new_h, new_w)) # h and w are swapped for lable because for images, # x and y axes are axis 1 and 0 respectively # lable = lable * [new_w / w, new_h / h] return &#123;'image': img, 'lable': lable&#125;class RandomCrop(object): \"\"\"Crop randomly the image in a sample. Args: output_size (tuple or int): Desired output size. If int, square crop is made. \"\"\" def __init__(self, output_size): assert isinstance(output_size, (int, tuple)) if isinstance(output_size, int): self.output_size = (output_size, output_size) else: assert len(output_size) == 2 self.output_size = output_size def __call__(self, sample): image, lable = sample['image'], sample['lable'] h, w = image.shape[:2] new_h, new_w = self.output_size top = np.random.randint(0, h - new_h) left = np.random.randint(0, w - new_w) image = image[top: top + new_h, left: left + new_w] # lable = lable - [left, top] return &#123;'image': image, 'lable': lable&#125;class ToTensor(object): \"\"\"Convert ndarrays in sample to Tensors.\"\"\" def __call__(self, sample): image, lable = sample['image'], sample['lable'] # print lable # swap color axis because # numpy image: H x W x C # torch image: C X H X W image = image.transpose((2, 0, 1)) return &#123;'image': torch.from_numpy(image), 'lable': torch.from_numpy(lable)&#125; 定义好callable类之后，通过torchvision.transforms.Compose将上述三个类结合在一起，传递给AnimalData类中的transform参数即可。 1234trsm = transforms.Compose([Rescale(256), RandomCrop(224), ToTensor()])data = AnimalData('./all', transform=trsm) Iterating through the dataset上一节中得到data实例之后可以通过for循环来一个一个读取数据，现在这是效率低下的。torch.utils.data.DadaLoader类解决了上述问题。其主要有如下特点： Batching the data Shuffling the data Load the data in parallel using multiprocessing workers. 实现起来也很简单： 12345dataloader = DataLoader(data, batch_size=4, shuffle=True, num_workers=4)for i_batch, bach_data in enumerate(dataloader): print i_batch print bach_data['image'].size() print bach_data['lable'] 第二种torchvisionpytorch几乎将上述所有工作都封装起来供我们使用，其中一个工具就是torchvision.datasets.ImageFolder,用于加载用户自定义的数据，要求我们的数据要有如下结构： root/ants/xxx.png root/ants/xxy.jpeg root/ants/xxz.png . . . root/bees/123.jpg root/bees/nsdf3.png root/bees/asd932_.png torchvision.transforms中也封装了各种各样的数据处理的工具，如Resize, ToTensor等等功能供我们使用。 加载数据代码如下： 12345678910111213141516171819202122232425from torchvision import transforms, utilsfrom torchvision import datasetsimport torchimport matplotlib.pyplot as plt train_data = datasets.ImageFolder('./data1', transform=transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor()]))train_loader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True, ) print len(train_loader)for i_batch, img in enumerate(train_loader): if i_batch == 0: print(img[1]) fig = plt.figure() grid = utils.make_grid(img[0]) plt.imshow(grid.numpy().transpose((1, 2, 0))) plt.show() break 附录最后欣赏一段torchvision源码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136# vision/torchvision/datasets/folder.pyimport torch.utils.data as datafrom PIL import Imageimport osimport os.pathIMG_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm']def is_image_file(filename): \"\"\"Checks if a file is an image. Args: filename (string): path to a file Returns: bool: True if the filename ends with a known image extension \"\"\" filename_lower = filename.lower() return any(filename_lower.endswith(ext) for ext in IMG_EXTENSIONS)def find_classes(dir): classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))] classes.sort() class_to_idx = &#123;classes[i]: i for i in range(len(classes))&#125; return classes, class_to_idxdef make_dataset(dir, class_to_idx): images = [] dir = os.path.expanduser(dir) for target in sorted(os.listdir(dir)): d = os.path.join(dir, target) if not os.path.isdir(d): continue for root, _, fnames in sorted(os.walk(d)): for fname in sorted(fnames): if is_image_file(fname): path = os.path.join(root, fname) item = (path, class_to_idx[target]) images.append(item) return imagesdef pil_loader(path): # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835) with open(path, 'rb') as f: img = Image.open(f) return img.convert('RGB')def accimage_loader(path): import accimage try: return accimage.Image(path) except IOError: # Potentially a decoding problem, fall back to PIL.Image return pil_loader(path)def default_loader(path): from torchvision import get_image_backend if get_image_backend() == 'accimage': return accimage_loader(path) else: return pil_loader(path)class ImageFolder(data.Dataset): \"\"\"A generic data loader where the images are arranged in this way: :: root/dog/xxx.png root/dog/xxy.png root/dog/xxz.png root/cat/123.png root/cat/nsdf3.png root/cat/asd932_.png Args: root (string): Root directory path. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. loader (callable, optional): A function to load an image given its path. Attributes: classes (list): List of the class names. class_to_idx (dict): Dict with items (class_name, class_index). imgs (list): List of (image path, class_index) tuples \"\"\" def __init__(self, root, transform=None, target_transform=None, loader=default_loader): classes, class_to_idx = find_classes(root) imgs = make_dataset(root, class_to_idx) if len(imgs) == 0: raise(RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\" \"Supported image extensions are: \" + \",\".join(IMG_EXTENSIONS))) self.root = root self.imgs = imgs self.classes = classes self.class_to_idx = class_to_idx self.transform = transform self.target_transform = target_transform self.loader = loader def __getitem__(self, index): \"\"\" Args: index (int): Index Returns: tuple: (image, target) where target is class_index of the target class. \"\"\" path, target = self.imgs[index] img = self.loader(path) if self.transform is not None: img = self.transform(img) if self.target_transform is not None: target = self.target_transform(target) return img, target def __len__(self): return len(self.imgs) def __repr__(self): fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n' fmt_str += ' Number of datapoints: &#123;&#125;\\n'.format(self.__len__()) fmt_str += ' Root Location: &#123;&#125;\\n'.format(self.root) tmp = ' Transforms (if any): ' fmt_str += '&#123;0&#125;&#123;1&#125;\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp))) tmp = ' Target Transforms (if any): ' fmt_str += '&#123;0&#125;&#123;1&#125;'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp))) return fmt_str 参考博客：https://www.jianshu.com/p/220357ca3342","categories":[{"name":"广东工业-物体检测","slug":"广东工业-物体检测","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/"},{"name":"函数分析","slug":"广东工业-物体检测/函数分析","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/函数分析/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/pytorch/"}]},{"title":"InceptionV4","slug":"InceptionV4","date":"2019-05-17T13:23:53.000Z","updated":"2019-05-17T14:48:20.231Z","comments":true,"path":"2019/05/17/InceptionV4/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/17/InceptionV4/","excerpt":"论文：《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, 3.08% test error 》 The paper address：http://arxiv.org/abs/1602.07261","text":"论文：《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, 3.08% test error 》 The paper address：http://arxiv.org/abs/1602.07261 引言google认为他们之前在改变架构选择上相对保守：网络结构的改变只局限于独立的网络组件范围内，从而保持剩下模型稳定。而如果不改变之前的这种原则，那么生成的模型将会比需要的还复杂（即过头了）。在这里，他们决定抛弃之前那个设计原则，对不同尺度的网格都采用统一的inception模块 。 在下面的网络结构图中：所有后面不带V的卷积，用的都是same-padded，也就是输出的网格大小等于输入网格的大小（如vgg的卷积一样）；带V的使用的是valid-padded，表示输出的网格尺寸是会逐步减小的（如lenet5的卷积一样）。 在下面的结构图中，每一个inception模块中都有一个1∗1的没有激活层的卷积层，用来扩展通道数，从而补偿因为inception模块导致的维度约间。其中Inception-ResNet-V1的结果与Inception v3相当；Inception-ResNet-V1与Inception v4结果差不多，不过实际过程中Inception v4会明显慢于Inception-ResNet-v2，这也许是因为层数太多了。且在Inception-ResNet结构中，只在传统层的上面使用BN层，而不在合并层上使用BN，虽然处处使用BN是有好处，不过更希望能够将一个完整的组件放入单独的GPU中。因为具有大量激活单元的层会占用过多的显存，所以希望这些地方丢弃BN，从而总体增加Inception模块的数量。使得不需要去解决计算资源和模块什么的权衡问题。 InceptionV4网络结构图如下： 子模块如下图所示： Stem模块： Inception模块 Reduction模块 Inception-ResNet-v1网络结构如图所示： 子模块： Stem模块: Inception模块： Reduction模块： Reduction-A结构 与Inception V4相同的是，在Inception-ResNet-A及Inception-ResNet-B后分别添加了Reduction-A和Reduction-B，其中Reduction-A的结构与Inception V4的一致，Reduction-B的结构如下。 Inception-ResNet-v2Inception-ResNet-v2的整体框架和Inception-ResNet-v1的一致。 只不过v2的计算量更加expensive些，它的stem结构与Inception V4的相同，Reduction-A与v1的相同，Inception-ResNet-A、Inception-ResNet-B、Inception-ResNet-C和Reduction-B的结构与v1的类似，只不过输出的channel数量更多。总的来说，Inception-ResNet-v2与Inception V4的相近。在Inception-ResNet-v2中同样使用了drop out和BN。 比较Inception-ResNet的stem模块和Reduction-B模块也略微不同。Inception-ResNet-v1和Inception-ResNet-v2主要在于Reduction-A结构不同：其中k,l,m,n表示filter bank size。 对残差模块的缩放我们发现，如果滤波器数量超过1000，残差网络开始出现不稳定，同时网络会在训练过程早期便会出现“死亡”，意即经过成千上万次迭代，在平均池化（average pooling） 之前的层开始只生成0。通过降低学习率，或增加额外的batch-normalizatioin都无法避免这种状况。我们发现，在将残差模块添加到activation激活层之前，对其进行放缩能够稳定训练。通常来说，我们将残差放缩因子定在0.1到0.3。 注：He在训练Residual Net时也发现这个问题，提出了“two phase”训练。首先“warm up”，使用较小的学习率。接着再使用较大的学习率。 缩放模块仅仅适用于最后的线性激活。 结论（1）Inception-ResNet-v1：混合Inception版本，它的计算效率同Inception-v3；（2）Inception-ResNet-v2：更加昂贵的混合Inception版本，同明显改善了识别性能；（3）Inception-v4：没有残差链接的纯净Inception变种，性能如同Inception-ResNet-v2我们研究了引入残差连接如何显著的提高inception网络的训练速度。而且仅仅凭借增加的模型尺寸，我们的最新的模型（带和不带残差连接）都优于我们以前的网络。","categories":[{"name":"CNN","slug":"CNN","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/CNN/"}],"tags":[{"name":"Inception","slug":"Inception","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/Inception/"}]},{"title":"InceptionV2、V3","slug":"InceptionV2-V3","date":"2019-05-17T13:02:01.000Z","updated":"2019-05-17T13:25:56.799Z","comments":true,"path":"2019/05/17/InceptionV2-V3/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/17/InceptionV2-V3/","excerpt":"论文：V2《Batch Normalization: Accelerating Deep Network Training by Reducing Internal 》 地址：http://arxiv.org/abs/1502.03167 论文：v3《Rethinking the Inception Architecture for Computer Vision, 3.5% test error 》 地址：http://arxiv.org/abs/1512.00567","text":"论文：V2《Batch Normalization: Accelerating Deep Network Training by Reducing Internal 》 地址：http://arxiv.org/abs/1502.03167 论文：v3《Rethinking the Inception Architecture for Computer Vision, 3.5% test error 》 地址：http://arxiv.org/abs/1512.00567 创新点InceptionV2 加入了BN层，使得每一层的特征高斯分布 将一个5x5的卷积核分解成两个3x3的卷积核 InceptionV3 采用不对称方式卷积，即将一个3x3的卷积分解成1x3与3x1的卷积代替 InceptionV2、V3卷积分解（Factorizing Convolutions）更大尺寸的卷积核可以带来更大的感受野，可以提取更多的特征，但是伴随着参数的增加，所以作者用用两个3x3的卷积核代替一个5x5的卷积核。这样可以得到与5x5相同的感受野，但是可以降低参数。如下图所示： 基于以上的想法作者将3x3的卷积核再次进行分解，如下图所示，用3个3x1取代3x3卷积： 因此，任意nxn的卷积都可以通过1xn卷积后接nx1卷积来替代。GoogLeNet团队发现在网络的前期使用这种分解效果并不好，在中度大小的特征图（feature map）上使用效果才会更好（特征图大小建议在12到20之间）。 v3一个最重要的改进是分解（Factorization），将7x7分解成两个一维的卷积（1x7,7x1），3x3也是一样（1x3,3x1），这样的好处，既可以加速计算（多余的计算能力可以用来加深网络），又可以将1个conv拆成2个conv，使得网络深度进一步增加，增加了网络的非线性（每增加一层都要进行ReLU） ，还有值得注意的地方是网络输入从224x224变为了299x299，更加精细设计了35x35/17x17/8x8的模块。 降低特征图大小一般情况下，如果想让图像缩小，可以有如下两种方式： 先池化再作Inception卷积，或者先作Inception卷积再作池化。但是方法一（左图）先作pooling（池化）会导致特征表示遇到瓶颈（特征缺失），方法二（右图）是正常的缩小，但计算量很大。为了同时保持特征表示且降低计算量，将网络结构改为下图，使用两个并行化的模块来降低计算量（卷积、池化并行执行，再进行合并）","categories":[{"name":"CNN","slug":"CNN","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/CNN/"}],"tags":[{"name":"Inception","slug":"Inception","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/Inception/"}]},{"title":"InceptionV1","slug":"InceptionV1","date":"2019-05-17T12:34:25.000Z","updated":"2019-05-17T13:03:50.841Z","comments":true,"path":"2019/05/17/InceptionV1/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/17/InceptionV1/","excerpt":"论文：《Going Deeper with Convolutions, 6.67% test error 》 地址：http://arxiv.org/abs/1409.4842","text":"论文：《Going Deeper with Convolutions, 6.67% test error 》 地址：http://arxiv.org/abs/1409.4842 概述直到GoogLeNet人们都在认为要想得到更高的性能是增加网络的深度与宽度（网络的层数与神经元数目），但是这样出现的问题就是: 若数据集有限，则参数过多容易出现过拟合现象 网络的计算复杂的增加，难以应用 随着网络层数的增加容易出现梯度消失的现象，难以优化模型 为了解决以上问题Inception出现，虽然网络深度与宽度增加，但是参数减少可以有效的克服上述的问题 Inception V1Inception v1的网络，将1x1，3x3，5x5的conv和3x3的pooling，堆叠在一起，一方面增加了网络的width，另一方面增加了网络对尺度的适应性； 图1：原始论文中的模型 图2：改进后的模型 图1是引用的原式论文的模型，所有的卷积操作与pooling操作都是在上一层数输出上来做的，这样做可以让模型自己选择合适的卷积操作，若输入的特征图维度很大时，5x5的卷积核参数量还是很大，所以作者在原式的结构上加上了1x1的卷积核，这样操作可以降低特征图的维度从而降低计算量，这就是InceptionV1结构图。 1x1卷积核的作用：1x1卷积的主要目的是为了减少维度，还用于修正线性激活（ReLU） 。 maxpooling的作用：以减少空间大小，降低过度拟合。 下图给出了GoogLeNet的结构图： 对上图说明如下： （1）GoogLeNet采用了模块化的结构（Inception结构），方便增添和修改； （2）网络最后采用了average pooling（平均池化）来代替全连接层，该想法来自NIN（Network in Network），事实证明这样可以将准确率提高0.6%。但是，实际在最后还是加了一个全连接层，主要是为了方便对输出进行灵活调整； （3）虽然移除了全连接，但是网络中依然使用了Dropout ; （4）为了避免梯度消失，网络额外增加了2个辅助的softmax用于向前传导梯度（辅助分类器）。辅助分类器是将中间某一层的输出用作分类，并按一个较小的权重（0.3）加到最终分类结果中，这样相当于做了模型融合，同时给网络增加了反向传播的梯度信号，也提供了额外的正则化，对于整个网络的训练很有裨益。而在实际测试的时候，这两个额外的softmax会被去掉。","categories":[{"name":"CNN","slug":"CNN","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/CNN/"}],"tags":[{"name":"Inception","slug":"Inception","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/Inception/"}]},{"title":"HetConv","slug":"HetConv","date":"2019-05-17T10:51:05.000Z","updated":"2019-05-17T12:24:09.621Z","comments":true,"path":"2019/05/17/HetConv/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/17/HetConv/","excerpt":"&lt;HetConv: Heterogeneous Kernel-Based Convolutions for Deep CNNs 论文:《 Heterogeneous Kernel-Based Convolutions for Deep CNNs 》 地址：https://arxiv.org/abs/1903.04120 源码：https://github.com/sxpro/HetConvolution2d_pytorch","text":"&lt;HetConv: Heterogeneous Kernel-Based Convolutions for Deep CNNs 论文:《 Heterogeneous Kernel-Based Convolutions for Deep CNNs 》 地址：https://arxiv.org/abs/1903.04120 源码：https://github.com/sxpro/HetConvolution2d_pytorch 引言​ 提出了新的神经网络架构，其中的而卷积操作是采用的异构核，该架构与VGG、Resnet等采用标准的卷积核相比，在不失精度的情况下计算量(FLPOs)可以减少3-8倍，与采用深度可卷积的架构相比计算量也可以得到减小。 ​ 目前卷积过滤器可以大致有：深度卷积(DW)、逐点卷积(PW)与群卷积(GW)，这些卷积方式在不降低精度的同时，可以降低计算量，大多数都是运用在轻量级的神经网络架构上。DW、PW、GW可以理解成同构卷积(Homogeneous Convolution )，即在卷积核的深度方向观察，卷积核的大小都是一致的。作者提出了一种异构卷积(Heterogeneous Convolution )，即在深度方向卷积核的大小不是一致的，比如一个标准的卷积核的长宽深度是3，3，128，但是异构卷积核可以是：前64是3x3大小的卷积核，后64是1x1的卷积核。异构卷积核可以有效的降低模型的计算量即FLOPs。 创新点 一个高效的异构卷积HetConv。 可以与大多数的轻量级网络相比可以保证0延时。 异构卷积HetConv结构比较​ 在本论文中，我们提出了一种包含异构卷积核（比如一些核的大小是 3×3，其余的是 1×1）的全新过滤器/卷积（HetConv），可以在保证原始模型同等准确度的同时降低 FLOPs。图 1 和图 2 展示了标准过滤器与 HetConv 过滤器之间的差异。 图 1：标准卷积过滤器（同构）和异构卷积过滤器（HetConv）之间的差异。其中 M 是指输入深度（输入通道的数量），P 是指 part（控制卷积过滤器中不同类型的核的数量）。在 M 个核中，M/P 个核的大小是 3×3，其余的都是 1×1。 图 2：我们提出的卷积过滤器（HetConv）与其它高效卷积过滤器的比较。我们的异构过滤器的延迟为零，其它（GWC+PWC 或 DWC+PWC）则有一个单元的延迟。 计算量比较 假设输出特征图的大小是D，卷积核的大小是K，输入特征图的通道数为M，输出特征图的通道数为N。 标准卷积核： $FLs_1=DDMNK*K$ 深度可分离卷积(DW+PW): $FLs_2=DDKKM+DDM*N$ 异构卷积HetConv： $PFs_3=DDKK(M/P)N+DDN(1-1/P)*M$ 异构卷积和与标准卷积核比较： $R=1/P+(1-1/P)/K^2$ 一般K的取值为1，3，5，，所以作者实验结构与VGG比较计算量可以减少8倍左右 异构卷积与PW+DW： $R = (K^2N1/P+N(1-1/P))/(K^2+N)$ K一般取值为1，3，5与N相比可以忽略，最后上式可以化简为$1/P$，从式中可以看出计算量可以减少很明显。 异构卷积核的内部分布 图 3：L 层处的卷积过滤器：我们提出的使用异构核的卷积过滤器（HetConv）。图中可以看到，每个通道都由 3×3 和 1×1 大小的异构核构成。在标准卷积过滤器中用 1×1 核替代 3×3 核能够在保持准确度的同时极大降低 FLOPs。一个特定层的过滤器排列成移位形式（即如果第一个过滤器从首个位置开始 3×3 核，则第二个过滤器从第二个位置开始 3×3 核，以此类推）。 延时比较 ​ 图 4：上图比较了不同类型的卷积的延迟情况。 论文的比较部分请自行比较 代码分析123456789101112131415161718192021222324252627282930class HetConv2d(nn.Module): def __init__(self, in_feats, out_feats, groups=1, p=2): super(HetConv2d_v2, self).__init__() if in_feats % groups != 0: raise ValueError('in_channels must be divisible by groups') self.in_feats = in_feats self.out_feats = out_feats self.groups = groups self.blocks = nn.ModuleList() for i in range(out_feats): self.blocks.append(self.make_HetConv2d(i, p)) def make_HetConv2d(self, n, p): layers = nn.ModuleList() for i in range(self.in_feats): if ((i - n) % (p)) == 0: layers.append(nn.Conv2d(1, 1, 3, 1, 1)) else: layers.append(nn.Conv2d(1, 1, 1, 1, 0)) return layers def forward(self, x): out = [] for i in range(0, self.out_feats): out_ = self.blocks[i][0](x[:, 0: 1, :, :]) for j in range(1, self.in_feats): out_ += self.blocks[i][j](x[:, j:j + 1, :, :]) out.append(out_) return torch.cat(out, 1)","categories":[{"name":"CNN","slug":"CNN","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/CNN/"}],"tags":[{"name":"HetConv","slug":"HetConv","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/HetConv/"}]},{"title":"iterrows对dataframe进行遍历","slug":"iterrows对dataframe进行遍历","date":"2019-05-17T08:55:45.000Z","updated":"2019-05-17T09:55:43.850Z","comments":true,"path":"2019/05/17/iterrows对dataframe进行遍历/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/17/iterrows对dataframe进行遍历/","excerpt":"","text":"定义一个数据 1a = pd.DataFrame([[1,2,3,4],[4,5,6,9],[7,8,9,50],[5,9,5,3]]) 现在对这个表格进行遍历，一般写法为： 1234for index,row in a.iterrows(): print(index) print(row)​ 00 11 22 33 4Name: 0, dtype: int6410 41 52 63 9Name: 1, dtype: int6420 71 82 93 50Name: 2, dtype: int6430 51 92 53 3Name: 3, dtype: int64​","categories":[{"name":"广东工业-物体检测","slug":"广东工业-物体检测","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/"},{"name":"函数分析","slug":"广东工业-物体检测/函数分析","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/函数分析/"}],"tags":[{"name":"python","slug":"python","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/python/"}]},{"title":"图片归一化","slug":"图片归一化","date":"2019-05-17T08:16:12.000Z","updated":"2019-05-17T09:55:58.300Z","comments":true,"path":"2019/05/17/图片归一化/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/17/图片归一化/","excerpt":"","text":"对Tensor进行变换class torchvision.transforms.Normalize(mean, std)给定均值：(R,G,B) 方差：（R，G，B），将会把Tensor正则化。即：Normalized_image=(image-mean)/std。 12# 图片归一化，由于采用ImageNet预训练网络，因此这里直接采用ImageNet网络的参数normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])","categories":[{"name":"广东工业-物体检测","slug":"广东工业-物体检测","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/"},{"name":"函数分析","slug":"广东工业-物体检测/函数分析","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/函数分析/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/pytorch/"}]},{"title":"train_test_split","slug":"train-test-split","date":"2019-05-17T08:03:22.000Z","updated":"2019-05-17T08:12:09.724Z","comments":true,"path":"2019/05/17/train-test-split/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/17/train-test-split/","excerpt":"","text":"在机器学习中，我们通常将原始数据按照比例分割为“测试集”和“训练集” ，从 sklearn.model_selection 中调用train_test_split 函数。 简单用法如下： 1234567X_train,X_test, y_train, y_test = sklearn.model_selection.train_test_split( train_data, train_target, test_size=0.4, random_state=666, stratify=y_train) train_data：所要划分的样本特征集 train_target：所要划分的样本结果 test_size：样本占比，如果是整数的话就是样本的数量 random_state：是随机数的种子。 随机数种子：其实就是该组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。比如你每次都填1，其他参数一样的情况下你得到的随机数组是一样的。但填0或不填，每次都会不一样。 stratify是为了保持split前类的分布。 比如有100个数据，80个属于A类，20个属于B类。如果train_test_split(… test_size=0.25, stratify = y_all), 那么split之后数据如下： training: 75个数据，其中60个属于A类，15个属于B类。 testing: 25个数据，其中20个属于A类，5个属于B类。 用了stratify参数，training集和testing集的类的比例是 A：B= 4：1，等同于split前的比例（80：20）。通常在这种类分布不平衡的情况下会用到stratify。 将stratify=X就是按照X中的比例分配 将stratify=y就是按照y中的比例分配","categories":[{"name":"广东工业-物体检测","slug":"广东工业-物体检测","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/"},{"name":"函数分析","slug":"广东工业-物体检测/函数分析","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/函数分析/"}],"tags":[{"name":"sklearn","slug":"sklearn","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/sklearn/"}]},{"title":"数据并行","slug":"数据并行","date":"2019-05-17T07:50:44.000Z","updated":"2019-05-17T07:56:28.727Z","comments":true,"path":"2019/05/17/数据并行/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/17/数据并行/","excerpt":"","text":"mdoel = torch.nn.DataParallel(model) 若设备有多个GPU可以使用，为了经可能充分的利用资源，可以将数据并行计算 12345678model = Model(input_size, output_size)if torch.cuda.device_count() &gt; 1: print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\") # dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs model = nn.DataParallel(model)if torch.cuda.is_available(): model.cuda() 若设备就一个GPU直接调用cuda()函数 12model = Model(input_size, output_size)model.cuda()","categories":[{"name":"广东工业-物体检测","slug":"广东工业-物体检测","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/"},{"name":"函数分析","slug":"广东工业-物体检测/函数分析","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/函数分析/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/pytorch/"}]},{"title":"os.path用法","slug":"os-path用法","date":"2019-05-17T07:12:26.000Z","updated":"2019-05-17T07:26:49.728Z","comments":true,"path":"2019/05/17/os-path用法/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/17/os-path用法/","excerpt":"","text":"os.path.basename(path) 返回的是path路径下的文件名 1234os.path.basename('G:\\guangdong\\scr\\gen_label_csv.py')#gen_label_csv.pyos.path.basename('G:\\guangdong\\scr')#scr这边默认是将scr当作文件处理 其他的os.path用法请参考：https://docs.python.org/3/library/os.path.html","categories":[{"name":"广东工业-物体检测","slug":"广东工业-物体检测","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/"},{"name":"函数分析","slug":"广东工业-物体检测/函数分析","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/函数分析/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/pytorch/"}]},{"title":"随机种子","slug":"主函数分析","date":"2019-05-17T06:49:06.000Z","updated":"2019-05-17T07:02:35.848Z","comments":true,"path":"2019/05/17/主函数分析/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/17/主函数分析/","excerpt":"1234np.random.seed(666)torch.manual_seed(666)torch.cuda.manual_seed_all(666)random.seed(666)","text":"1234np.random.seed(666)torch.manual_seed(666)torch.cuda.manual_seed_all(666)random.seed(666) np.random.seed()random.seed(666)作用：使得随机数据可预测，即只要seed的值一样，后续生成的随机数都一样。 123456789numpy.random.seed(0) numpy.random.rand(4) #array([ 0.55, 0.72, 0.6 , 0.54]) numpy.random.seed(0) numpy.random.rand(4) #array([ 0.55, 0.72, 0.6 , 0.54]) torch.manual_seed()&amp;torch.cuda.manual_seed_all() torch.manual_seed(args.seed)#为CPU设置种子用于生成随机数，以使得结果是确定的 torch.cuda.manual_seed(args.seed)#为当前GPU设置随机种子；如果使用多个GPU，应该使用 torch.cuda.manual_seed_all()为所有的GPU设置种子。","categories":[{"name":"广东工业-物体检测","slug":"广东工业-物体检测","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/"},{"name":"函数分析","slug":"广东工业-物体检测/函数分析","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/函数分析/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/pytorch/"}]},{"title":"MobileNetV1","slug":"MobileNetV1","date":"2019-05-10T06:10:18.000Z","updated":"2019-05-17T12:26:39.238Z","comments":true,"path":"2019/05/10/MobileNetV1/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/10/MobileNetV1/","excerpt":"","text":"MobileNetV1 《MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications 》 The paper address：https://arxiv.org/abs/1704.04861 The coding address：https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.py WWH（What&amp;Why&amp;How）​ MobileNet描述了一个高效的网络架构，可以构建一个非常小、低延时、满足嵌入式设备的要求。模型的大小显著下降，但是实际效果很好。 Innovation Points 深度可分离卷积（Depthwise Separable Convolution） 宽度因子（Width Multiplier）&amp; 分辨率因子（Resolution Multiplier） Depthwise Separable Convolution 深度可分离卷积：将普通卷积分成深度卷积（DW）与逐点卷积（PW），这样做可大幅度降低参数量与计算量。 问题描述 Depthwise Separable Convolution与普通卷积的输入与输出均相同，中间过程不同。 输入： ，其中 为原始图片尺寸， 为输入channel数量。 输出： ，其中 为输出图片尺寸， 为输出channel数量。 普通卷积 卷积核： 卷积核如下图所示： 深度可分离卷积深度卷积 输入： ，输出 卷积核：M个 。 理解： 将特征图看成是M张二维的特征图即M张 的特征图 将M张特征图分别与 的卷积核卷积 也就是对channel进行分解 逐点卷积 输入 ，输出 。 卷积核： 。 计算量对比分子为Depthwise Separable卷积，分母为普通卷积。 深度可分离卷积与标准卷积相比：当采用3x3的卷积核，计算量可以减少9倍左右 结构 注意：如果是需要下采样，则在第一个深度卷积上取步长为2. 实现代码1234567891011121314151617class Block(nn.Module): '''Depthwise conv + Pointwise conv''' def __init__(self, in_planes, out_planes, stride=1): super(Block, self).__init__() self.conv1 = nn.Conv2d\\ (in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=in_planes, bias=False) self.bn1 = nn.BatchNorm2d(in_planes) self.conv2 = nn.Conv2d\\ (in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False) self.bn2 = nn.BatchNorm2d(out_planes) def forward(self, x): out = F.relu(self.bn1(self.conv1(x))) out = F.relu(self.bn2(self.conv2(out))) return out MobileNetV1网络网络结构 网络介绍 MobileNet优化的重点放在优化延迟(latency)，兼顾模型大小。 网络参数、计算量分布： MobileNet中大多数计算量和参数都在1*1的卷积中，可以使用高度优化的。 由于模型较小，所以可以减少使用正则化，因为模型小不容易过拟合。 Width Multiplier&amp;Resolution MultiplierWidth Multiplier, ： 用于控制输入和输出的通道数，即输入通道从M 变为αM ,输出通道从N变为αN。 对于depthwise卷积操作，其计算量为： 可设置α∈(0,1]，通常取1,0.75,0.5和0.25 。 Resolution Multiplier， 用于控制输入和内部层表示。即用分辨率因子控制输入的分辨率，该参数用于控制特征图的宽和高 。 对于depthwise卷积操作，其计算量为： 可设置ρ∈(0,1]，通常设置输入分辨率为224,192,160和128","categories":[{"name":"CNN","slug":"CNN","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/CNN/"}],"tags":[{"name":"MobileNet","slug":"MobileNet","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/MobileNet/"}]},{"title":"MobileNetV2","slug":"MobileNetV2","date":"2019-05-10T06:10:18.000Z","updated":"2019-05-17T12:23:37.154Z","comments":true,"path":"2019/05/10/MobileNetV2/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/10/MobileNetV2/","excerpt":"MobileNetV2 《Inverted Residuals and Linear Bottlenecks Mobile Networks for Classification, Detection and Segmentation 》 The paper address：https://128.84.21.199/abs/1801.04381 The coding address：https://github.com/miraclewkf/MobileNetV2-PyTorch","text":"MobileNetV2 《Inverted Residuals and Linear Bottlenecks Mobile Networks for Classification, Detection and Segmentation 》 The paper address：https://128.84.21.199/abs/1801.04381 The coding address：https://github.com/miraclewkf/MobileNetV2-PyTorch WWH（What&amp;Why&amp;How）​ MobileNetV2是对MobileNetV1的改进，同样式轻量级的神经网络。实现了分类/目标检测/语义分割对目标任务，将MobileNetV2作为SSD的基础网络设计目标检测模型SSDLite。可以使参数降低一个数量级，但是mPA无明显变化。 ​ MobileNet-V1 最大的特点就是采用depth-wise separable convolution来减少运算量以及参数量，而在网络结构上，没有采用shortcut的方式。 ResNet及DenseNet等一系列采用shortcut的网络的成功，表明了shortcut是个非常好的东西，作者希望引入shortcut到MobileNet中。但是将residual block运用到depth-wise separable convolution，会碰到如下两个问题： DWConv layer 提取特征限制于输入特征维度，若采用residual block，1x1 PW conv操作后会先将输入特征图压缩(一般压缩率为0.25)，再经行DW conv后提取的特征或更少。MobileNetV2是先经过1x1 的PW conv操作将特征图扩张（本文扩大6倍），这也就可以不受输入通道的限制，可以提取更多的特征。 深度可分离卷积的PW conv相对于是对上一层DWconv的压缩，PWconv后跟随的是ReLU，根据ReLU的性质，输入特征若为负数，经过激活层输入的特征全是0，本来特征已经经过压缩，这会进一步损失特征值；若输入特征是正数，经过激活层输出特征是还原始的输入值。根据这一特点改变激活函数用Linear bottlenecks 代替。 Innovation Points Inverted residuals Linear Bottlenecks Inverted residuals​ Inverted residuals，通常的residuals block是先经过一个1x1的Conv layer，把feature map的通道数“压”下来，再经过3x3 Conv layer，最后经过一个1x1 的Conv layer，将feature map 通道数再“扩张”回去。即先“压缩”，最后“扩张”回去。 而 inverted residuals就是 先“扩张”，最后“压缩”。如下图所示。 ​ inverted residuals 可以认为是 residual block 的拓展。在 0&lt;t&lt;1，其实就是标准的残差模块。论文中 t 大部分为 6，呈现梭子的外形，而传统残差设计是沙漏形状。 Linear Bottlenecks​ 为了避免ReLU对特征的破坏，在residual block sum之前的PW conv后的激活层该成Linear Bottlenecks。在MobileNet V1中除了引入depthwise separable convolution代替传统的卷积，还做了一个实验是用width multiplier参数来做模型通道的缩减，相当于给模型“瘦身”，这样特征信息就能更集中在缩减后的通道中，但是如果此时加上一个非线性激活层，比如ReLU，就会有较大的信息丢失，因此为了减少信息丢失，就有了文中的linear bottleneck，意思就是bottleneck的输出不接非线性激活层，所以是linear，原因是： 对于ReLU层输出的非零值而言，ReLU层起到的就是一个线性变换的作用，这个从ReLU的曲线就能看出来。 ReLU层可以保留input manifold的信息，但是只有当input manifold是输入空间的一个低维子空间时才有效。 作者经过实验证明：当把原始输入维度增加到15或30后再作为ReLU的输入，输出恢复到原始维度后基本不会丢失太多的输入信息；相比之下如果原始输入维度只增加到2或3后再作为ReLU的输入，输出恢复到原始维度后信息丢失较多。因此在MobileNet V2中，执行降维的卷积层后面不会接类似ReLU这样的非线性激活层，也就是linear bottleneck的含义。 比较MobileNetV1和MobileNetV2的区别： 1、相同点： ​ 都采用 Depth-wise (DW) 卷积搭配 Point-wise (PW) 卷积的方式来提特征。这两个操作合起来也被称为 Depth-wise Separable Convolution，之前在 Xception 中被广泛使用。这么做的好处是理论上可以成倍的减少卷积层的时间复杂度和空间复杂度。标准卷积的计算复杂度近似为 DW + PW 组合卷积的 K^2倍。 2、不同点：（Linear Bottleneck） V2 在 DW 卷积之前新加了一个 PW 卷积。这么做的原因，是因为 DW 卷积由于本身的计算特性决定它自己没有改变通道数的能力，上一层给它多少通道，它就只能输出多少通道。所以如果上一层给的通道数本身很少的话，DW 也只能很委屈的在低维空间提特征，因此效果不够好。现在 V2 为了改善这个问题，给每个 DW 之前都配备了一个 PW，专门用来升维，定义升维系数6 ，这样不管输入通道数C_in 是多是少，经过第一个 PW 升维之后，DW 都是在相对的更高维进行着辛勤工作的。 V2 去掉了第二个 PW 的激活函数。论文作者称其为 Linear Bottleneck。这么做的原因，是因为作者认为激活函数在高维空间能够有效的增加非线性，而在低维空间时则会破坏特征，不如线性的效果好。由于第二个 PW 的主要功能就是降维，因此按照上面的理论，降维之后就不宜再使用 ReLU6 了。 精度对比如下图。 ResNet和MobileNet-V2的对比 1、相同点： （1）MobileNet V2 借鉴 ResNet，都采用了 的模式。 （2）MobileNet V2 借鉴 ResNet，同样使用 Shortcut 将输出与输入相加（未在上式画出） 2、不同点：（Inverted Residual Block） ResNet 使用 标准卷积 提特征，MobileNet 始终使用 DW卷积 提特征。 ResNet 先降维 (0.25倍)、卷积、再升维，而 MobileNet V2 则是 先升维 (6倍)、卷积、再降维。直观的形象上来看，ResNet 的微结构是沙漏形，而 MobileNet V2 则是纺锤形，刚好相反。因此论文作者将 MobileNet V2 的结构称为 Inverted Residual Block。这么做也是因为使用DW卷积而作的适配，希望特征提取能够在高维进行。 精度对比图。 网络结构​ 论文提出的 MobileNetV2 模型结构容易理解，基本单元 bottleneck 就是 Inverted residuals 模块，所用到的 tricks 比如 Dwise，就是 Depthwise Separable Convolutions，即各通道分别卷积。表 3 所示的分类网络结构输入图像分辨率 224x224，输出是全卷积而非 softmax，k 就是识别目标的类别数目。 ​ MobileNetV2 的网络结构中，第 6 行 stride=2，会导致下面通道分辨率变成14x14，从表格看，这个一处应该有误。 ​ 特别的，针对stride=1 和stride=2，在block上有稍微不同，主要是为了与shortcut的维度匹配，因此，stride=2时，不采用shortcut。 具体如下图：","categories":[{"name":"CNN","slug":"CNN","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/CNN/"}],"tags":[{"name":"MobileNet","slug":"MobileNet","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/MobileNet/"}]},{"title":"数据清洗","slug":"数据清洗","date":"2019-03-17T08:12:40.000Z","updated":"2019-05-17T06:43:30.400Z","comments":true,"path":"2019/03/17/数据清洗/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/03/17/数据清洗/","excerpt":"","text":"数据清洗预处理工具 关系型数据库或者Python 元数据与数据的特征查看 对数据有直观的了解，位置后的数据处理做准备 缺省值清洗 确定缺省值的范围 去除不需要的字段 填充缺省值内容（重要 ） 重新获取数据","categories":[{"name":"面试","slug":"面试","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/面试/"},{"name":"机器学习","slug":"面试/机器学习","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/面试/机器学习/"}],"tags":[]},{"title":"深度学习基础","slug":"深度学习","date":"2019-03-17T02:00:51.000Z","updated":"2019-03-18T11:43:23.621Z","comments":true,"path":"2019/03/17/深度学习/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/03/17/深度学习/","excerpt":"","text":"过拟合与欠拟合 欠拟合：指模型不能在训练数据集上获得足够低的训练误差 过拟合：指模型的训练误差与测试误差（泛化误差）上的误差过大 反映在评价指标上就是模型在训练集上表现良好，但是在测试集或者在新数据集上的表现一般（泛化能力差） 降低过拟合风险的方法 所有为了减小测试误差的策略统称为正则化方法，这些方法可能会议增大训练误差为代价。 数据增强 图像：平移、旋转、缩放 利用生成对抗网络生成新数据 NLP：利用机器翻译生成新数据 降低模型复杂度 神经网络：减少网络层、神经元个数 决策树：降低树的深度、剪枝 权值约束（添加正则化项） L1、L2正则化 集成学习 神经网络：Dropout 决策树：随机森林、GDBT 提前终止 降低欠拟合风险的方法 加入新的特征 交叉特征、多项式特征 升读学习：因子分解机、Deep-Crossing 增加模型复杂度 线性模型：添加高次项 神经网络：增加网络层数、神经元个数 减小正则化项的系数 添加正则化项是为了限制模型的学习能力，减小正则化项可以放宽这个限制 模型通常更加倾向于更大的权重，更大的权重可以使模型更好的拟合 反向传播算法反向传播的作用、目的、本质 概述：梯度下降法中需要利用损失函数对所有参数的梯度来寻找局部最小点，而反向传播算法就是用于计算梯度的算法，其本质就是利用链式求导法则对每个参数求偏导。 公式推导https://blog.csdn.net/lien0906/article/details/79193103 激活函数激活函数的作用——为什么使用非线性激活函数使用非线性激活函数的目的是为了像网络中加入非线性因素；增强网络的表示能力，解决线性网络存在的问题。 为什么加入非线性网络可以增强网络的表达能力呢——神经网络的万能近似定理 神经网络的万能近似定理认为震惊网络具有至少一个非线性隐含层，那么只要给网络足够数量的隐藏单元，他就可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的函数。 如果不使用非线性激活函数，那么每一层输出都是上一层输入的线性组合；此时无论网络有多少层，其整体也将是线性的，这会导致失去万能近似的性质。 但是部分层是纯线性是可以接受的，有助于减少网络的参数。 常见的激活函数整流线性单元ReLU ReLU通常是激活函数最好的默认选择 =\\max(0,z)) ReLU的拓展 ReLU及其扩展都是基于以下公式： 绝对值整流（absolute value rectification） 当a=-1，此时函数就是g(z)=|z| 渗漏整流线性单元 当a是一个很小的值，如a=0.001 参数化线性整流 将a作为一个可学习的参数 maxout单元 maxout单元进一步扩展了ReLU，他是一个可学习的K段函数，参数数量是普通全连接层的 k 倍 。 ####sigmoid与tanh sigmoid(z)，常记作 σ(z)； tanh(z) 的图像与 sigmoid(z) 大致相同，区别是值域为 (-1, 1) =\\frac{1}{1+\\exp(-z)}) 其他激活函数ReLU（优势）与sigmoid比较1.避免梯度消失 sigmoid函数在输入取绝对值非常大的正值或者负值的时候会出现饱和现象——在图像上表现的很平，此时函数会对输入的微小变化不敏感——从而造成梯度消失 ReLU的导数始终是一个常数——负半区为0，正半区为1，——所以不会发生梯度消失 2.减缓过拟合 ReLU在负半区输出为0，一旦神经元的激活值进入负半轴，那么该机或值就不会产生梯度/不会被训练，造成了网络的稀疏性——稀疏激活。 有利于减少参数的相互依赖，缓解过拟合问题的发生。 3.加速计算 ReLU的求导不涉及浮点运算 为什么 ReLU 不是全程可微/可导也能用于基于梯度的学习？ 在实现过程中通常返回左导数或者右导数的其中一个。 正则化Batch Normalizaton(批标准化) BN是一种正则化方法（较少泛化误差），主要的作用有： 加速网络的训练（缓解梯度消失，支持更大的学习率） 防止过拟合 降低参数初始化的要求 动机： 训练的本质就是学习数据的分布。如果训练数据与测试数据分布不同会降低模型的泛化能力，因此应该在训练前对所有输入数据做归一化处理。 在神经网络中，因为每个隐层的参数不同，会使下一层的输入发生变化，从而导致每一批数据的分布也发生变化，从而导致网络在每次迭代中需要拟合不同的数据分布，增加了网络的训练难度与过拟合的风险。 基本原理 BN方法会正对每一批数据，在网络的每一层输入之前增加归一化处理，是输入的均值为0，标准差为1，目的是将数据限制在统一的分布下。 具体就是，针对每一层的K个神经元，计算这一批数据在低K个神经元的均值与标准差，然后将归一化的值作为该神经元的激活函值。 BN可以看作在各层之间加入了一个新的计算层，对数据进行额外的约束，从而增加模型的泛化能力 但是BN也降低了模型的拟合能力，破坏了之前学习到的特征分布。 为了恢复原式的数据，BN引入了一个重构变化来还原最优的输入数据的分布 其中 γ 和 β 为可训练参数。 小结： 以上过程可归纳为一个 BN(x) 函数： ) 其中 &amp;=\\gamma\\boldsymbol{\\hat{x}_i}+\\beta\\&space;&amp;=\\gamma\\frac{\\boldsymbol{x_i}-\\boldsymbol{\\mathrm{E}[x_i]}}{\\sqrt{\\boldsymbol{\\mathrm{Var}[x_i]}+\\epsilon}}+\\beta&space;\\end{aligned}) 完整算法： L1/L2 范数正则化L1/L2范数的作用、异同相同点：限制了模型的学习能力——通过限制模型参数的规模，使模型偏好于权值较小的目标函数，防止过拟合。 不同点： L1正则化可以产生稀疏的权值举证，可以用于特征选择，同时一定程度上防止过拟合；L2正则化用于防止模型过拟合 L1正则化适用于特征之间有关联的情况；L2正则化适用于特征之间没有关联的情况。 为什么L1和L2正则化可以防止过拟合 L1、L2正则化回事模型偏好于更小的权重 更小的权重意味着更低的模型复杂度，添加L1和L2正则化相当于为模型添加了某种先验，限制参数的分布，从而降低了模型的复杂度。 模型的复杂度低意味着模型对于噪声与异常点的抗干扰能力增强，从而提高了模型的泛化能力——直观来说就是对训练数据的拟合刚刚号，不会过分拟合数据——奥卡姆剃刀定理 为什么L1正则化可以产生稀疏的权值，L2不会？ 对于目标函数添加范数正则化，训练时相当于在范数的约束下求目标函数J的最小值 带有L1 范数（左）和L2 范数（右）约束的二维图示 图中 J 与 L1 首次相交的点即是最优解。L1 在和每个坐标轴相交的地方都会有“顶点”出现，多维的情况下，这些顶点会更多；在顶点的位置就会产生稀疏的解。而 J 与这些“顶点”相交的机会远大于其他点，因此 L1 正则化会产生稀疏的解。 L2 不会产生“顶点”，因此 J 与 L2 相交的点具有稀疏性的概率就会变得非常小。 DropoutBagging集成学习 集成方法的主要想法是分别训练不同的模型，然后让所有的模型表决最终的输出。 集成学习凑效的原因是不同模型通常不会在测试集上产生相同的误差。 集成模型能至少与它的任何一成员 表现的一样好。如果成员的误差是独立的，集成将显著提升模型的性能。 Bagging是一种集成学习的策略——具体来说，Bagging涉及构造K个不同的数据集 每个数据集从原始数据集中重复采样构成，和原始数据集具有相同的样例，——这就意味着每个数据集有大概率缺少来自原始数据集的例子，但是也包含若干重复的例子 更具体的就是，如果采样所得到的训练集与原式的数据集大小相同，那么所得到的数据集中大概有原始数据集2/3的实例。 集成算法与神经网络 神经网络能够找到足够多的不同的解，意味着他们可以从模型平均中受益——即使所有的模型都在同一数据集上训练。 神经网络中随机初始化的差异、批训练数据的随机选择、超参数的差异等非确定性实现往往足以使得继承中的不同成员有部分独立的误差。 Dropout策略 简单来说，Dropout是通过共享参数提供了一种廉价的Bagging集成近似——Drpout相当于继承了从基础网络去除部分单元后形成的子网络。 通常隐层的采样率为0.5,输入的采样率为0.8，超参数也可以这样，但其采样率一般为1 权重比例推断规则 权重比例推断规则的目的是确保在测试时一个单元的期望总输入与在训练时该单元的期望总输入大致相同。 实践时，如果使用 0.5 的采样概率，权重比例规则相当于在训练结束后将权重乘 0.5，然后像平常一样使用模型；等价的，另一种方法是在训练时将单元的状态乘 2。 Dropout 与 Bagging 的不同 在 Bagging 的情况下，所有模型都是独立的；而在 Dropout 的情况下，所有模型共享参数，其中每个模型继承父神经网络参数的不同子集。 在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。","categories":[{"name":"面试","slug":"面试","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/面试/"}],"tags":[]},{"title":"改进快速排序","slug":"快速排序","date":"2019-03-17T02:00:51.000Z","updated":"2019-03-18T12:26:56.541Z","comments":true,"path":"2019/03/17/快速排序/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/03/17/快速排序/","excerpt":"","text":"##partition算法 （荷兰国旗问题）给定一个数组arr，和一个数num，请把小于num的数放在数组的左边，等于num的数放在数组的中间，大于num的数放在数组的右边。要求额外空间复杂度O(1)，时间复杂度O(N) 思想： 大于num的数放在数组的左边，小于num的数放在数组的右边，设当前指针为cur，小于区域的最后一个数指针为less，大于区域的第一个数为more。 解题： 要比较的数x=num，cur+1； 要比较的数x&lt;num,x与小于区域的下一个数交换，并且cur+1、less+1 要比较的数x&gt;num,x与大于区域的前一个数交换，并且more+1 代码： 123456789101112131415161718192021public static int[] pattena(int[] arr,int L,int R,int num)&#123; int less = L-1; int more = R+1; int cur = L; while (cur&lt;more)&#123; if(arr[cur]&lt;num)&#123; awap(arr,++less,cur++); &#125;else if(arr[cur]&gt;num)&#123; awap(arr,cur,--more); &#125;else &#123; cur++; &#125; &#125; //return new int[]&#123;less+1,more-1&#125;; return arr; &#125;public static void awap(int[] arr,int i ,int j)&#123; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp;&#125; 基于partition改进后的快速排序 随机快速排序的细节和复杂度分析可以用荷兰国旗问题来改进快速排序时间复杂度O(N*logN)，额外空间复杂度O(logN) 解题： 将数组的最后一个数最为num 小于num的放在左边，—&gt; 递归 大于num的放在右边，—&gt; 递归 代码实现： 12345678910111213141516171819202122232425262728293031public static int[] quickSort(int arr[],int L, int R) &#123; if(L&lt;R) &#123; int num = arr[arr.length - 1]; int[] idnex = partition(arr, L, R,num); quickSort(arr, L, idnex[0]-1); quickSort(arr, idnex[1]+1, R); &#125; return arr; &#125;public static int[] partition(int[] arr,int L,int R,int num)&#123; int less = L-1; int more = R; int cur = L; while (cur &lt; more) &#123; if (arr[cur]&lt;num) &#123; exchange(arr,++less,cur++); &#125; else if (arr[cur]&gt;num) &#123; exchange(arr,--more, cur); &#125;else &#123; cur++; &#125; &#125; exchange(arr, more, R); return new int[]&#123;less + 1, more &#125;;&#125;public static void exchange(int[] arr,int i, int j) &#123; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp;&#125;","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/数据结构/"}],"tags":[]},{"title":"CBAM","slug":"CBAM网络","date":"2019-03-16T06:10:18.000Z","updated":"2019-05-15T12:10:01.580Z","comments":true,"path":"2019/03/16/CBAM网络/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/03/16/CBAM网络/","excerpt":"摘要 CBAM特点就是轻量级通用模块 可以很好的加入到CNN结构中 端到端可训练的CNNs","text":"摘要 CBAM特点就是轻量级通用模块 可以很好的加入到CNN结构中 端到端可训练的CNNs 作者贡献 提出了一个高效的attention模块—-CBAM，该模块能够嵌入到目前的主流CNN网络结构中。 通过额外的分离实验证明了CBAM中attention的有效性。 在多个平台上（ImageNet-1K，MS COCO和VOC 2007）上证明了CBAM的性能提升。 Convolutional Block Attention Module给定输入特征为 对于一个中间层的feature map：，CBAM将会顺序推理出1维的channel attention map 以及2维的spatial attention map ，整个过程如下所示： %20%5Cotimes%20F) %20%5Cotimes%20F%5E%7B%27%7D) 其中为element-wise multiplication，首先将channel attention map与输入的feature map相乘得到，之后计算的spatial attention map，并将两者相乘得到最终的输出。下图为CBAM的示意图： Channel attention moduleeature map 的每个channel都被视为一个feature detector，channel attention主要关注于输入图片中什么(what)是有意义的。为了高效地计算channel attention，论文使用最大池化和平均池化对feature map在空间维度上进行压缩，得到两个不同的空间背景描述：和。使用由MLP组成的共享网络对这两个不同的空间背景描述进行计算得到channel attention map：。计算过程如下： 其中，，后使用了Relu作为激活函数。 Spatial attention module. 与channel attention不同，spatial attention主要关注于位置信息(where)。为了计算spatial attention，论文首先在channel的维度上使用最大池化和平均池化得到两个不同的特征描述和，然后使用concatenation将两个特征描述合并，并使用卷积操作生成spatial attention map %20%5Cin%20%5Cmathbb%20R_%7BH*W%7D)。计算过程如下： 其中，表示7*7的卷积层 下图为channel attention和spatial attention的示意图： 部分代码： 12345678910111213141516171819202122232425262728293031323334class ChannelAttention(nn.Module): def __init__(self, in_planes, ratio=16): super(ChannelAttention, self).__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) self.max_pool = nn.AdaptiveMaxPool2d(1) self.fc1 = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False) self.relu1 = nn.ReLU() self.fc2 = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False) self.sigmoid = nn.Sigmoid() def forward(self, x): avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x)))) max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x)))) out = avg_out + max_out return self.sigmoid(out)class SpatialAttention(nn.Module): def __init__(self, kernel_size=7): super(SpatialAttention, self).__init__() assert kernel_size in (3, 7), 'kernel size must be 3 or 7' padding = 3 if kernel_size == 7 else 1 self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False) self.sigmoid = nn.Sigmoid() def forward(self, x): avg_out = torch.mean(x, dim=1, keepdim=True) max_out, _ = torch.max(x, dim=1, keepdim=True) x = torch.cat([avg_out, max_out], dim=1) x = self.conv1(x) return self.sigmoid(x)","categories":[{"name":"CNN","slug":"CNN","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/CNN/"}],"tags":[]},{"title":"hexo基本使用","slug":"hexo基本使用","date":"2019-03-14T11:26:33.000Z","updated":"2019-05-17T12:26:06.798Z","comments":true,"path":"2019/03/14/hexo基本使用/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/03/14/hexo基本使用/","excerpt":"","text":"创建新的文章1hexo n 文章的名字 服务器的开启123hexo clearhexo ghexo s 服务器的关闭1ctrl + c 新建分类页与标签页 分类页 1hexo n page categories 标签页 1hexo n page tags 修改完章连接的样式 打开文件：..\\themes\\source\\css_common\\components\\post\\post.styl 12345678//输入代码.post-body p a &#123; color: #345; border-bottom: none; &amp;:hover&#123; color: red; &#125;&#125; 修改文章底部带#号的标签如： 打开文件 ：..\\themes\\next\\layout_macro\\post.swig 修改代码 123456789&lt;footer class=&quot;post-footer&quot;&gt; &#123;% if post.tags and post.tags.length and not is_index %&#125; &lt;div class=&quot;post-tags&quot;&gt; &#123;% for tag in post.tags %&#125; &lt;a href=&quot;&#123;&#123; url_for(tag.path) &#125;&#125;&quot; rel=&quot;tag&quot;&gt;&lt;i class=&quot;fa fa-google&quot;&gt;&lt;/i&gt; &#123;&#123; tag.name &#125;&#125;&lt;/a&gt; &#123;% endfor %&#125; &lt;/div&gt; &#123;% endif %&#125; ... 修改后的样式： 添加Valine评论功能增加搜索功能 打开主题、站点的配置文件 打开官网，找到第三方服务集成，找到搜索服务选择搜索样式： http://theme-next.iissnan.com/third-party-services.html#local-search 在git输入npm install hexo-generator-searchdb --save 编辑主题配置文件 123# Local searchlocal_search: enable: true 添加站点配置文件 12345search: path: search.xml field: post format: html limit: 10000 效果如图： 添加不蒜子统计功能 打开主题的配置文件 打开官网，找到第三方服务集成，找到统计服务经行修改 http://theme-next.iissnan.com/third-party-services.html#analytics-busuanzi 增加分享功能 打开主题的配置文件 找到baidushare修改代码 123baidushare: type: button baidushare: true 隐藏底部的强力驱动 注释掉代码段 1234567891011121314151617&lt;!--&#123;% if theme.footer.powered.enable %&#125; &lt;div class=\"powered-by\"&gt;&#123;# #&#125;&#123;&#123; __('footer.powered', next_url('https://hexo.io', 'Hexo', &#123;class: 'theme-link'&#125;)) &#125;&#125;&#123;# #&#125;&#123;% if theme.footer.powered.version %&#125; v&#123;&#123; hexo_env('version') &#125;&#125;&#123;% endif %&#125;&#123;# #&#125;&lt;/div&gt;&#123;% endif %&#125;&#123;% if theme.footer.powered.enable and theme.footer.theme.enable %&#125; &lt;span class=\"post-meta-divider\"&gt;|&lt;/span&gt;&#123;% endif %&#125;&#123;% if theme.footer.theme.enable %&#125; &lt;div class=\"theme-info\"&gt;&#123;# #&#125;&#123;&#123; __('footer.theme') &#125;&#125; – &#123;&#123; next_url('https://theme-next.org', 'NexT.' + theme.scheme, &#123;class: 'theme-link'&#125;) &#125;&#125;&#123;# #&#125;&#123;% if theme.footer.theme.version %&#125; v&#123;&#123; version &#125;&#125;&#123;% endif %&#125;&#123;##&#125;&lt;/div&gt;&#123;% endif %&#125;--&gt; 实现字数统计与阅读时长功能","categories":[{"name":"工具","slug":"工具","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/工具/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/Hexo/"}]}]}