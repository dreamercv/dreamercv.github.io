{"meta":{"title":"一个专注DL的小白","subtitle":null,"description":"专注=完美","author":"范斌","url":"https://dreamercv.github.io/dreamercv.github.io","root":"/dreamercv.github.io/"},"pages":[{"title":"分类","date":"2019-03-12T04:12:00.000Z","updated":"2019-03-12T04:13:17.141Z","comments":true,"path":"categories/index.html","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-03-12T04:06:44.000Z","updated":"2019-03-12T04:11:05.079Z","comments":true,"path":"tags/index.html","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/index.html","excerpt":"","text":""},{"title":"Me","date":"2019-03-15T10:50:36.000Z","updated":"2019-03-15T13:25:26.514Z","comments":true,"path":"about/index.html","permalink":"https://dreamercv.github.io/dreamercv.github.io/about/index.html","excerpt":"","text":"水滴石穿，积少成多！"}],"posts":[{"title":"OpenCV","slug":"OpenCV","date":"2019-06-02T10:34:39.000Z","updated":"2019-06-02T10:34:39.701Z","comments":true,"path":"2019/06/02/OpenCV/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/06/02/OpenCV/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"稀缺心态","slug":"稀缺心态","date":"2019-06-02T10:25:39.000Z","updated":"2019-06-02T10:32:21.785Z","comments":true,"path":"2019/06/02/稀缺心态/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/06/02/稀缺心态/","excerpt":"","text":"稀缺心态 一旦你有了省钱的脑袋，就不会有精力培养一个挣钱的脑袋 节约带宽 重要的事，强制提醒；Ongoing的事，默认处理；多次思考的事，打包解决。这样可以把脑力留出来一部分保障理性的思考与决策。 构建闲余 构建一个缓冲器，吸收了所有的不稳定因素。只有足够充足的闲余，才能让稀缺心态造成的负面影响降到最低。","categories":[{"name":"杂谈","slug":"杂谈","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/杂谈/"}],"tags":[]},{"title":"稀缺心态","slug":"杂谈/稀缺心态","date":"2019-06-02T10:25:39.000Z","updated":"2019-06-02T10:25:39.174Z","comments":true,"path":"2019/06/02/杂谈/稀缺心态/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/06/02/杂谈/稀缺心态/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"test","date":"2019-05-31T01:10:18.428Z","updated":"2019-06-02T01:20:40.485Z","comments":true,"path":"2019/05/31/test/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/31/test/","excerpt":"","text":"$x_1$，$x_2$，$x_3$ $ output = \\left{ \\begin{aligned} 0, \\quad if \\ \\ \\sum_i w_i x_i \\leqslant threshold \\ 1, \\quad if \\ \\ \\sum_i w_i x_i &gt; threshold \\end{aligned} \\right. $ $4 \\times 1 + 1 \\times 3 + (-3) \\times 2 = 1$ $$ \\sum_i w_i x_i &lt; threshold=3, output = 0 $$ 设最终误差为 $ E $且输出层的激活函数为线性激活函数，对于输出那么 $ E $ 对于输出节点 $ y_l $ 的偏导数是 $ y_l - t_l $，其中 $ t_l $ 是真实值，$ \\frac{\\partial y_l}{\\partial z_l} $ 是指上面提到的激活函数，$ z_l $ 是上面提到的加权和，那么这一层的 $ E $ 对于 $ z_l $ 的偏导数为 $ \\frac{\\partial E}{\\partial z_l} = \\frac{\\partial E}{\\partial y_l} \\frac{\\partial y_l}{\\partial z_l} $。同理，下一层也是这么计算，只不过 $ \\frac{\\partial E}{\\partial y_k} $ 计算方法变了，一直反向传播到输入层，最后有 $ \\frac{\\partial E}{\\partial x_i} = \\frac{\\partial E}{\\partial y_j} \\frac{\\partial y_j}{\\partial z_j} $，且 $ \\frac{\\partial z_j}{\\partial x_i} = w_i j $。然后调整这些过程中的权值，再不断进行前向传播和反向传播的过程，最终得到一个比较好的结果。 $ x_{i,j} $ 表示图像第 $ i $ 行第 $ j $ 列元素。$ w_{m,n} $ 表示 filter 第 $ m $ 行第 $ n $ 列权重。 $ w_b $ 表示 $filter$ 的偏置项。 表$a_i,_j$示 feature map 第 $ i$ 行第 $ j $ 列元素。 $f$ 表示激活函数，这里以$ ReLU$ 函数为例。 卷积计算公式如下： $$ a_{i,j} = f(\\sum_{m=0}^2 \\sum_{n=0}^2 w_{m,n} x_{i+m, j+n} + w_b ) $$ 当步长为 $1$ 时，计算 feature map 元素 $ a_{0,0} $ 如下： $$ a_{0,0} = f(\\sum_{m=0}^2 \\sum_{n=0}^2 w_{m,n} x_{0+m, 0+n} + w_b )= relu(w_{0,0} x_{0,0} + w_{0,1} x_{0,1} + w_{0,2} x_{0,2} + w_{1,0} x_{1,0} + w_{1,1} x_{1,1} + w_{1,2} x_{1,2} + w_{2,0} x_{2,0} + w_{2,1} x_{2,1} + w_{2,2} x_{2,2}) = 1 + 0 + 1 + 0 + 1 + 0 + 0 + 0 + 1 = 4 $$ $$ W_2 = (W_1 - F + 2P)/S + 1\\ H_2 = (H_1 - F + 2P)/S + 1 $$ ​ 其中 Layer $ L_1 $ 是输入层，Layer $ L_2 $ 是隐含层，Layer $ L_3 $ 是输出层。 ​ 假设输入数据集为 $ D={x_1, x_2, …, x_n} $，输出数据集为 $ y_1, y_2, …, y_n $。 ​ 如果输入和输出是一样，即为自编码模型。如果原始数据经过映射，会得到不同于输入的输出。 假设有如下的网络层： ​ 输入层包含神经元 $ i_1, i_2 $，偏置 $ b_1 $；隐含层包含神经元 $ h_1, h_2 $，偏置 $ b_2 $，输出层为 $ o_1, o_2 $，$ w_i $ 为层与层之间连接的权重，激活函数为 $sigmoid$ 函数。对以上参数取初始值，如下图所示 其中： 输入数据 $ i1=0.05, i2 = 0.10 $ 输出数据 $ o1=0.01, o2=0.99 $; 初始权重 $ w1=0.15, w2=0.20, w3=0.25,w4=0.30, w5=0.40, w6=0.45, w7=0.50, w8=0.55 $ 目标：给出输入数据 $ i1,i2 $ ( $0.05$和$0.10$ )，使输出尽可能与原始输出 $ o1,o2 $，( $0.01$和$0.99$)接近。 前向传播 输入层 –&gt; 输出层 计算神经元 $ h1 $ 的输入加权和： $ net_{h1} = w_1 i_1 + w_2 i_2 + b_1 * 1$ $net_{h1} = 0.15 0.05 + 0.2 0.1 + 0.35 * 1 = 0.3775 $ 神经元 $ h1 $ 的输出 $ o1 $ ：（此处用到激活函数为 sigmoid 函数）： $$ out_{h1} = \\frac{1}{1 + e^{-net_{h1}}} = \\frac{1}{1 + e^{-0.3775}} = 0.593269992 $$ 同理，可计算出神经元 $ h2 $ 的输出 $ o1 $： $$ out_{h2} = 0.596884378 $$ 隐含层–&gt;输出层： 计算输出层神经元 $ o1 $ 和 $ o2 $ 的值： $$ net_{o1} = w_5 out_{h1} + w_6 out_{h2} + b_2 * 1 $$ $$ net_{o1} = 0.4 0.593269992 + 0.45 0.596884378 + 0.6 * 1 = 1.105905967 $$ $$ out_{o1} = \\frac{1}{1 + e^{-net_{o1}}} = \\frac{1}{1 + e^{1.105905967}} = 0.75136079 $$ 这样前向传播的过程就结束了，我们得到输出值为 $ [0.75136079 , 0.772928465] $，与实际值 $ [0.01 , 0.99] $ 相差还很远，现在我们对误差进行反向传播，更新权值，重新计算输出。 反向传播 ​ 1.计算总误差 总误差：(这里使用Square Error) $$ E_{total} = \\sum \\frac{1}{2}(target - output)^2 $$ 但是有两个输出，所以分别计算 $ o1 $ 和 $ o2 $ 的误差，总误差为两者之和： $E_{o1} = \\frac{1}{2}(target_{o1} - out_{o1})^2 = \\frac{1}{2}(0.01 - 0.75136507)^2 = 0.274811083$. $E_{o2} = 0.023560026$. $E_{total} = E_{o1} + E_{o2} = 0.274811083 + 0.023560026 = 0.298371109$. ​ 2.隐含层 –&gt; 输出层的权值更新： 以权重参数 $ w5 $ 为例，如果我们想知道 $ w5 $ 对整体误差产生了多少影响，可以用整体误差对 $ w5 $ 求偏导求出：（链式法则） $$ \\frac{\\partial E_{total}}{\\partial w5} = \\frac{\\partial E_{total}}{\\partial out_{o1}} \\frac{\\partial out_{o1}}{\\partial net_{o1}} \\frac{\\partial net_{o1}}{\\partial w5} $$ 函数的定义为：$ f(x) = tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $，值域为 $ (-1,1) $。 $f(x) = \\left{ \\begin{aligned} ax, \\quad x0 \\end{aligned} \\right$，值域为 $ (-∞,+∞) $。 $f^{‘}(x)=\\frac{1}{1+e^{-x}}\\left( 1- \\frac{1}{1+e^{-x}} \\right)=f(x)(1-f(x))$ $f^{‘}(x)=-(tanh(x))^2$ 当$x=10$,或$x=-10$，$f^{‘}(x) \\approx0$,当$x=0$$f^{`}(x) =1$ 当$x=10$,或$x=-10$，$f^{‘}(x) \\approx0$,当$x=0$$f^{‘}(x) =0.25$","categories":[],"tags":[]},{"title":"DenseNet","slug":"CNN/DenseNet","date":"2019-05-30T14:04:20.000Z","updated":"2019-05-30T14:38:01.816Z","comments":true,"path":"2019/05/30/CNN/DenseNet/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/30/CNN/DenseNet/","excerpt":"论文：Densely Connected Convolutional Networks 地址：https://arxiv.org/pdf/1608.06993.pdf 代码：https://github.com/gpleiss/efficient_densenet_pytorch","text":"论文：Densely Connected Convolutional Networks 地址：https://arxiv.org/pdf/1608.06993.pdf 代码：https://github.com/gpleiss/efficient_densenet_pytorch 引言文章提出的DenseNet（Dense Convolutional Network）主要还是和ResNet及Inception网络做对比，思想上有借鉴，但却是全新的结构，网络结构并不复杂，却非常有效！众所周知，最近一两年卷积神经网络提高效果的方向，要么深（比如ResNet，解决了网络深时候的梯度消失问题）要么宽（比如GoogleNet的Inception），而作者则是从feature入手，通过对feature的极致利用达到更好的效果和更少的参数。 创新点 减轻了vanishing-gradient（梯度消失） 加强了feature的传递 更有效地利用了feature 一定程度上较少了参数数量 相关工作在深度学习网络中，随着网络深度的加深，梯度消失问题会愈加明显，作者在保证网络中层与层之间最大程度的信息传输的前提下，直接将所有层连接起来！ Dense block结构图在传统的卷积神经网络中，如果你有L层，那么就会有L个连接，但是在DenseNet中，会有L(L+1)/2个连接。简单讲，就是每一层的输入来自前面所有层的输出。如下图：x0是input，H1的输入是x0（input），H2的输入是x0和x1（x1是H1的输出）…… DenseNet的一个优点是网络更窄，参数更少，很大一部分原因得益于这种dense block的设计，后面有提到在dense block中每个卷积层的输出feature map的数量都很小（小于100），而不是像其他网络一样动不动就几百上千的宽度。同时这种连接方式使得特征和梯度的传递更加有效，网络也就更加容易训练。原文的一句话非常喜欢：Each layer has direct access to the gradients from the loss function and the original input signal, leading to an implicit deep supervision.直接解释了为什么这个网络的效果会很好。前面提到过梯度消失问题在网络深度越深的时候越容易出现，原因就是输入信息和梯度信息在很多层之间传递导致的，而现在这种dense connection相当于每一层都直接连接input和loss，因此就可以减轻梯度消失现象，这样更深网络不是问题。另外作者还观察到这种dense connection有正则化的效果，因此对于过拟合有一定的抑制作用，博主认为是因为参数减少了（后面会介绍为什么参数会减少），所以过拟合现象减轻。 DenseNet结构图前面的Figure 1表示的是dense block，而下面的Figure 2表示的则是一个DenseNet的结构图，在这个结构图中包含了3个dense block。作者将DenseNet分成多个dense block，原因是希望各个dense block内的feature map的size统一，这样在做concatenation就不会有size的问题。 这个Table1就是整个网络的结构图。这个表中的k=32，k=48中的k是growth rate，表示每个dense block中每层输出的feature map个数。为了避免网络变得很宽，作者都是采用较小的k，比如32这样，作者的实验也表明小的k可以有更好的效果。根据dense block的设计，后面几层可以得到前面所有层的输入，因此concat后的输入channel还是比较大的。另外这里每个dense block的33卷积前面都包含了一个11的卷积操作，就是所谓的bottleneck layer，目的是减少输入的feature map数量，既能降维减少计算量，又能融合各个通道的特征，何乐而不为。另外作者为了进一步压缩参数，在每两个dense block之间又增加了11的卷积操作。因此在后面的实验对比中，如果你看到DenseNet-C这个网络，表示增加了这个Translation layer，该层的11卷积的输出channel默认是输入channel到一半。如果你看到DenseNet-BC这个网络，表示既有bottleneck layer，又有Translation layer。 再详细说下bottleneck和transition layer操作。在每个Dense Block中都包含很多个子结构，以DenseNet-169的Dense Block（3）为例，包含32个11和33的卷积操作，也就是第32个子结构的输入是前面31层的输出结果，每层输出的channel是32（growth rate），那么如果不做bottleneck操作，第32层的33卷积操作的输入就是3132+（上一个Dense Block的输出channel），近1000了。而加上11的卷积，代码中的11卷积的channel是growth rate4，也就是128，然后再作为33卷积的输入。这就大大减少了计算量，这就是bottleneck。至于transition layer，放在两个Dense Block中间，是因为每个Dense Block结束后的输出channel个数很多，需要用11的卷积核来降维。还是以DenseNet-169的Dense Block（3）为例，虽然第32层的33卷积输出channel只有32个（growth rate），但是紧接着还会像前面几层一样有通道的concat操作，即将第32层的输出和第32层的输入做concat，前面说过第32层的输入是1000左右的channel，所以最后每个Dense Block的输出也是1000多的channel。因此这个transition layer有个参数reduction（范围是0到1），表示将这些输出缩小到原来的多少倍，默认是0.5，这样传给下一个Dense Block的时候channel数量就会减少一半，这就是transition layer的作用。文中还用到dropout操作来随机减少分支，避免过拟合，毕竟这篇文章的连接确实多。","categories":[{"name":"CNN","slug":"CNN","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/CNN/"}],"tags":[]},{"title":"前馈网络的写法","slug":"pytorch/前馈网络的写法","date":"2019-05-30T13:02:14.000Z","updated":"2019-05-30T13:55:08.041Z","comments":true,"path":"2019/05/30/pytorch/前馈网络的写法/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/30/pytorch/前馈网络的写法/","excerpt":"于此类网络如果每次都写复杂的forward函数会有些麻烦，在此就有两种简化方式，ModuleList和Sequential。其中Sequential是一个特殊的module，它包含几个子Module，前向传播时会将输入一层接一层的传递下去。ModuleList也是一个特殊的module，可以包含几个子module，可以像用list一样使用它，但不能直接把输入传给ModuleList。下面举例说明。 对于此类网络如果每次都写复杂的forward函数会有些麻烦，在此就有两种简化方式，ModuleList和Sequential。其中Sequential是一个特殊的module，它包含几个子Module，前向传播时会将输入一层接一层的传递下去。ModuleList也是一个特殊的module，可以包含几个子module，可以像用list一样使用它，但不能直接把输入传给ModuleList。下面举例说明。","text":"于此类网络如果每次都写复杂的forward函数会有些麻烦，在此就有两种简化方式，ModuleList和Sequential。其中Sequential是一个特殊的module，它包含几个子Module，前向传播时会将输入一层接一层的传递下去。ModuleList也是一个特殊的module，可以包含几个子module，可以像用list一样使用它，但不能直接把输入传给ModuleList。下面举例说明。 对于此类网络如果每次都写复杂的forward函数会有些麻烦，在此就有两种简化方式，ModuleList和Sequential。其中Sequential是一个特殊的module，它包含几个子Module，前向传播时会将输入一层接一层的传递下去。ModuleList也是一个特殊的module，可以包含几个子module，可以像用list一样使用它，但不能直接把输入传给ModuleList。下面举例说明。 Sequential创建前向传播过程Sequential+add_module创建前向过程1234net1 = nn.Sequential()net1.add_module('conv',nn.Conv2d(3,3,3))net1.add_module('bn',nn.BatchNorm2d(3))net1.add_module('relu',nn.ReLU()) Sequential直接创建前向过程–常用1234net2 = nn.Sequential(nn.Conv2d(3,3,3), nn.BatchNorm2d(3), nn.ReLU() ) Sequential+collections创建前向过程1234567from collections import OrderedDictnet3 = nn.Sequential(OrderedDict([ ('conv',nn.Conv2d(3,3,3)), ('bn',nn.BatchNorm2d(3)), ('relu',nn.ReLU())])) 检查1234567891011121314print('net1:',net1)'''输出：net1: Sequential( (conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU())'''print(net1.conv）'''输出： Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))''' ModuleList创建前向过程123456789101112131415161718192021222324252627282930class myModule(nn.Module): def __init__(self): super(myModule,self).__init__() self.module = nn.ModuleList([nn.Conv2d(3,3,3), nn.BatchNorm2d(3), nn.ReLU()]) def forward(): passmodel = myModule()print(model)'''输出：myModule( (module): ModuleList( (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ))'''for name,param in model.named_parameters(): print(name,param.size())'''输出： module.0.weight torch.Size([3, 3, 3, 3]) module.0.bias torch.Size([3]) module.1.weight torch.Size([3]) module.1.bias torch.Size([3])'''","categories":[{"name":"pytorch","slug":"pytorch","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/pytorch/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/pytorch/"}]},{"title":"ResNext","slug":"CNN/ResNext","date":"2019-05-30T11:07:19.000Z","updated":"2019-05-30T14:42:06.409Z","comments":true,"path":"2019/05/30/CNN/ResNext/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/30/CNN/ResNext/","excerpt":"论文：Aggregated Residual Transformations for Deep Neural Networks 地址：https://arxiv.org/abs/1611.05431 代码：https://github.com/miraclewkf/ResNeXt-PyTorch","text":"论文：Aggregated Residual Transformations for Deep Neural Networks 地址：https://arxiv.org/abs/1611.05431 代码：https://github.com/miraclewkf/ResNeXt-PyTorch 引言作者提出 ResNeXt 的主要原因在于：传统的要提高模型的准确率，都是加深或加宽网络，但是随着超参数数量的增加（比如channels数，filter size等等），网络设计的难度和计算开销也会增加。因此本文提出的 ResNeXt 结构可以在不增加参数复杂度的前提下提高准确率，同时还减少了超参数的数量。 作者在论文中首先提到VGG，VGG主要采用堆叠网络来实现，之前的 ResNet 也借用了这样的思想。然后提到 Inception 系列网络，简单讲就是 split-transform-merge 的策略，但是 Inception 系列网络有个问题：网络的超参数设定的针对性比较强，当应用在别的数据集上时需要修改许多参数，因此可扩展性一般。 创新点 cardinality 相关工作作者在这篇论文中提出网络 ResNeXt，同时采用 VGG 堆叠的思想和 Inception 的 split-transform-merge 思想，但是可扩展性比较强，可以认为是在增加准确率的同时基本不改变或降低模型的复杂度。这里提到一个名词cardinality，原文的解释是the size of the set of transformations，如下图 Fig1 右边是 cardinality=32 的样子，这里注意每个被聚合的拓扑结构都是一样的(这也是和 Inception 的差别，减轻设计负担)增加 cardinality 比增加深度和宽度更有效 ResNext的等效结构 下面是 ResNeXt-50 参数 主要遵从了两个原则： feature map 大小不变时，标准堆叠 当 feature map 的大小减半，则通道数增加一倍 实施细节表 1 中 conv3、conv4、conv5 的下采样过程采用 stride 为 2 的 3x3 卷积。使用 8 块 GPU 来训练模型优化器：SGDmomentum：0.9batch size：256 （每块 GPU 上 32）weight decay：0.0001初始学习速率：0.1学习速率衰减策略同测试时从 短边为 256 的图像中裁出一个 224x224 的图像进行测试 代码实现基于 图 3c ，并且在卷积后加BN+ReLU，在 shortcuut 加和只有使用ReLU。图3的三种形式是等价的，之所以选择 c 来实现，因为它更简单、代码更高效。","categories":[{"name":"CNN","slug":"CNN","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/CNN/"}],"tags":[]},{"title":"贴片天线单元","slug":"MOM/贴片天线单元","date":"2019-05-30T00:03:26.000Z","updated":"2019-05-30T14:23:03.124Z","comments":true,"path":"2019/05/30/MOM/贴片天线单元/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/30/MOM/贴片天线单元/","excerpt":"","text":"基于HFSS矩形微带天线仿真设计阵列天线其单元的工作频率是4.5GHz、贴片长度13.88mm时的回波损耗-34.86dB。","categories":[{"name":"MOM","slug":"MOM","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/MOM/"}],"tags":[]},{"title":"数据加载器","slug":"pytorch/数据加载器","date":"2019-05-24T14:24:49.000Z","updated":"2019-05-30T14:18:04.843Z","comments":true,"path":"2019/05/24/pytorch/数据加载器/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/24/pytorch/数据加载器/","excerpt":"发现一个很好用的图片读取工具以前要一张图片一张图片的读取，可以直接使用pytorch中的torchvision直接读取图片的类别与位置。","text":"发现一个很好用的图片读取工具以前要一张图片一张图片的读取，可以直接使用pytorch中的torchvision直接读取图片的类别与位置。 遍历文件读数据以前我读取数据是一个文件夹挨个浏览比一看一下以前的方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import osimport mathimport numpy as npimport pandas as pdimport os.path as ospfrom tqdm import tqdmlabel_warp = &#123;'正常': 0, '不导电': 1, '擦花': 2, '横条压凹': 3, '桔皮': 4, '漏底': 5, '碰伤': 6, '起坑': 7, '凸粉': 8, '涂层开裂': 9, '脏点': 10, '其他': 11, &#125;# train datadata_path = 'data/guangdong_round1_train2_20180916'img_path, label = [], []for first_path in os.listdir(data_path): first_path = osp.join(data_path, first_path) if '无瑕疵样本' in first_path: for img in os.listdir(first_path): img_path.append(osp.join(first_path, img)) label.append('正常') else: for second_path in os.listdir(first_path): defect_label = second_path second_path = osp.join(first_path, second_path) if defect_label != '其他': for img in os.listdir(second_path): img_path.append(osp.join(second_path, img)) label.append(defect_label) else: for third_path in os.listdir(second_path): third_path = osp.join(second_path, third_path) if osp.isdir(third_path): for img in os.listdir(third_path): if 'DS_Store' not in img: img_path.append(osp.join(third_path, img)) label.append(defect_label)label_file = pd.DataFrame(&#123;'img_path': img_path, 'label': label&#125;)label_file['label'] = label_file['label'].map(label_warp)label_file.to_csv('data/label.csv', index=False)# test datatest_data_path = 'data/guangdong_round1_test_a_20180916'all_test_img = os.listdir(test_data_path)test_img_path = []for img in all_test_img: if osp.splitext(img)[1] == '.jpg': test_img_path.append(osp.join(test_data_path, img))test_file = pd.DataFrame(&#123;'img_path': test_img_path&#125;)test_file.to_csv('data/test.csv', index=False) ImageFolder现在两句函数就搞定了，感叹自己学的少工具没有充分利用 12345678910111213141516171819202122from torchvision.datasets import ImageFolderfolder_set = ImageFolder('./data/Train/')folder_set.class_to_idx#&#123;'不导电': 0, '横条压凹': 1, '正常样本': 2, '涂层开裂': 3, '漏底': 4&#125;folder_set.imgs'''[('./data/Train/不导电\\\\不导电20180830131536对照样本.jpg', 0), ('./data/Train/不导电\\\\不导电20180830131551对照样本.jpg', 0), ('./data/Train/不导电\\\\不导电20180830131628对照样本.jpg', 0), ('./data/Train/不导电\\\\不导电20180830131827对照样本.jpg', 0), ('./data/Train/不导电\\\\不导电20180830134330对照样本.jpg', 0), ('./data/Train/不导电\\\\不导电20180830134402对照样本.jpg', 0), ('./data/Train/不导电\\\\不导电20180830134743对照样本.jpg', 0), ('./data/Train/不导电\\\\不导电20180830135013对照样本.jpg', 0), ('./data/Train/不导电\\\\不导电20180830140724对照样本.jpg', 0), ('./data/Train/不导电\\\\不导电20180830142107对照样本.jpg', 0), ('./data/Train/不导电\\\\不导电20180830155325对照样本.jpg', 0), ('./data/Train/不导电\\\\不导电20180831100625对照样本.jpg', 0), ('./data/Train/不导电\\\\不导电20180831101421对照样本.jpg', 0), ('./data/Train/不导电\\\\不导电20180831101642对照样本.jpg', 0),...]'''","categories":[{"name":"pytorch","slug":"pytorch","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/pytorch/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/pytorch/"}]},{"title":"目标检测论文综述","slug":"计算机视觉/目标检测论文综述","date":"2019-05-24T02:46:21.000Z","updated":"2019-05-30T14:22:40.329Z","comments":true,"path":"2019/05/24/计算机视觉/目标检测论文综述/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/24/计算机视觉/目标检测论文综述/","excerpt":"","text":"","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/计算机视觉/"}],"tags":[]},{"title":"CNN网络专题综述","slug":"CNN/CNN网络专题综述","date":"2019-05-24T01:47:05.000Z","updated":"2019-05-30T14:23:31.416Z","comments":true,"path":"2019/05/24/CNN/CNN网络专题综述/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/24/CNN/CNN网络专题综述/","excerpt":"论文《A Survey of the Recent Architectures of Deep Convolutional Neural Networks 》 地址 https://arxiv.org/ftp/arxiv/papers/1901/1901.06032.pdf","text":"论文《A Survey of the Recent Architectures of Deep Convolutional Neural Networks 》 地址 https://arxiv.org/ftp/arxiv/papers/1901/1901.06032.pdf 基于空间利用的 CNNCNN 有大量参数，如处理单元数量（神经元）、层数、滤波器大小、步幅、学习率和激活函数等。由于 CNN 考虑输入像素的邻域（局部性），可以使用不同大小的滤波器来探索不同级别的相关性。因此，在 2000 年初，研究人员利用空间变换来提升性能，此外，还评估了不同大小的滤波器对网络学习率的影响。不同大小的滤波器封装不同级别的粒度；通常，较小的滤波器提取细粒度信息，而较大的滤波器提取粗粒度信息。这样，通过调整滤波器大小，CNN 可以在粗粒度和细粒度的细节上都表现很好。 LeNet AlexNet ZefNet VGG GoogLeNet 基于深度的 CNN深度 CNN 架构基于这样一种假设：随着深度的增加，网络可以通过大量非线性映射和改进的特征表示更好地逼近目标函数。网络深度在监督学习的成功中起了重要作用。理论研究已表明，深度网络能够以指数方式比浅层网络更有效地表示特定的 20 个函数类型。2001 年，Csáji 表示了通用近似定理，指出单个隐藏层足够逼近任何函数，但这需要指数级的神经元，因而通常导致计算上行不通。在这方面，Bengio 和 elalleau 认为更深的网络有潜力在更少的成本下保持网络的表现能力。2013 年，Bengio 等人通过 实证表明，对于复杂的任务，深度网络在计算和统计上都更有效。在 2014-ILSVR 竞赛中表现最佳的 Inception 和 VGG 则进一步说明，深度是调节网络学习能力的重要维度。 一旦特征被提取，只要其相对于其他位置的近似位置被保留，其提取位置就变得没那么重要了。池化或下采样（如卷积）是一种有趣的局部操作。它总结了感受野附近的类似信息，并输出了该局部区域内的主要反应。作为卷积运算的输出结果，特征图案可能会出现在图像中的不同位置。 Highway Networks (2015) ResNet(2015) Inception-V3/V4 &amp; Inception-ResNet ResNext 基于多路径的 CNN深度网络的训练颇具挑战性，这也是近来很多深度网络研究的主题。深度 CNN 为复杂任务提供了高效的计算和统计。但是，更深的网络可能会遭遇性能下降或梯度消失/爆炸的问题，而这通常是由增加深度而非过拟合造成的。梯度消失问题不仅会导致更高的测试误差，还会导致更高的训练误差。为了训练更深的网络，多路径或跨层连接的概念被提出。多路径或捷径连接可以通过跳过一些中间层，系统地将一层连接到另一层，以使特定的信息流跨过层。跨层连接将网络划分为几块。这些路径也尝试通过使较低层访问梯度来解决梯度消失问题。为此，使用了不同类型的捷径连接，如零填充、基于投影、dropout 和 1x1 连接等。 激活函数是一种决策函数，有助于学习复杂的模式。选择适当的激活函数可以加速学习过程。卷积特征图的激活函数定义为等式 Highway Networks ResNet DenseNets 基于宽度的多连接 CNN2012 至 2015 年，网络架构的重点是深度的力量，以及多通道监管连接在网络正则化中的重要性。然而，网络的宽度和深度一样重要。通过在一层之内并行使用多处理单元，多层感知机获得了在感知机上映射复杂函数的优势。这表明宽度和深度一样是定义学习原则的一个重要参数。Lu 等人和 Hanin &amp; Sellke 最近表明，带有线性整流激活函数的神经网络要足够宽才能随着深度增加保持通用的近似特性。并且，如果网络的最大宽度不大于输入维度，紧致集上的连续函数类无法被任意深度的网络很好地近似。因此，多层堆叠（增加层）可能不会增加神经网络的表征能力。与深度架构相关的一个重要问题是，有些层或处理单元可能无法学习有用的特征。为了解决这一问题，研究的重点从深度和较窄的架构转移到了较浅和较宽的架构上。 WideResNet Pyramidal Net Xception Inception Family 基于特征图（通道特征图）开发的 CNNCNN 因其分层学习和自动特征提取能力而闻名于 MV 任务中。特征选择在决定分类、分割和检测模块的性能上起着重要作用。传统特征提取技术中分类模块的性能要受限于特征的单一性。相较于传统技术，CNN 使用多阶段特征提取，根据分配的输入来提取不同类型的特征（CNN 中称之为特征图）。但是，一些特征图有很少或者几乎没有目标鉴别作用。巨大的特征集有噪声效应，会导致网络过拟合。这表明，除了网络工程外，特定类别特征图的选取对改进网络的泛化性能至关重要。在这一部分，特征图和通道会交替使用，因为很多研究者已经用通道这个词代替了特征图。 SENet CMPE-SENet 基于通道（输入通道）利用的 CNN图像表征在决定图像处理算法的性能方面起着重要作用。图像的良好表征可以定义来自紧凑代码的图像的突出特征。在不同的研究中，不同类型的传统滤波器被用来提取单一类型图像的不同级别信息。这些不同的表征被用作模型的输入，以提高性能。CNN 是一个很好的特征学习器，它能根据问题自动提取鉴别特征。但是，CNN 的学习依赖于输入表征。如果输入中缺乏多样性和类别定义信息，CNN 作为鉴别器的性能就会受到影响。为此，辅助学习器的概念被引入到 CNN 中来提升网络的输入表征。 CB-CNN 基于注意力的 CNN不同的抽象级别在定义神经网络的鉴别能力方面有着重要的作用。除此之外，选择与上下文相关的特征对于图像定位和识别也很重要。在人类的视觉系统中，这种现象叫做注意力。人类在一次又一次的匆匆一瞥中观察场景并注意与上下文相关的部分。在这个过程中，人类不仅注意选择的区域，而且推理出关于那个位置的物体的不同解释。因此，它有助于人类以更好的方式来抓取视觉结构。类似的解释能力被添加到像 RNN 和 LSTM 这样的神经网络中。上述网络利用注意力模块来生成序列数据，并且根据新样本在先前迭代中的出现来对其加权。不同的研究者把注意力概念加入到 CNN 中来改进表征和克服数据的计算限制问题。注意力概念有助于让 CNN 变得更加智能，使其在杂乱的背景和复杂的场景中也能识别物体。 Residual Attention Network (RAN) Convolutional Block Attention Module (CBAM) Concurrent Spatial and Channel Excitation Mechanism","categories":[{"name":"CNN","slug":"CNN","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/CNN/"}],"tags":[]},{"title":"MOM","slug":"MOM/MOM","date":"2019-05-22T14:43:27.000Z","updated":"2019-05-29T13:57:25.540Z","comments":true,"path":"2019/05/22/MOM/MOM/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/22/MOM/MOM/","excerpt":"课题总方向： 算出2*2共形天线的输入阻抗，表面电流，辐射功率，天线阵增益 加入H_2矩阵的快速求解阻抗矩阵 由于耦合，对方向图进行优化","text":"课题总方向： 算出2*2共形天线的输入阻抗，表面电流，辐射功率，天线阵增益 加入H_2矩阵的快速求解阻抗矩阵 由于耦合，对方向图进行优化 细节单元贴片的尺寸，这里说的是平面是贴片的尺寸，如果圆柱的班级你过于波长满足一定的关系是，曲面贴片是可以近似成平面的（参考：高动态圆柱共形微带北斗天线设计）： 天线阵实现了高增益，波束宽度更大。 高动态圆柱共形微带北斗天线设计​ 该论文主要介绍了一腔模型为理论，分析了圆柱共形天线，不过里面涉及到了矩形微带单元尺寸的设计（曲面贴片何时可以等效成平面贴片的尺寸），可以借鉴。 共形缝隙微带天线阵 阵列单元使用的是低剖面缝隙天线，使用H形缝隙完成单元设计，相对贷款5.53%，增益4.2dBi，水平面不圆度&lt;1.5dB。 通过正交H型缝隙的方式对圆贴片辐射单元耦合馈电，实现天线的圆极化性能优化带宽设计了一种在L波段的微带天线，该天线的驻波比小于2，轴比小于3dB覆盖范围达到157.5°，实现了天线宽频带宽和宽覆盖角圆极化性能。 阵列天线的互耦特性研究在第二张的2.3节作者介绍了微带贴片的辐射特性中有利用传输线理论给出了E面H面的辐射方向图 借鉴第三章中阵列天线的研究方法，详细介绍了矩量法来分析阵列，其中提到了互阻抗、傅里叶变化的概念（不明白）。 第四章应该是考虑耦合情况下方向图的优化（先收藏着） 偶极子单元圆柱共形阵列电磁特性的MOM分析","categories":[{"name":"MOM","slug":"MOM","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/MOM/"}],"tags":[]},{"title":"二叉树概念","slug":"剑指offer/二叉树概念","date":"2019-05-20T13:21:57.000Z","updated":"2019-05-20T15:25:09.433Z","comments":true,"path":"2019/05/20/剑指offer/二叉树概念/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/20/剑指offer/二叉树概念/","excerpt":"","text":"二叉树 完全二叉树 满二叉树 平衡二叉树 排序二叉树 二叉树存储方式 顺序存储 链式存储 应用 html，xml 路由协议 MySQL 文件系统目录 机器学习算法等 二叉树 左子树、右子树 二叉树性质","categories":[{"name":"面试","slug":"面试","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/面试/"},{"name":"剑指offer","slug":"面试/剑指offer","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/面试/剑指offer/"}],"tags":[{"name":"二叉树","slug":"二叉树","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/二叉树/"}]},{"title":"机器学习一","slug":"机器学习/机器学习一","date":"2019-05-20T11:20:33.000Z","updated":"2019-05-20T11:21:10.906Z","comments":true,"path":"2019/05/20/机器学习/机器学习一/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/20/机器学习/机器学习一/","excerpt":"","text":"","categories":[{"name":"面试","slug":"面试","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/面试/"},{"name":"机器学习","slug":"面试/机器学习","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/面试/机器学习/"}],"tags":[]},{"title":"深度学习一","slug":"深度学习/深度学习一","date":"2019-05-20T11:03:53.000Z","updated":"2019-05-20T11:19:44.087Z","comments":true,"path":"2019/05/20/深度学习/深度学习一/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/20/深度学习/深度学习一/","excerpt":"","text":"卷积核是多层卷积核通常从属于后层，为后层提供了各种查看前层特征的视角，这个视角是自动形成的。 卷积核厚度等于1时为2D卷积，对应平面点相乘然后把结果加起来，相当于点积运算； 卷积核厚度大于1时为3D卷积，每片分别平面点求卷积，然后把每片结果加起来，作为3D卷积结果；1x1卷积属于3D卷积的一个特例，有厚度无面积, 直接把每片单个点乘以权重再相加。 归纳之，卷积的意思就是把一个区域，不管是一维线段，二维方阵，还是三维长方块，全部按照卷积核的维度形状，对应逐点相乘再求和，浓缩成一个标量值也就是降到零维度，作为下一层的一个feature map的一个点的值！ 什么是卷积首先说一下什么是卷积核：卷积核是用来存放权重参数，卷积操作就是将输入特征图与卷积核对应的点相乘再相加。如下图所示卷积操作：","categories":[{"name":"面试","slug":"面试","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/面试/"},{"name":"深度学习","slug":"面试/深度学习","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/面试/深度学习/"}],"tags":[]},{"title":"图像标注工具Labelme安装与使用","slug":"广东工业-物体检测/图像标注工具Labelme安装与使用","date":"2019-05-20T06:55:36.000Z","updated":"2019-05-20T08:20:42.663Z","comments":true,"path":"2019/05/20/广东工业-物体检测/图像标注工具Labelme安装与使用/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/20/广东工业-物体检测/图像标注工具Labelme安装与使用/","excerpt":"","text":"安装切换到需要的python版本安装 创建python环境，这里我是默认的python3.6的环境，所以没有另外创建环境，可以直接安装 1pip install labelme 安装完成后，直接在cmd中输入labelme，则进入到labelme界面： 制作数据源制作单个数据源1labelme_json_to_dataset.exe G:\\guangdong\\round2\\label\\擦花20180830172530对照样本.json 输入上述命令生成文件夹里面的文件有： 其中png是我们要的带bbox的数据源。 批量制作数据源若要自动生成png文件，需要修改Anaconda3\\Lib\\site-packages\\labelme\\cli\\json_to_dataset.py文件使之可以批量修改。","categories":[{"name":"广东工业-物体检测","slug":"广东工业-物体检测","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/"},{"name":"图片标注","slug":"广东工业-物体检测/图片标注","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/图片标注/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/工具/"}]},{"title":"Res2Net","slug":"CNN/Res2Net","date":"2019-05-19T13:51:40.000Z","updated":"2019-05-19T14:41:44.673Z","comments":true,"path":"2019/05/19/CNN/Res2Net/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/19/CNN/Res2Net/","excerpt":"论文：《Res2Net: A New Multi-scale Backbone Architecture》 地址： https://arxiv.org/pdf/1904.01169.pdf 代码：https://github.com/4uiiurz1/pytorch-res2net","text":"论文：《Res2Net: A New Multi-scale Backbone Architecture》 地址： https://arxiv.org/pdf/1904.01169.pdf 代码：https://github.com/4uiiurz1/pytorch-res2net 引言在多个视觉任务中，如图像分类，目标检测，动作识别，语义分割等，设计一个好的多尺度特征是非常重要的。 因此，多尺度的特征在传统方法和深度学习里面都得到了广泛应用。通常我们需要采用一个大感受野的特征提取器来获得不同尺度的特征描述，而卷积神经网络通过一堆卷积层可以很自然的由粗到细多尺度的提取特征。如何设计更高效的网络结构是提升卷积神经网络性能的关键。 作者提出了一种简单有效的多尺度提取方法。与现有的增强单层网络多尺度表达能力的 CNNs 方法不同，它是在更细的粒度上提升了多尺度表征能力。 Res2Net 已有的许多工作都是采用的上图（a）作为其 basic block，因此作者希望找到一种能保持计算量不增加，却有更强多尺度特征提取能力的结构来替代它。如上图（b）所示，作者采用了更小的卷积组来替代 bottleneck block 里面的 3x3 卷积。具体操作如下，首先将 1x1 卷积后的特征图均分为 s 个特征图子集。每个特征图子集的大小相同，但是通道数是输入特征图的 1/s。对每一个特征图子集 X_i，有一个对应的 3x3 卷积K_i(), 假设 K_i() 的输出是 y_i。接下来每个特征图子集 X_i 会加上 K_i-1() 的输出，然后一起输入进 K_i()。为了在增大 s 的值时减少参数量，作者省去了 X_1 的 3x3 网络。因此，输出 y_i 可以用如下公式表示： 根据图（b），可以发现每一个 X_j(j&lt;=i) 下的 3x3 卷积可以利用之前所有的特性信息，它的输出会有比 X_j 更大的感受野。因此这样的组合可以使 Res2Net 的输出有更多样的感受野信息。为了更好的融合不同尺度的信息，作者将它们的输出拼接起来，然后再送入 1x1 卷积，如上图（b）所示。 结论Res2Net 是一种简洁有效的模块，探索了 CNN 在更细粒度级别的多尺度表达能力。它揭示了 CNN 网络里面除了深度，宽度等现有维度之外，还可以有新的维度“尺度”。Res2Net 模块可以很容易地融合进 SOTA 的方法。在 CIFAR10 和 ImageNet 上图像分类的结果表明，使用 Res2Net 模块的网络比 ResNet，ResNeXt，DLA 等网络效果更好。鉴于Res2Net已经在几个具有代表性的计算机视觉任务体现出了优越性，作者认为网络的多尺度表征能力是非常重要的。","categories":[{"name":"CNN","slug":"CNN","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/CNN/"}],"tags":[]},{"title":"GroupConvolution","slug":"CNN/GroupConvolution","date":"2019-05-19T13:14:28.000Z","updated":"2019-05-19T13:45:03.287Z","comments":true,"path":"2019/05/19/CNN/GroupConvolution/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/19/CNN/GroupConvolution/","excerpt":"","text":"引言Group convolution 分组卷积，最早在AlexNet中出现，由于当时的硬件资源有限，训练AlexNet时卷积操作不能全部放在同一个GPU处理，因此作者把feature maps分给多个GPU分别进行处理，最后把多个GPU的结果进行融合。 分组卷积标准卷积 从图中可以看出标准卷积的参数量是h_1*w_1*H*W*c_2 分组卷积 如图所示输入特征图的参数：H，W，c_1，输出特征图的参数为H，W，c_2。 将输入图片的维度分成g组，每组的维度是c_1/g，这样子卷积核的深度就是c_1/g，将每组卷积输出的特征图的维度是c_2/g，之后进行拼接，形成输入特征图的维度是c_2，那么总的参数是h_1*w_1*c_1/g*H*W*c_2/g*g=h_1*w_1*c_1/g*H*W*c_2，参数与标准的卷积相比减少了1/g","categories":[{"name":"CNN","slug":"CNN","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/CNN/"}],"tags":[]},{"title":"VGG","slug":"CNN/VGG","date":"2019-05-18T14:16:08.000Z","updated":"2019-05-30T13:55:26.779Z","comments":true,"path":"2019/05/18/CNN/VGG/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/18/CNN/VGG/","excerpt":"论文：《Very Deep Convolutional Networks for Large-Scale Image Recognition》","text":"论文：《Very Deep Convolutional Networks for Large-Scale Image Recognition》 引言VGGNet探索了卷积神经网络的深度与其性能之间的关系，成功地构筑了16~19层深的卷积神经网络，证明了增加网络的深度能够在一定程度上影响网络最终的性能，使错误率大幅下降，同时拓展性又很强，迁移到其它图片数据上的泛化性也非常好。到目前为止，VGG仍然被用来提取图像特征。 创新点 用了较小的卷积核 每层卷积里含有子层卷积 采用小池化核 全连接层用卷积层代替 详细介绍 VGG也是用了五层卷积层、三层全连接层，卷积层之间加入了池化层，所有的激活层都是用ReLU。 小卷积核代替AlexNet的大卷积核VGG中用了多个3x3的小卷积核代替AlexNet中的7x7等大卷积核。这样一来可以可以增加网络的深度，提高模型的非线性映射，提高模型的拟合能力。减少参数。 作者认为两个3x3的感受野与一个5x5的感受野效果相同，这样可以增加非线性映射，也能很好地减少参数（例如7x7的参数为49个，而3个3x3的参数为27） 。 子层卷积小卷积核是VGG的一个重要特点，虽然VGG是在模仿AlexNet的网络结构，但没有采用AlexNet中比较大的卷积核尺寸（如7x7），而是通过降低卷积核的大小（3x3），增加卷积子层数来达到同样的性能（VGG：从1到4卷积子层，AlexNet：1子层）。 池化相比AlexNet的3x3的池化核，VGG全部采用2x2的池化核。 全连接转卷积（测试阶段）这也是VGG的一个特点，在网络测试阶段将训练阶段的三个全连接替换为三个卷积，使得测试得到的全卷积网络因为没有全连接的限制，因而可以接收任意宽或高为的输入，这在测试阶段很重要。 如本节第一个图所示，输入图像是224x224x3，如果后面三个层都是全连接，那么在测试阶段就只能将测试的图像全部都要缩放大小到224x224x3，才能符合后面全连接层的输入数量要求，这样就不便于测试工作的开展。 而“全连接转卷积”，替换过程如下： 例如7x7x512的层要跟4096个神经元的层做全连接，则替换为对7x7x512的层作通道数为4096、卷积核为1x1的卷积。 VGG网络架构 在这篇论文中分别使用了A、A-LRN、B、C、D、E这6种网络结构进行测试，这6种网络结构相似，都是由5层卷积层、3层全连接层组成，其中区别在于每个卷积层的子层数量不同，从A至E依次增加（子层数量从1到4），总的网络深度从11层到19层（添加的层以粗体显示），表格中的卷积层参数表示为“conv⟨感受野大小⟩-通道数⟩”，例如con3-128，表示使用3x3的卷积核，通道数为128。为了简洁起见，在表格中不显示ReLU激活功能。 其中，网络结构D就是著名的VGG16，网络结构E就是著名的VGG19。 以网络结构D（VGG16）为例，介绍其处理过程如下，请对比上面的表格和下方这张图，留意图中的数字变化，有助于理解VGG16的处理过程：","categories":[{"name":"CNN","slug":"CNN","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/CNN/"}],"tags":[]},{"title":"AlexNet","slug":"CNN/AlexNet","date":"2019-05-18T12:57:09.000Z","updated":"2019-05-30T11:07:43.885Z","comments":true,"path":"2019/05/18/CNN/AlexNet/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/18/CNN/AlexNet/","excerpt":"论文地址：http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf","text":"论文地址：http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf 引言2012年，Hinton的学生AlexKrizhevsky提出了深度卷积神经网络模型，可以算是LeNet的一种更深更宽的版本，其获得了2012年ILSVRC比赛分类项目的冠军，top-5错误率16.4%，使用额外数据可达到15.3%。AlexNet包含了八个学习层——5个卷积层和3个全连接层。 特点 使用了ReLU作为非线性激活函数 使用了Dropout，数据增强 重叠池化 ReLU之前的激活函数使用的都是sigmod，使用该函数作为激活函数很容易出现梯度消失的现象。 数据增强有一种观点认为神经网络是靠数据喂出来的，如果能够增加训练数据，提供海量数据进行训练，则能够有效提升算法的准确率，因为这样可以避免过拟合，从而可以进一步增大、加深网络结构。而当训练数据有限时，可以通过一些变换从已有的训练数据集中生成一些新的数据，以快速地扩充训练数据。 其中，最简单、通用的图像数据变形的方式：水平翻转图像，从原始图像中随机裁剪、平移变换，颜色、光照变换。 重叠池化之前的池化是不重叠的如图所示： 在AlexNet中使用的池化（Pooling）却是可重叠的，也就是说，在池化的时候，每次移动的步长小于池化的窗口长度。AlexNet池化的大小为3×3的正方形，每次池化移动步长为2，这样就会出现重叠。重叠池化可以避免过拟合，这个策略贡献了0.3%的Top-5错误率。 Dropout 引入Dropout主要是为了防止过拟合。在神经网络中Dropout通过修改神经网络本身结构来实现，对于某一层的神经元，通过定义的概率将神经元置为0，这个神经元就不参与前向和后向传播，就如同在网络中被删除了一样，同时保持输入层与输出层神经元的个数不变，然后按照神经网络的学习方法进行参数更新。在下一次迭代中，又重新随机删除一些神经元（置为0），直至训练结束。 Dropout应该算是AlexNet中一个很大的创新，以至于“神经网络之父”Hinton在后来很长一段时间里的演讲中都拿Dropout说事。Dropout也可以看成是一种模型组合，每次生成的网络结构都不一样，通过组合多个模型的方式能够有效地减少过拟合，Dropout只需要两倍的训练时间即可实现模型组合（类似取平均）的效果，非常高效。 如下图所示： 网络结构下图是AlexNet的网络结构图： AlexNet网络结构共有8层，前面5层是卷积层，后面3层是全连接层，最后一个全连接层的输出传递给一个1000路的softmax层，对应1000个类标签的分布。","categories":[{"name":"CNN","slug":"CNN","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/CNN/"}],"tags":[]},{"title":"DL中的Top","slug":"深度学习/DL中的Top","date":"2019-05-18T04:43:32.000Z","updated":"2019-05-30T14:14:13.671Z","comments":true,"path":"2019/05/18/深度学习/DL中的Top/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/18/深度学习/DL中的Top/","excerpt":"","text":"top1—–就是你预测的label取最后概率向量里面最大的那一个作为预测结果，如过你的预测结果中概率最大的那个分类正确，则预测正确。否则预测错误 top5—–就是最后概率向量最大的前五名中，只要出现了正确概率即为预测正确。否则预测错误。","categories":[{"name":"面试","slug":"面试","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/面试/"},{"name":"深度学习","slug":"面试/深度学习","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/面试/深度学习/"}],"tags":[]},{"title":"Markdown点滴","slug":"工具/markdown点滴","date":"2019-05-18T04:19:15.000Z","updated":"2019-05-18T04:26:32.906Z","comments":true,"path":"2019/05/18/工具/markdown点滴/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/18/工具/markdown点滴/","excerpt":"搭建博客时，需要设置页面布局等功能，一些Markdown的隐藏语法与hexo功能介绍。","text":"搭建博客时，需要设置页面布局等功能，一些Markdown的隐藏语法与hexo功能介绍。 导入图片 导入图片设置图片大小与位置 1&lt;img src=\"http://prjlqbksp.bkt.clouddn.com/blog/20190517/xLPq64JApYSM.png?imageslim\" width=\"400\" hegiht=\"30\" align=center/&gt; 显示设置 博客显示部分内容 123&lt;meta name=\"referrer\" content=\"no-referrer\" /&gt; &lt;!--more--&gt; 文字居中1&lt;center&gt;图1：原始论文中的模型&lt;/center&gt;","categories":[{"name":"工具","slug":"工具","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/工具/"}],"tags":[{"name":"博客","slug":"博客","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/博客/"}]},{"title":"__call__方法","slug":"广东工业-物体检测/call-方法","date":"2019-05-18T04:08:50.000Z","updated":"2019-05-18T04:15:23.714Z","comments":true,"path":"2019/05/18/广东工业-物体检测/call-方法/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/18/广东工业-物体检测/call-方法/","excerpt":"在Python的class中有一些函数往往具有特殊的意义。__init__()和__call__()就是class很有用的两类特殊的函数。","text":"在Python的class中有一些函数往往具有特殊的意义。__init__()和__call__()就是class很有用的两类特殊的函数。 __init__()在Python中，__init__()函数的意义等同于类的构造器（同理，__del__()等同于类的析构函数）。因此，__init__()方法的作用是创建一个类的实例。 __call__()Python中的函数是一级对象。这意味着Python中的函数的引用可以作为输入传递到其他的函数/方法中，并在其中被执行。而Python中类的实例（对象）可以被当做函数对待。也就是说，我们可以将它们作为输入传递到其他的函数/方法中并调用他们，正如我们调用一个正常的函数那样。而类中__call__()函数的意义正在于此。为了将一个类实例当做函数调用，我们需要在类中实现__call__()方法。也就是我们要在类中实现如下方法：def __call__(self, *args)。这个方法接受一定数量的变量作为输入。假设x是X类的一个实例。那么调用x.__call__(1,2)等同于调用x(1,2)。这个实例本身在这里相当于一个函数。 总结那么，__init__()和__call__()的区别如下： __init__()的作用是初始化某个类的一个实例。 __call__()的作用是使实例能够像函数一样被调用，同时不影响实例本身的生命周期（__call__()不影响一个实例的构造和析构）。但是__call__()可以用来改变实例的内部成员的值。即，若函数中有__call__()方法可以直接将实例当作函数调用。模糊了实例与对象的关系。 12345678910111213class X(object): def __init__(self, a, b, range): self.a = a self.b = b self.range = range def __call__(self, a, b): self.a = a self.b = b print('__call__ with （&#123;&#125;, &#123;&#125;）'.format(self.a, self.b)) def __del__(self, a, b, range): del self.a del self.b del self.range 123xInstance = X(1, 2, 3)xInstance(1,2)#两种效果一致#__call__ with (1, 2)","categories":[{"name":"广东工业-物体检测","slug":"广东工业-物体检测","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/"},{"name":"函数分析","slug":"广东工业-物体检测/函数分析","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/函数分析/"}],"tags":[{"name":"python","slug":"python","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/python/"}]},{"title":"transforms的二十二个方法","slug":"广东工业-物体检测/transforms的二十二个方法","date":"2019-05-18T02:34:02.000Z","updated":"2019-05-18T02:53:50.461Z","comments":true,"path":"2019/05/18/广东工业-物体检测/transforms的二十二个方法/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/18/广东工业-物体检测/transforms的二十二个方法/","excerpt":"预处理方法 ：裁剪—Crop、翻转和旋转—Flip and Rotation、图像变换、对transforms操作，使数据增强更灵活","text":"预处理方法 ：裁剪—Crop、翻转和旋转—Flip and Rotation、图像变换、对transforms操作，使数据增强更灵活 裁剪——Crop中心裁剪：transforms.CenterCrop随机裁剪：transforms.RandomCrop随机长宽比裁剪：transforms.RandomResizedCrop上下左右中心裁剪：transforms.FiveCrop上下左右中心裁剪后翻转：transforms.TenCrop 翻转和旋转——Flip and Rotation依概率p水平翻转：transforms.RandomHorizontalFlip(p=0.5)依概率p垂直翻转：transforms.RandomVerticalFlip(p=0.5)随机旋转：transforms.RandomRotation 图像变换resize：transforms.Resize标准化：transforms.Normalize转为tensor，并归一化至[0-1]：transforms.ToTensor填充：transforms.Pad修改亮度、对比度和饱和度：transforms.ColorJitter转灰度图：transforms.Grayscale线性变换：transforms.LinearTransformation()仿射变换：transforms.RandomAffine依概率p转为灰度图：transforms.RandomGrayscale将数据转换为PILImage：transforms.ToPILImagetransforms.Lambda：Apply a user-defined lambda as a transform. 对transforms操作，使数据增强更灵活transforms.RandomChoice(transforms)， 从给定的一系列transforms中选一个进行操作transforms.RandomApply(transforms, p=0.5)，给一个transform加上概率，依概率进行操作transforms.RandomOrder，将transforms中的操作随机打乱 裁剪—Crop随机裁剪：transforms.RandomCropclass torchvision.transforms.RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode=‘constant’)功能：依据给定的size随机裁剪参数：size- (sequence or int)，若为sequence,则为(h,w)，若为int，则(size,size)padding-(sequence or int, optional)，此参数是设置填充多少个pixel。 当为int时，图像上下左右均填充int个，例如padding=4，则上下左右均填充4个pixel，若为32*32，则会变成40*40。 当为sequence时，若有2个数，则第一个数表示左右扩充多少，第二个数表示上下的。当有4个数时，则为左，上，右，下。 fill- (int or tuple) 填充的值是什么（仅当填充模式为constant时有用）。int时，各通道均填充该值，当长度为3的tuple时，表示RGB通道需要填充的值。padding_mode- 填充模式，这里提供了4种填充模式，1.constant，常量。2.edge 按照图片边缘的像素值来填充。3.reflect，暂不了解。 4. symmetric，暂不了解。 中心裁剪：transforms.CenterCropclass torchvision.transforms.CenterCrop(size)功能：依据给定的size从中心裁剪参数：size- (sequence or int)，若为sequence,则为(h,w)，若为int，则(size,size) 随机长宽比裁剪 transforms.RandomResizedCropclass torchvision.transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)功能：随机大小，随机长宽比裁剪原始图片，最后将图片resize到设定好的size参数：size- 输出的分辨率scale- 随机crop的大小区间，如scale=(0.08, 1.0)，表示随机crop出来的图片会在的0.08倍至1倍之间。ratio- 随机长宽比设置interpolation- 插值的方法，默认为双线性插值(PIL.Image.BILINEAR) 上下左右中心裁剪：transforms.FiveCropclass torchvision.transforms.FiveCrop(size)功能：对图片进行上下左右以及中心裁剪，获得5张图片，返回一个4D-tensor参数：size- (sequence or int)，若为sequence,则为(h,w)，若为int，则(size,size) 上下左右中心裁剪后翻转: transforms.TenCropclass torchvision.transforms.TenCrop(size, vertical_flip=False)功能：对图片进行上下左右以及中心裁剪，然后全部翻转（水平或者垂直），获得10张图片，返回一个4D-tensor。参数：size- (sequence or int)，若为sequence,则为(h,w)，若为int，则(size,size)vertical_flip (bool) - 是否垂直翻转，默认为flase，即默认为水平翻转 翻转和旋转——Flip and Rotation依概率p水平翻转transforms.RandomHorizontalFlipclass torchvision.transforms.RandomHorizontalFlip(p=0.5)功能：依据概率p对PIL图片进行水平翻转参数：p- 概率，默认值为0.5 依概率p垂直翻转transforms.RandomVerticalFlipclass torchvision.transforms.RandomVerticalFlip(p=0.5)功能：依据概率p对PIL图片进行垂直翻转参数：p- 概率，默认值为0.5 随机旋转：transforms.RandomRotationclass torchvision.transforms.RandomRotation(degrees, resample=False, expand=False, center=None)功能：依degrees随机旋转一定角度参数：degress- (sequence or float or int)，若为单个数，如 30，则表示在（-30，+30）之间随机旋转若为sequence，如(30，60)，则表示在30-60度之间随机旋转resample- 重采样方法选择，可选 PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC，默认为最近邻expand- ?center- 可选为中心旋转还是左上角旋转 图像变换resize：transforms.Resizeclass torchvision.transforms.Resize(size, interpolation=2)功能：重置图像分辨率参数：size- If size is an int, if height &gt; width, then image will be rescaled to (size height / width, size)，所以建议size设定为hwinterpolation- 插值方法选择，默认为PIL.Image.BILINEAR 标准化：transforms.Normalizeclass torchvision.transforms.Normalize(mean, std)功能：对数据按通道进行标准化，即先减均值，再除以标准差，注意是h*w*c 转为tensor：transforms.ToTensorclass torchvision.transforms.ToTensor功能：将PIL Image或者 ndarray 转换为tensor，并且归一化至[0-1]注意事项：归一化至[0-1]是直接除以255，若自己的ndarray数据尺度有变化，则需要自行修改。 填充：transforms.Padclass torchvision.transforms.Pad(padding, fill=0, padding_mode=‘constant’)功能：对图像进行填充参数：padding-(sequence or int, optional)，此参数是设置填充多少个pixel。 当为int时，图像上下左右均填充int个，例如padding=4，则上下左右均填充4个pixel，若为32*32，则会变成40*40。 当为sequence时，若有2个数，则第一个数表示左右扩充多少，第二个数表示上下的。当有4个数时，则为左，上，右，下。 fill- (int or tuple)填充的值是什么（仅当填充模式为constant时有用）。int时，各通道均填充该值，当长度为3的tuple时，表示RGB通道需要填充的值。padding_mode- 填充模式，这里提供了4种填充模式，1.constant，常量。2.edge 按照图片边缘的像素值来填充。3.reflect，？ 4. symmetric，？ 修改亮度、对比度和饱和度：transforms.ColorJitterclass torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)功能：修改修改亮度、对比度和饱和度 转灰度图：transforms.Grayscaleclass torchvision.transforms.Grayscale(num_output_channels=1)功能：将图片转换为灰度图参数：num_output_channels- (int) ，当为1时，正常的灰度图，当为3时， 3 channel with r == g == b 线性变换：transforms.LinearTransformation()class torchvision.transforms.LinearTransformation(transformation_matrix)功能：对矩阵做线性变化，可用于白化处理！ whitening: zero-center the data, compute the data covariance matrix参数：transformation_matrix(Tensor) – tensor [D x D], D = C x H x W 仿射变换：transforms.RandomAffineclass torchvision.transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0)功能：仿射变换 依概率p转为灰度图：transforms.RandomGrayscaleclass torchvision.transforms.RandomGrayscale(p=0.1)功能：依概率p将图片转换为灰度图，若通道数为3，则3 channel with r == g == b 将数据转换为PILImage：transforms.ToPILImageclass torchvision.transforms.ToPILImage(mode=None)功能：将tensor 或者 ndarray的数据转换为 PIL Image 类型数据参数：mode- 为None时，为1通道， mode=3通道默认转换为RGB，4通道默认转换为RGBA transforms.LambdaApply a user-defined lambda as a transform.暂不了解，待补充。 对transforms操作，使数据增强更灵活PyTorch不仅可设置对图片的操作，还可以对这些操作进行随机选择、组合 transforms.RandomChoice(transforms)功能：从给定的一系列transforms中选一个进行操作，randomly picked from a list transforms.RandomApply(transforms, p=0.5)功能：给一个transform加上概率，以一定的概率执行该操作 transforms.RandomOrder`功能：将transforms中的操作顺序随机打乱 转载：https://blog.csdn.net/weixin_38533896/article/details/86028509#_Crop_36","categories":[{"name":"广东工业-物体检测","slug":"广东工业-物体检测","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/"},{"name":"函数分析","slug":"广东工业-物体检测/函数分析","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/函数分析/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/pytorch/"}]},{"title":"数据加载与处理","slug":"广东工业-物体检测/数据加载与处理","date":"2019-05-18T02:06:08.000Z","updated":"2019-05-18T02:18:29.336Z","comments":true,"path":"2019/05/18/广东工业-物体检测/数据加载与处理/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/18/广东工业-物体检测/数据加载与处理/","excerpt":"一种是整个数据集都在一个文件夹下，内部再另附一个label文件，说明每个文件夹的状态，如这个数据库。这种存放数据的方式可能更适合在非分类问题上得到应用。 一种则是更适合使用在分类问题上，即把不同种类的数据分为不同的文件夹存放起来。","text":"一种是整个数据集都在一个文件夹下，内部再另附一个label文件，说明每个文件夹的状态，如这个数据库。这种存放数据的方式可能更适合在非分类问题上得到应用。 一种则是更适合使用在分类问题上，即把不同种类的数据分为不同的文件夹存放起来。 本文首先结合官方turorials介绍第一种方法，以了解其数据加载的原理；然后以代码形式简单介绍第二种方法。其中第二种方法和第一种方法的原理相同，其差别在于第二种方法运用了trochvision中提供的已写好的工具ImageFolder，因此实现起来更为简单。 第一种torch.utils.data.Dataset是一个抽象类，用户想要加载自定义的数据只需要继承这个类，并且覆写其中的两个方法即可： __len__: 覆写这个方法使得len(dataset)可以返回整个数据集的大小 __getitem__: 覆写这个方法使得dataset[i]可以返回数据集中第i个样本 不覆写这两个方法会直接返回错误，其源码如下： 12345def __getitem__(self, index): raise NotImplementedErrordef __len__(self): raise NotImplementedError 这里我随便从网上下载了20张图像，10张小猫，10张小狗。为了省事儿(只是想验证下继承Dataset类是否好用)，我没有给数据集增加标签文件，而是直接把1-10号定义为小猫，11-20号定义为小狗，这样会给__len__和__getitem__减小麻烦，其目录结构如下： 建立的自定义类如下： 123456789101112131415161718192021222324252627282930from torch.utils.data import DataLoader, Datasetfrom skimage import io, transformimport matplotlib.pyplot as plt import os import torchfrom torchvision import transformsimport numpy as np class AnimalData(Dataset): def __init__(self, root_dir, transform=None): self.root_dir = root_dir self.transform = transform def __len__(self): return 20 def __getitem__(self, idx): filenames = os.listdir(self.root_dir) filename = filenames[idx] img = io.imread(os.path.join(self.root_dir, filename)) # print filename[:-5] if (int(filename[:-5]) &gt; 10): lable = np.array([0]) else: lable = np.array([1]) sample = &#123;'image': img, 'lable':lable&#125; if self.transform: sample = self.transform(sample) return sample Transforms &amp; Compose transforms可以注意到上一节中AnimalData类中__init__中有个transform参数，这也是这一节中要讲清楚的问题。 从网上随便下载的图片必然大小不一，而cnn的结构却要求输入图像要有固定的大小；numpy中的图像通道定义为H, W, C，而pytorch中的通道定义为C, H, W; pytorch中输入数据需要将numpy array改为tensor类型；输入数据往往需要归一化，等等。 基于以上考虑，我们可以自定义一些Callable的类，然后作为trasform参数传递给上一节定义的dataset类。为了更加方便，torchvision.transforms.Compose提供了Compose类，可以一次性将我们自定义的callable类传递给dataset类，直接得到转换后的数据。 这里我直接copy了教程上的三个类：Rescale, RandomCrop, ToTensor,稍作改动，适应我的数据库。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980class Rescale(object): \"\"\"Rescale the image in a sample to a given size. Args: output_size (tuple or int): Desired output size. If tuple, output is matched to output_size. If int, smaller of image edges is matched to output_size keeping aspect ratio the same. \"\"\" def __init__(self, output_size): assert isinstance(output_size, (int, tuple)) self.output_size = output_size def __call__(self, sample): image, lable = sample['image'], sample['lable'] h, w = image.shape[:2] if isinstance(self.output_size, int): if h &gt; w: new_h, new_w = self.output_size * h / w, self.output_size else: new_h, new_w = self.output_size, self.output_size * w / h else: new_h, new_w = self.output_size new_h, new_w = int(new_h), int(new_w) img = transform.resize(image, (new_h, new_w)) # h and w are swapped for lable because for images, # x and y axes are axis 1 and 0 respectively # lable = lable * [new_w / w, new_h / h] return &#123;'image': img, 'lable': lable&#125;class RandomCrop(object): \"\"\"Crop randomly the image in a sample. Args: output_size (tuple or int): Desired output size. If int, square crop is made. \"\"\" def __init__(self, output_size): assert isinstance(output_size, (int, tuple)) if isinstance(output_size, int): self.output_size = (output_size, output_size) else: assert len(output_size) == 2 self.output_size = output_size def __call__(self, sample): image, lable = sample['image'], sample['lable'] h, w = image.shape[:2] new_h, new_w = self.output_size top = np.random.randint(0, h - new_h) left = np.random.randint(0, w - new_w) image = image[top: top + new_h, left: left + new_w] # lable = lable - [left, top] return &#123;'image': image, 'lable': lable&#125;class ToTensor(object): \"\"\"Convert ndarrays in sample to Tensors.\"\"\" def __call__(self, sample): image, lable = sample['image'], sample['lable'] # print lable # swap color axis because # numpy image: H x W x C # torch image: C X H X W image = image.transpose((2, 0, 1)) return &#123;'image': torch.from_numpy(image), 'lable': torch.from_numpy(lable)&#125; 定义好callable类之后，通过torchvision.transforms.Compose将上述三个类结合在一起，传递给AnimalData类中的transform参数即可。 1234trsm = transforms.Compose([Rescale(256), RandomCrop(224), ToTensor()])data = AnimalData('./all', transform=trsm) Iterating through the dataset上一节中得到data实例之后可以通过for循环来一个一个读取数据，现在这是效率低下的。torch.utils.data.DadaLoader类解决了上述问题。其主要有如下特点： Batching the data Shuffling the data Load the data in parallel using multiprocessing workers. 实现起来也很简单： 12345dataloader = DataLoader(data, batch_size=4, shuffle=True, num_workers=4)for i_batch, bach_data in enumerate(dataloader): print i_batch print bach_data['image'].size() print bach_data['lable'] 第二种torchvisionpytorch几乎将上述所有工作都封装起来供我们使用，其中一个工具就是torchvision.datasets.ImageFolder,用于加载用户自定义的数据，要求我们的数据要有如下结构： root/ants/xxx.png root/ants/xxy.jpeg root/ants/xxz.png . . . root/bees/123.jpg root/bees/nsdf3.png root/bees/asd932_.png torchvision.transforms中也封装了各种各样的数据处理的工具，如Resize, ToTensor等等功能供我们使用。 加载数据代码如下： 12345678910111213141516171819202122232425from torchvision import transforms, utilsfrom torchvision import datasetsimport torchimport matplotlib.pyplot as plt train_data = datasets.ImageFolder('./data1', transform=transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor()]))train_loader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True, ) print len(train_loader)for i_batch, img in enumerate(train_loader): if i_batch == 0: print(img[1]) fig = plt.figure() grid = utils.make_grid(img[0]) plt.imshow(grid.numpy().transpose((1, 2, 0))) plt.show() break 附录最后欣赏一段torchvision源码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136# vision/torchvision/datasets/folder.pyimport torch.utils.data as datafrom PIL import Imageimport osimport os.pathIMG_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm']def is_image_file(filename): \"\"\"Checks if a file is an image. Args: filename (string): path to a file Returns: bool: True if the filename ends with a known image extension \"\"\" filename_lower = filename.lower() return any(filename_lower.endswith(ext) for ext in IMG_EXTENSIONS)def find_classes(dir): classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))] classes.sort() class_to_idx = &#123;classes[i]: i for i in range(len(classes))&#125; return classes, class_to_idxdef make_dataset(dir, class_to_idx): images = [] dir = os.path.expanduser(dir) for target in sorted(os.listdir(dir)): d = os.path.join(dir, target) if not os.path.isdir(d): continue for root, _, fnames in sorted(os.walk(d)): for fname in sorted(fnames): if is_image_file(fname): path = os.path.join(root, fname) item = (path, class_to_idx[target]) images.append(item) return imagesdef pil_loader(path): # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835) with open(path, 'rb') as f: img = Image.open(f) return img.convert('RGB')def accimage_loader(path): import accimage try: return accimage.Image(path) except IOError: # Potentially a decoding problem, fall back to PIL.Image return pil_loader(path)def default_loader(path): from torchvision import get_image_backend if get_image_backend() == 'accimage': return accimage_loader(path) else: return pil_loader(path)class ImageFolder(data.Dataset): \"\"\"A generic data loader where the images are arranged in this way: :: root/dog/xxx.png root/dog/xxy.png root/dog/xxz.png root/cat/123.png root/cat/nsdf3.png root/cat/asd932_.png Args: root (string): Root directory path. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. loader (callable, optional): A function to load an image given its path. Attributes: classes (list): List of the class names. class_to_idx (dict): Dict with items (class_name, class_index). imgs (list): List of (image path, class_index) tuples \"\"\" def __init__(self, root, transform=None, target_transform=None, loader=default_loader): classes, class_to_idx = find_classes(root) imgs = make_dataset(root, class_to_idx) if len(imgs) == 0: raise(RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\" \"Supported image extensions are: \" + \",\".join(IMG_EXTENSIONS))) self.root = root self.imgs = imgs self.classes = classes self.class_to_idx = class_to_idx self.transform = transform self.target_transform = target_transform self.loader = loader def __getitem__(self, index): \"\"\" Args: index (int): Index Returns: tuple: (image, target) where target is class_index of the target class. \"\"\" path, target = self.imgs[index] img = self.loader(path) if self.transform is not None: img = self.transform(img) if self.target_transform is not None: target = self.target_transform(target) return img, target def __len__(self): return len(self.imgs) def __repr__(self): fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n' fmt_str += ' Number of datapoints: &#123;&#125;\\n'.format(self.__len__()) fmt_str += ' Root Location: &#123;&#125;\\n'.format(self.root) tmp = ' Transforms (if any): ' fmt_str += '&#123;0&#125;&#123;1&#125;\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp))) tmp = ' Target Transforms (if any): ' fmt_str += '&#123;0&#125;&#123;1&#125;'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp))) return fmt_str 参考博客：https://www.jianshu.com/p/220357ca3342","categories":[{"name":"广东工业-物体检测","slug":"广东工业-物体检测","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/"},{"name":"函数分析","slug":"广东工业-物体检测/函数分析","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/函数分析/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/pytorch/"}]},{"title":"InceptionV4","slug":"CNN/InceptionV4","date":"2019-05-17T13:23:53.000Z","updated":"2019-05-17T14:48:20.231Z","comments":true,"path":"2019/05/17/CNN/InceptionV4/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/17/CNN/InceptionV4/","excerpt":"论文：《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, 3.08% test error 》 The paper address：http://arxiv.org/abs/1602.07261","text":"论文：《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, 3.08% test error 》 The paper address：http://arxiv.org/abs/1602.07261 引言google认为他们之前在改变架构选择上相对保守：网络结构的改变只局限于独立的网络组件范围内，从而保持剩下模型稳定。而如果不改变之前的这种原则，那么生成的模型将会比需要的还复杂（即过头了）。在这里，他们决定抛弃之前那个设计原则，对不同尺度的网格都采用统一的inception模块 。 在下面的网络结构图中：所有后面不带V的卷积，用的都是same-padded，也就是输出的网格大小等于输入网格的大小（如vgg的卷积一样）；带V的使用的是valid-padded，表示输出的网格尺寸是会逐步减小的（如lenet5的卷积一样）。 在下面的结构图中，每一个inception模块中都有一个1∗1的没有激活层的卷积层，用来扩展通道数，从而补偿因为inception模块导致的维度约间。其中Inception-ResNet-V1的结果与Inception v3相当；Inception-ResNet-V1与Inception v4结果差不多，不过实际过程中Inception v4会明显慢于Inception-ResNet-v2，这也许是因为层数太多了。且在Inception-ResNet结构中，只在传统层的上面使用BN层，而不在合并层上使用BN，虽然处处使用BN是有好处，不过更希望能够将一个完整的组件放入单独的GPU中。因为具有大量激活单元的层会占用过多的显存，所以希望这些地方丢弃BN，从而总体增加Inception模块的数量。使得不需要去解决计算资源和模块什么的权衡问题。 InceptionV4网络结构图如下： 子模块如下图所示： Stem模块： Inception模块 Reduction模块 Inception-ResNet-v1网络结构如图所示： 子模块： Stem模块: Inception模块： Reduction模块： Reduction-A结构 与Inception V4相同的是，在Inception-ResNet-A及Inception-ResNet-B后分别添加了Reduction-A和Reduction-B，其中Reduction-A的结构与Inception V4的一致，Reduction-B的结构如下。 Inception-ResNet-v2Inception-ResNet-v2的整体框架和Inception-ResNet-v1的一致。 只不过v2的计算量更加expensive些，它的stem结构与Inception V4的相同，Reduction-A与v1的相同，Inception-ResNet-A、Inception-ResNet-B、Inception-ResNet-C和Reduction-B的结构与v1的类似，只不过输出的channel数量更多。总的来说，Inception-ResNet-v2与Inception V4的相近。在Inception-ResNet-v2中同样使用了drop out和BN。 比较Inception-ResNet的stem模块和Reduction-B模块也略微不同。Inception-ResNet-v1和Inception-ResNet-v2主要在于Reduction-A结构不同：其中k,l,m,n表示filter bank size。 对残差模块的缩放我们发现，如果滤波器数量超过1000，残差网络开始出现不稳定，同时网络会在训练过程早期便会出现“死亡”，意即经过成千上万次迭代，在平均池化（average pooling） 之前的层开始只生成0。通过降低学习率，或增加额外的batch-normalizatioin都无法避免这种状况。我们发现，在将残差模块添加到activation激活层之前，对其进行放缩能够稳定训练。通常来说，我们将残差放缩因子定在0.1到0.3。 注：He在训练Residual Net时也发现这个问题，提出了“two phase”训练。首先“warm up”，使用较小的学习率。接着再使用较大的学习率。 缩放模块仅仅适用于最后的线性激活。 结论（1）Inception-ResNet-v1：混合Inception版本，它的计算效率同Inception-v3；（2）Inception-ResNet-v2：更加昂贵的混合Inception版本，同明显改善了识别性能；（3）Inception-v4：没有残差链接的纯净Inception变种，性能如同Inception-ResNet-v2我们研究了引入残差连接如何显著的提高inception网络的训练速度。而且仅仅凭借增加的模型尺寸，我们的最新的模型（带和不带残差连接）都优于我们以前的网络。","categories":[{"name":"CNN","slug":"CNN","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/CNN/"}],"tags":[{"name":"Inception","slug":"Inception","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/Inception/"}]},{"title":"InceptionV2、V3","slug":"CNN/InceptionV2-V3","date":"2019-05-17T13:02:01.000Z","updated":"2019-05-17T13:25:56.799Z","comments":true,"path":"2019/05/17/CNN/InceptionV2-V3/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/17/CNN/InceptionV2-V3/","excerpt":"论文：V2《Batch Normalization: Accelerating Deep Network Training by Reducing Internal 》 地址：http://arxiv.org/abs/1502.03167 论文：v3《Rethinking the Inception Architecture for Computer Vision, 3.5% test error 》 地址：http://arxiv.org/abs/1512.00567","text":"论文：V2《Batch Normalization: Accelerating Deep Network Training by Reducing Internal 》 地址：http://arxiv.org/abs/1502.03167 论文：v3《Rethinking the Inception Architecture for Computer Vision, 3.5% test error 》 地址：http://arxiv.org/abs/1512.00567 创新点InceptionV2 加入了BN层，使得每一层的特征高斯分布 将一个5x5的卷积核分解成两个3x3的卷积核 InceptionV3 采用不对称方式卷积，即将一个3x3的卷积分解成1x3与3x1的卷积代替 InceptionV2、V3卷积分解（Factorizing Convolutions）更大尺寸的卷积核可以带来更大的感受野，可以提取更多的特征，但是伴随着参数的增加，所以作者用用两个3x3的卷积核代替一个5x5的卷积核。这样可以得到与5x5相同的感受野，但是可以降低参数。如下图所示： 基于以上的想法作者将3x3的卷积核再次进行分解，如下图所示，用3个3x1取代3x3卷积： 因此，任意nxn的卷积都可以通过1xn卷积后接nx1卷积来替代。GoogLeNet团队发现在网络的前期使用这种分解效果并不好，在中度大小的特征图（feature map）上使用效果才会更好（特征图大小建议在12到20之间）。 v3一个最重要的改进是分解（Factorization），将7x7分解成两个一维的卷积（1x7,7x1），3x3也是一样（1x3,3x1），这样的好处，既可以加速计算（多余的计算能力可以用来加深网络），又可以将1个conv拆成2个conv，使得网络深度进一步增加，增加了网络的非线性（每增加一层都要进行ReLU） ，还有值得注意的地方是网络输入从224x224变为了299x299，更加精细设计了35x35/17x17/8x8的模块。 降低特征图大小一般情况下，如果想让图像缩小，可以有如下两种方式： 先池化再作Inception卷积，或者先作Inception卷积再作池化。但是方法一（左图）先作pooling（池化）会导致特征表示遇到瓶颈（特征缺失），方法二（右图）是正常的缩小，但计算量很大。为了同时保持特征表示且降低计算量，将网络结构改为下图，使用两个并行化的模块来降低计算量（卷积、池化并行执行，再进行合并）","categories":[{"name":"CNN","slug":"CNN","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/CNN/"}],"tags":[{"name":"Inception","slug":"Inception","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/Inception/"}]},{"title":"InceptionV1","slug":"CNN/InceptionV1","date":"2019-05-17T12:34:25.000Z","updated":"2019-05-17T13:03:50.841Z","comments":true,"path":"2019/05/17/CNN/InceptionV1/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/17/CNN/InceptionV1/","excerpt":"论文：《Going Deeper with Convolutions, 6.67% test error 》 地址：http://arxiv.org/abs/1409.4842","text":"论文：《Going Deeper with Convolutions, 6.67% test error 》 地址：http://arxiv.org/abs/1409.4842 概述直到GoogLeNet人们都在认为要想得到更高的性能是增加网络的深度与宽度（网络的层数与神经元数目），但是这样出现的问题就是: 若数据集有限，则参数过多容易出现过拟合现象 网络的计算复杂的增加，难以应用 随着网络层数的增加容易出现梯度消失的现象，难以优化模型 为了解决以上问题Inception出现，虽然网络深度与宽度增加，但是参数减少可以有效的克服上述的问题 Inception V1Inception v1的网络，将1x1，3x3，5x5的conv和3x3的pooling，堆叠在一起，一方面增加了网络的width，另一方面增加了网络对尺度的适应性； 图1：原始论文中的模型 图2：改进后的模型 图1是引用的原式论文的模型，所有的卷积操作与pooling操作都是在上一层数输出上来做的，这样做可以让模型自己选择合适的卷积操作，若输入的特征图维度很大时，5x5的卷积核参数量还是很大，所以作者在原式的结构上加上了1x1的卷积核，这样操作可以降低特征图的维度从而降低计算量，这就是InceptionV1结构图。 1x1卷积核的作用：1x1卷积的主要目的是为了减少维度，还用于修正线性激活（ReLU） 。 maxpooling的作用：以减少空间大小，降低过度拟合。 下图给出了GoogLeNet的结构图： 对上图说明如下： （1）GoogLeNet采用了模块化的结构（Inception结构），方便增添和修改； （2）网络最后采用了average pooling（平均池化）来代替全连接层，该想法来自NIN（Network in Network），事实证明这样可以将准确率提高0.6%。但是，实际在最后还是加了一个全连接层，主要是为了方便对输出进行灵活调整； （3）虽然移除了全连接，但是网络中依然使用了Dropout ; （4）为了避免梯度消失，网络额外增加了2个辅助的softmax用于向前传导梯度（辅助分类器）。辅助分类器是将中间某一层的输出用作分类，并按一个较小的权重（0.3）加到最终分类结果中，这样相当于做了模型融合，同时给网络增加了反向传播的梯度信号，也提供了额外的正则化，对于整个网络的训练很有裨益。而在实际测试的时候，这两个额外的softmax会被去掉。","categories":[{"name":"CNN","slug":"CNN","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/CNN/"}],"tags":[{"name":"Inception","slug":"Inception","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/Inception/"}]},{"title":"HetConv","slug":"CNN/HetConv","date":"2019-05-17T10:51:05.000Z","updated":"2019-05-17T12:24:09.621Z","comments":true,"path":"2019/05/17/CNN/HetConv/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/17/CNN/HetConv/","excerpt":"&lt;HetConv: Heterogeneous Kernel-Based Convolutions for Deep CNNs 论文:《 Heterogeneous Kernel-Based Convolutions for Deep CNNs 》 地址：https://arxiv.org/abs/1903.04120 源码：https://github.com/sxpro/HetConvolution2d_pytorch","text":"&lt;HetConv: Heterogeneous Kernel-Based Convolutions for Deep CNNs 论文:《 Heterogeneous Kernel-Based Convolutions for Deep CNNs 》 地址：https://arxiv.org/abs/1903.04120 源码：https://github.com/sxpro/HetConvolution2d_pytorch 引言​ 提出了新的神经网络架构，其中的而卷积操作是采用的异构核，该架构与VGG、Resnet等采用标准的卷积核相比，在不失精度的情况下计算量(FLPOs)可以减少3-8倍，与采用深度可卷积的架构相比计算量也可以得到减小。 ​ 目前卷积过滤器可以大致有：深度卷积(DW)、逐点卷积(PW)与群卷积(GW)，这些卷积方式在不降低精度的同时，可以降低计算量，大多数都是运用在轻量级的神经网络架构上。DW、PW、GW可以理解成同构卷积(Homogeneous Convolution )，即在卷积核的深度方向观察，卷积核的大小都是一致的。作者提出了一种异构卷积(Heterogeneous Convolution )，即在深度方向卷积核的大小不是一致的，比如一个标准的卷积核的长宽深度是3，3，128，但是异构卷积核可以是：前64是3x3大小的卷积核，后64是1x1的卷积核。异构卷积核可以有效的降低模型的计算量即FLOPs。 创新点 一个高效的异构卷积HetConv。 可以与大多数的轻量级网络相比可以保证0延时。 异构卷积HetConv结构比较​ 在本论文中，我们提出了一种包含异构卷积核（比如一些核的大小是 3×3，其余的是 1×1）的全新过滤器/卷积（HetConv），可以在保证原始模型同等准确度的同时降低 FLOPs。图 1 和图 2 展示了标准过滤器与 HetConv 过滤器之间的差异。 图 1：标准卷积过滤器（同构）和异构卷积过滤器（HetConv）之间的差异。其中 M 是指输入深度（输入通道的数量），P 是指 part（控制卷积过滤器中不同类型的核的数量）。在 M 个核中，M/P 个核的大小是 3×3，其余的都是 1×1。 图 2：我们提出的卷积过滤器（HetConv）与其它高效卷积过滤器的比较。我们的异构过滤器的延迟为零，其它（GWC+PWC 或 DWC+PWC）则有一个单元的延迟。 计算量比较 假设输出特征图的大小是D，卷积核的大小是K，输入特征图的通道数为M，输出特征图的通道数为N。 标准卷积核： $FLs_1=DDMNK*K$ 深度可分离卷积(DW+PW): $FLs_2=DDKKM+DDM*N$ 异构卷积HetConv： $PFs_3=DDKK(M/P)N+DDN(1-1/P)*M$ 异构卷积和与标准卷积核比较： $R=1/P+(1-1/P)/K^2$ 一般K的取值为1，3，5，，所以作者实验结构与VGG比较计算量可以减少8倍左右 异构卷积与PW+DW： $R = (K^2N1/P+N(1-1/P))/(K^2+N)$ K一般取值为1，3，5与N相比可以忽略，最后上式可以化简为$1/P$，从式中可以看出计算量可以减少很明显。 异构卷积核的内部分布 图 3：L 层处的卷积过滤器：我们提出的使用异构核的卷积过滤器（HetConv）。图中可以看到，每个通道都由 3×3 和 1×1 大小的异构核构成。在标准卷积过滤器中用 1×1 核替代 3×3 核能够在保持准确度的同时极大降低 FLOPs。一个特定层的过滤器排列成移位形式（即如果第一个过滤器从首个位置开始 3×3 核，则第二个过滤器从第二个位置开始 3×3 核，以此类推）。 延时比较 ​ 图 4：上图比较了不同类型的卷积的延迟情况。 论文的比较部分请自行比较 代码分析123456789101112131415161718192021222324252627282930class HetConv2d(nn.Module): def __init__(self, in_feats, out_feats, groups=1, p=2): super(HetConv2d_v2, self).__init__() if in_feats % groups != 0: raise ValueError('in_channels must be divisible by groups') self.in_feats = in_feats self.out_feats = out_feats self.groups = groups self.blocks = nn.ModuleList() for i in range(out_feats): self.blocks.append(self.make_HetConv2d(i, p)) def make_HetConv2d(self, n, p): layers = nn.ModuleList() for i in range(self.in_feats): if ((i - n) % (p)) == 0: layers.append(nn.Conv2d(1, 1, 3, 1, 1)) else: layers.append(nn.Conv2d(1, 1, 1, 1, 0)) return layers def forward(self, x): out = [] for i in range(0, self.out_feats): out_ = self.blocks[i][0](x[:, 0: 1, :, :]) for j in range(1, self.in_feats): out_ += self.blocks[i][j](x[:, j:j + 1, :, :]) out.append(out_) return torch.cat(out, 1)","categories":[{"name":"CNN","slug":"CNN","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/CNN/"}],"tags":[{"name":"HetConv","slug":"HetConv","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/HetConv/"}]},{"title":"iterrows对dataframe进行遍历","slug":"广东工业-物体检测/iterrows对dataframe进行遍历","date":"2019-05-17T08:55:45.000Z","updated":"2019-05-17T09:55:43.850Z","comments":true,"path":"2019/05/17/广东工业-物体检测/iterrows对dataframe进行遍历/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/17/广东工业-物体检测/iterrows对dataframe进行遍历/","excerpt":"","text":"定义一个数据 1a = pd.DataFrame([[1,2,3,4],[4,5,6,9],[7,8,9,50],[5,9,5,3]]) 现在对这个表格进行遍历，一般写法为： 1234for index,row in a.iterrows(): print(index) print(row)​ 00 11 22 33 4Name: 0, dtype: int6410 41 52 63 9Name: 1, dtype: int6420 71 82 93 50Name: 2, dtype: int6430 51 92 53 3Name: 3, dtype: int64​","categories":[{"name":"广东工业-物体检测","slug":"广东工业-物体检测","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/"},{"name":"函数分析","slug":"广东工业-物体检测/函数分析","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/函数分析/"}],"tags":[{"name":"python","slug":"python","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/python/"}]},{"title":"图片归一化","slug":"广东工业-物体检测/图片归一化","date":"2019-05-17T08:16:12.000Z","updated":"2019-05-17T09:55:58.300Z","comments":true,"path":"2019/05/17/广东工业-物体检测/图片归一化/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/17/广东工业-物体检测/图片归一化/","excerpt":"","text":"对Tensor进行变换class torchvision.transforms.Normalize(mean, std)给定均值：(R,G,B) 方差：（R，G，B），将会把Tensor正则化。即：Normalized_image=(image-mean)/std。 12# 图片归一化，由于采用ImageNet预训练网络，因此这里直接采用ImageNet网络的参数normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])","categories":[{"name":"广东工业-物体检测","slug":"广东工业-物体检测","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/"},{"name":"函数分析","slug":"广东工业-物体检测/函数分析","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/函数分析/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/pytorch/"}]},{"title":"train_test_split","slug":"广东工业-物体检测/train-test-split","date":"2019-05-17T08:03:22.000Z","updated":"2019-05-17T08:12:09.724Z","comments":true,"path":"2019/05/17/广东工业-物体检测/train-test-split/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/17/广东工业-物体检测/train-test-split/","excerpt":"","text":"在机器学习中，我们通常将原始数据按照比例分割为“测试集”和“训练集” ，从 sklearn.model_selection 中调用train_test_split 函数。 简单用法如下： 1234567X_train,X_test, y_train, y_test = sklearn.model_selection.train_test_split( train_data, train_target, test_size=0.4, random_state=666, stratify=y_train) train_data：所要划分的样本特征集 train_target：所要划分的样本结果 test_size：样本占比，如果是整数的话就是样本的数量 random_state：是随机数的种子。 随机数种子：其实就是该组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。比如你每次都填1，其他参数一样的情况下你得到的随机数组是一样的。但填0或不填，每次都会不一样。 stratify是为了保持split前类的分布。 比如有100个数据，80个属于A类，20个属于B类。如果train_test_split(… test_size=0.25, stratify = y_all), 那么split之后数据如下： training: 75个数据，其中60个属于A类，15个属于B类。 testing: 25个数据，其中20个属于A类，5个属于B类。 用了stratify参数，training集和testing集的类的比例是 A：B= 4：1，等同于split前的比例（80：20）。通常在这种类分布不平衡的情况下会用到stratify。 将stratify=X就是按照X中的比例分配 将stratify=y就是按照y中的比例分配","categories":[{"name":"广东工业-物体检测","slug":"广东工业-物体检测","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/"},{"name":"函数分析","slug":"广东工业-物体检测/函数分析","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/函数分析/"}],"tags":[{"name":"sklearn","slug":"sklearn","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/sklearn/"}]},{"title":"数据并行","slug":"广东工业-物体检测/数据并行","date":"2019-05-17T07:50:44.000Z","updated":"2019-05-17T07:56:28.727Z","comments":true,"path":"2019/05/17/广东工业-物体检测/数据并行/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/17/广东工业-物体检测/数据并行/","excerpt":"","text":"mdoel = torch.nn.DataParallel(model) 若设备有多个GPU可以使用，为了经可能充分的利用资源，可以将数据并行计算 12345678model = Model(input_size, output_size)if torch.cuda.device_count() &gt; 1: print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\") # dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs model = nn.DataParallel(model)if torch.cuda.is_available(): model.cuda() 若设备就一个GPU直接调用cuda()函数 12model = Model(input_size, output_size)model.cuda()","categories":[{"name":"广东工业-物体检测","slug":"广东工业-物体检测","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/"},{"name":"函数分析","slug":"广东工业-物体检测/函数分析","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/函数分析/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/pytorch/"}]},{"title":"os.path用法","slug":"广东工业-物体检测/os-path用法","date":"2019-05-17T07:12:26.000Z","updated":"2019-05-17T07:26:49.728Z","comments":true,"path":"2019/05/17/广东工业-物体检测/os-path用法/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/17/广东工业-物体检测/os-path用法/","excerpt":"","text":"os.path.basename(path) 返回的是path路径下的文件名 1234os.path.basename('G:\\guangdong\\scr\\gen_label_csv.py')#gen_label_csv.pyos.path.basename('G:\\guangdong\\scr')#scr这边默认是将scr当作文件处理 其他的os.path用法请参考：https://docs.python.org/3/library/os.path.html","categories":[{"name":"广东工业-物体检测","slug":"广东工业-物体检测","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/"},{"name":"函数分析","slug":"广东工业-物体检测/函数分析","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/广东工业-物体检测/函数分析/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/pytorch/"}]},{"title":"随机种子","slug":"pytorch/随机种子","date":"2019-05-17T06:49:06.000Z","updated":"2019-05-30T14:25:42.223Z","comments":true,"path":"2019/05/17/pytorch/随机种子/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/17/pytorch/随机种子/","excerpt":"1234np.random.seed(666)torch.manual_seed(666)torch.cuda.manual_seed_all(666)random.seed(666)","text":"1234np.random.seed(666)torch.manual_seed(666)torch.cuda.manual_seed_all(666)random.seed(666) np.random.seed()random.seed(666)作用：使得随机数据可预测，即只要seed的值一样，后续生成的随机数都一样。 123456789numpy.random.seed(0) numpy.random.rand(4) #array([ 0.55, 0.72, 0.6 , 0.54]) numpy.random.seed(0) numpy.random.rand(4) #array([ 0.55, 0.72, 0.6 , 0.54]) torch.manual_seed()&amp;torch.cuda.manual_seed_all() torch.manual_seed(args.seed)#为CPU设置种子用于生成随机数，以使得结果是确定的 torch.cuda.manual_seed(args.seed)#为当前GPU设置随机种子；如果使用多个GPU，应该使用 torch.cuda.manual_seed_all()为所有的GPU设置种子。","categories":[{"name":"pytorch","slug":"pytorch","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/pytorch/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/pytorch/"}]},{"title":"MobileNetV1","slug":"CNN/MobileNetV1","date":"2019-05-10T06:10:18.000Z","updated":"2019-05-17T12:26:39.238Z","comments":true,"path":"2019/05/10/CNN/MobileNetV1/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/10/CNN/MobileNetV1/","excerpt":"","text":"MobileNetV1 《MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications 》 The paper address：https://arxiv.org/abs/1704.04861 The coding address：https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.py WWH（What&amp;Why&amp;How）​ MobileNet描述了一个高效的网络架构，可以构建一个非常小、低延时、满足嵌入式设备的要求。模型的大小显著下降，但是实际效果很好。 Innovation Points 深度可分离卷积（Depthwise Separable Convolution） 宽度因子（Width Multiplier）&amp; 分辨率因子（Resolution Multiplier） Depthwise Separable Convolution 深度可分离卷积：将普通卷积分成深度卷积（DW）与逐点卷积（PW），这样做可大幅度降低参数量与计算量。 问题描述 Depthwise Separable Convolution与普通卷积的输入与输出均相同，中间过程不同。 输入： ，其中 为原始图片尺寸， 为输入channel数量。 输出： ，其中 为输出图片尺寸， 为输出channel数量。 普通卷积 卷积核： 卷积核如下图所示： 深度可分离卷积深度卷积 输入： ，输出 卷积核：M个 。 理解： 将特征图看成是M张二维的特征图即M张 的特征图 将M张特征图分别与 的卷积核卷积 也就是对channel进行分解 逐点卷积 输入 ，输出 。 卷积核： 。 计算量对比分子为Depthwise Separable卷积，分母为普通卷积。 深度可分离卷积与标准卷积相比：当采用3x3的卷积核，计算量可以减少9倍左右 结构 注意：如果是需要下采样，则在第一个深度卷积上取步长为2. 实现代码1234567891011121314151617class Block(nn.Module): '''Depthwise conv + Pointwise conv''' def __init__(self, in_planes, out_planes, stride=1): super(Block, self).__init__() self.conv1 = nn.Conv2d\\ (in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=in_planes, bias=False) self.bn1 = nn.BatchNorm2d(in_planes) self.conv2 = nn.Conv2d\\ (in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False) self.bn2 = nn.BatchNorm2d(out_planes) def forward(self, x): out = F.relu(self.bn1(self.conv1(x))) out = F.relu(self.bn2(self.conv2(out))) return out MobileNetV1网络网络结构 网络介绍 MobileNet优化的重点放在优化延迟(latency)，兼顾模型大小。 网络参数、计算量分布： MobileNet中大多数计算量和参数都在1*1的卷积中，可以使用高度优化的。 由于模型较小，所以可以减少使用正则化，因为模型小不容易过拟合。 Width Multiplier&amp;Resolution MultiplierWidth Multiplier, ： 用于控制输入和输出的通道数，即输入通道从M 变为αM ,输出通道从N变为αN。 对于depthwise卷积操作，其计算量为： 可设置α∈(0,1]，通常取1,0.75,0.5和0.25 。 Resolution Multiplier， 用于控制输入和内部层表示。即用分辨率因子控制输入的分辨率，该参数用于控制特征图的宽和高 。 对于depthwise卷积操作，其计算量为： 可设置ρ∈(0,1]，通常设置输入分辨率为224,192,160和128","categories":[{"name":"CNN","slug":"CNN","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/CNN/"}],"tags":[{"name":"MobileNet","slug":"MobileNet","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/MobileNet/"}]},{"title":"MobileNetV2","slug":"CNN/MobileNetV2","date":"2019-05-10T06:10:18.000Z","updated":"2019-05-17T12:23:37.154Z","comments":true,"path":"2019/05/10/CNN/MobileNetV2/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/05/10/CNN/MobileNetV2/","excerpt":"MobileNetV2 《Inverted Residuals and Linear Bottlenecks Mobile Networks for Classification, Detection and Segmentation 》 The paper address：https://128.84.21.199/abs/1801.04381 The coding address：https://github.com/miraclewkf/MobileNetV2-PyTorch","text":"MobileNetV2 《Inverted Residuals and Linear Bottlenecks Mobile Networks for Classification, Detection and Segmentation 》 The paper address：https://128.84.21.199/abs/1801.04381 The coding address：https://github.com/miraclewkf/MobileNetV2-PyTorch WWH（What&amp;Why&amp;How）​ MobileNetV2是对MobileNetV1的改进，同样式轻量级的神经网络。实现了分类/目标检测/语义分割对目标任务，将MobileNetV2作为SSD的基础网络设计目标检测模型SSDLite。可以使参数降低一个数量级，但是mPA无明显变化。 ​ MobileNet-V1 最大的特点就是采用depth-wise separable convolution来减少运算量以及参数量，而在网络结构上，没有采用shortcut的方式。 ResNet及DenseNet等一系列采用shortcut的网络的成功，表明了shortcut是个非常好的东西，作者希望引入shortcut到MobileNet中。但是将residual block运用到depth-wise separable convolution，会碰到如下两个问题： DWConv layer 提取特征限制于输入特征维度，若采用residual block，1x1 PW conv操作后会先将输入特征图压缩(一般压缩率为0.25)，再经行DW conv后提取的特征或更少。MobileNetV2是先经过1x1 的PW conv操作将特征图扩张（本文扩大6倍），这也就可以不受输入通道的限制，可以提取更多的特征。 深度可分离卷积的PW conv相对于是对上一层DWconv的压缩，PWconv后跟随的是ReLU，根据ReLU的性质，输入特征若为负数，经过激活层输入的特征全是0，本来特征已经经过压缩，这会进一步损失特征值；若输入特征是正数，经过激活层输出特征是还原始的输入值。根据这一特点改变激活函数用Linear bottlenecks 代替。 Innovation Points Inverted residuals Linear Bottlenecks Inverted residuals​ Inverted residuals，通常的residuals block是先经过一个1x1的Conv layer，把feature map的通道数“压”下来，再经过3x3 Conv layer，最后经过一个1x1 的Conv layer，将feature map 通道数再“扩张”回去。即先“压缩”，最后“扩张”回去。 而 inverted residuals就是 先“扩张”，最后“压缩”。如下图所示。 ​ inverted residuals 可以认为是 residual block 的拓展。在 0&lt;t&lt;1，其实就是标准的残差模块。论文中 t 大部分为 6，呈现梭子的外形，而传统残差设计是沙漏形状。 Linear Bottlenecks​ 为了避免ReLU对特征的破坏，在residual block sum之前的PW conv后的激活层该成Linear Bottlenecks。在MobileNet V1中除了引入depthwise separable convolution代替传统的卷积，还做了一个实验是用width multiplier参数来做模型通道的缩减，相当于给模型“瘦身”，这样特征信息就能更集中在缩减后的通道中，但是如果此时加上一个非线性激活层，比如ReLU，就会有较大的信息丢失，因此为了减少信息丢失，就有了文中的linear bottleneck，意思就是bottleneck的输出不接非线性激活层，所以是linear，原因是： 对于ReLU层输出的非零值而言，ReLU层起到的就是一个线性变换的作用，这个从ReLU的曲线就能看出来。 ReLU层可以保留input manifold的信息，但是只有当input manifold是输入空间的一个低维子空间时才有效。 作者经过实验证明：当把原始输入维度增加到15或30后再作为ReLU的输入，输出恢复到原始维度后基本不会丢失太多的输入信息；相比之下如果原始输入维度只增加到2或3后再作为ReLU的输入，输出恢复到原始维度后信息丢失较多。因此在MobileNet V2中，执行降维的卷积层后面不会接类似ReLU这样的非线性激活层，也就是linear bottleneck的含义。 比较MobileNetV1和MobileNetV2的区别： 1、相同点： ​ 都采用 Depth-wise (DW) 卷积搭配 Point-wise (PW) 卷积的方式来提特征。这两个操作合起来也被称为 Depth-wise Separable Convolution，之前在 Xception 中被广泛使用。这么做的好处是理论上可以成倍的减少卷积层的时间复杂度和空间复杂度。标准卷积的计算复杂度近似为 DW + PW 组合卷积的 K^2倍。 2、不同点：（Linear Bottleneck） V2 在 DW 卷积之前新加了一个 PW 卷积。这么做的原因，是因为 DW 卷积由于本身的计算特性决定它自己没有改变通道数的能力，上一层给它多少通道，它就只能输出多少通道。所以如果上一层给的通道数本身很少的话，DW 也只能很委屈的在低维空间提特征，因此效果不够好。现在 V2 为了改善这个问题，给每个 DW 之前都配备了一个 PW，专门用来升维，定义升维系数6 ，这样不管输入通道数C_in 是多是少，经过第一个 PW 升维之后，DW 都是在相对的更高维进行着辛勤工作的。 V2 去掉了第二个 PW 的激活函数。论文作者称其为 Linear Bottleneck。这么做的原因，是因为作者认为激活函数在高维空间能够有效的增加非线性，而在低维空间时则会破坏特征，不如线性的效果好。由于第二个 PW 的主要功能就是降维，因此按照上面的理论，降维之后就不宜再使用 ReLU6 了。 精度对比如下图。 ResNet和MobileNet-V2的对比 1、相同点： （1）MobileNet V2 借鉴 ResNet，都采用了 的模式。 （2）MobileNet V2 借鉴 ResNet，同样使用 Shortcut 将输出与输入相加（未在上式画出） 2、不同点：（Inverted Residual Block） ResNet 使用 标准卷积 提特征，MobileNet 始终使用 DW卷积 提特征。 ResNet 先降维 (0.25倍)、卷积、再升维，而 MobileNet V2 则是 先升维 (6倍)、卷积、再降维。直观的形象上来看，ResNet 的微结构是沙漏形，而 MobileNet V2 则是纺锤形，刚好相反。因此论文作者将 MobileNet V2 的结构称为 Inverted Residual Block。这么做也是因为使用DW卷积而作的适配，希望特征提取能够在高维进行。 精度对比图。 网络结构​ 论文提出的 MobileNetV2 模型结构容易理解，基本单元 bottleneck 就是 Inverted residuals 模块，所用到的 tricks 比如 Dwise，就是 Depthwise Separable Convolutions，即各通道分别卷积。表 3 所示的分类网络结构输入图像分辨率 224x224，输出是全卷积而非 softmax，k 就是识别目标的类别数目。 ​ MobileNetV2 的网络结构中，第 6 行 stride=2，会导致下面通道分辨率变成14x14，从表格看，这个一处应该有误。 ​ 特别的，针对stride=1 和stride=2，在block上有稍微不同，主要是为了与shortcut的维度匹配，因此，stride=2时，不采用shortcut。 具体如下图：","categories":[{"name":"CNN","slug":"CNN","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/CNN/"}],"tags":[{"name":"MobileNet","slug":"MobileNet","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/MobileNet/"}]},{"title":"数据清洗","slug":"机器学习/数据清洗","date":"2019-03-17T08:12:40.000Z","updated":"2019-05-17T06:43:30.400Z","comments":true,"path":"2019/03/17/机器学习/数据清洗/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/03/17/机器学习/数据清洗/","excerpt":"","text":"数据清洗预处理工具 关系型数据库或者Python 元数据与数据的特征查看 对数据有直观的了解，位置后的数据处理做准备 缺省值清洗 确定缺省值的范围 去除不需要的字段 填充缺省值内容（重要 ） 重新获取数据","categories":[{"name":"面试","slug":"面试","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/面试/"},{"name":"机器学习","slug":"面试/机器学习","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/面试/机器学习/"}],"tags":[]},{"title":"深度学习基础","slug":"深度学习/深度学习基础一","date":"2019-03-17T02:00:51.000Z","updated":"2019-05-30T14:16:10.785Z","comments":true,"path":"2019/03/17/深度学习/深度学习基础一/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/03/17/深度学习/深度学习基础一/","excerpt":"","text":"过拟合与欠拟合 欠拟合：指模型不能在训练数据集上获得足够低的训练误差 过拟合：指模型的训练误差与测试误差（泛化误差）上的误差过大 反映在评价指标上就是模型在训练集上表现良好，但是在测试集或者在新数据集上的表现一般（泛化能力差） 降低过拟合风险的方法 所有为了减小测试误差的策略统称为正则化方法，这些方法可能会议增大训练误差为代价。 数据增强 图像：平移、旋转、缩放 利用生成对抗网络生成新数据 NLP：利用机器翻译生成新数据 降低模型复杂度 神经网络：减少网络层、神经元个数 决策树：降低树的深度、剪枝 权值约束（添加正则化项） L1、L2正则化 集成学习 神经网络：Dropout 决策树：随机森林、GDBT 提前终止 降低欠拟合风险的方法 加入新的特征 交叉特征、多项式特征 升读学习：因子分解机、Deep-Crossing 增加模型复杂度 线性模型：添加高次项 神经网络：增加网络层数、神经元个数 减小正则化项的系数 添加正则化项是为了限制模型的学习能力，减小正则化项可以放宽这个限制 模型通常更加倾向于更大的权重，更大的权重可以使模型更好的拟合 反向传播算法反向传播的作用、目的、本质 概述：梯度下降法中需要利用损失函数对所有参数的梯度来寻找局部最小点，而反向传播算法就是用于计算梯度的算法，其本质就是利用链式求导法则对每个参数求偏导。 公式推导https://blog.csdn.net/lien0906/article/details/79193103 激活函数激活函数的作用——为什么使用非线性激活函数使用非线性激活函数的目的是为了像网络中加入非线性因素；增强网络的表示能力，解决线性网络存在的问题。 为什么加入非线性网络可以增强网络的表达能力呢——神经网络的万能近似定理 神经网络的万能近似定理认为震惊网络具有至少一个非线性隐含层，那么只要给网络足够数量的隐藏单元，他就可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的函数。 如果不使用非线性激活函数，那么每一层输出都是上一层输入的线性组合；此时无论网络有多少层，其整体也将是线性的，这会导致失去万能近似的性质。 但是部分层是纯线性是可以接受的，有助于减少网络的参数。 常见的激活函数整流线性单元ReLU ReLU通常是激活函数最好的默认选择 =\\max(0,z)) ReLU的拓展 ReLU及其扩展都是基于以下公式： 绝对值整流（absolute value rectification） 当a=-1，此时函数就是g(z)=|z| 渗漏整流线性单元 当a是一个很小的值，如a=0.001 参数化线性整流 将a作为一个可学习的参数 maxout单元 maxout单元进一步扩展了ReLU，他是一个可学习的K段函数，参数数量是普通全连接层的 k 倍 。 ####sigmoid与tanh sigmoid(z)，常记作 σ(z)； tanh(z) 的图像与 sigmoid(z) 大致相同，区别是值域为 (-1, 1) =\\frac{1}{1+\\exp(-z)}) 其他激活函数ReLU（优势）与sigmoid比较1.避免梯度消失 sigmoid函数在输入取绝对值非常大的正值或者负值的时候会出现饱和现象——在图像上表现的很平，此时函数会对输入的微小变化不敏感——从而造成梯度消失 ReLU的导数始终是一个常数——负半区为0，正半区为1，——所以不会发生梯度消失 2.减缓过拟合 ReLU在负半区输出为0，一旦神经元的激活值进入负半轴，那么该机或值就不会产生梯度/不会被训练，造成了网络的稀疏性——稀疏激活。 有利于减少参数的相互依赖，缓解过拟合问题的发生。 3.加速计算 ReLU的求导不涉及浮点运算 为什么 ReLU 不是全程可微/可导也能用于基于梯度的学习？ 在实现过程中通常返回左导数或者右导数的其中一个。 正则化Batch Normalizaton(批标准化) BN是一种正则化方法（较少泛化误差），主要的作用有： 加速网络的训练（缓解梯度消失，支持更大的学习率） 防止过拟合 降低参数初始化的要求 动机： 训练的本质就是学习数据的分布。如果训练数据与测试数据分布不同会降低模型的泛化能力，因此应该在训练前对所有输入数据做归一化处理。 在神经网络中，因为每个隐层的参数不同，会使下一层的输入发生变化，从而导致每一批数据的分布也发生变化，从而导致网络在每次迭代中需要拟合不同的数据分布，增加了网络的训练难度与过拟合的风险。 基本原理 BN方法会正对每一批数据，在网络的每一层输入之前增加归一化处理，是输入的均值为0，标准差为1，目的是将数据限制在统一的分布下。 具体就是，针对每一层的K个神经元，计算这一批数据在低K个神经元的均值与标准差，然后将归一化的值作为该神经元的激活函值。 BN可以看作在各层之间加入了一个新的计算层，对数据进行额外的约束，从而增加模型的泛化能力 但是BN也降低了模型的拟合能力，破坏了之前学习到的特征分布。 为了恢复原式的数据，BN引入了一个重构变化来还原最优的输入数据的分布 其中 γ 和 β 为可训练参数。 小结： 以上过程可归纳为一个 BN(x) 函数： ) 其中 &amp;=\\gamma\\boldsymbol{\\hat{x}_i}+\\beta\\&space;&amp;=\\gamma\\frac{\\boldsymbol{x_i}-\\boldsymbol{\\mathrm{E}[x_i]}}{\\sqrt{\\boldsymbol{\\mathrm{Var}[x_i]}+\\epsilon}}+\\beta&space;\\end{aligned}) 完整算法： L1/L2 范数正则化L1/L2范数的作用、异同相同点：限制了模型的学习能力——通过限制模型参数的规模，使模型偏好于权值较小的目标函数，防止过拟合。 不同点： L1正则化可以产生稀疏的权值举证，可以用于特征选择，同时一定程度上防止过拟合；L2正则化用于防止模型过拟合 L1正则化适用于特征之间有关联的情况；L2正则化适用于特征之间没有关联的情况。 为什么L1和L2正则化可以防止过拟合 L1、L2正则化回事模型偏好于更小的权重 更小的权重意味着更低的模型复杂度，添加L1和L2正则化相当于为模型添加了某种先验，限制参数的分布，从而降低了模型的复杂度。 模型的复杂度低意味着模型对于噪声与异常点的抗干扰能力增强，从而提高了模型的泛化能力——直观来说就是对训练数据的拟合刚刚号，不会过分拟合数据——奥卡姆剃刀定理 为什么L1正则化可以产生稀疏的权值，L2不会？ 对于目标函数添加范数正则化，训练时相当于在范数的约束下求目标函数J的最小值 带有L1 范数（左）和L2 范数（右）约束的二维图示 图中 J 与 L1 首次相交的点即是最优解。L1 在和每个坐标轴相交的地方都会有“顶点”出现，多维的情况下，这些顶点会更多；在顶点的位置就会产生稀疏的解。而 J 与这些“顶点”相交的机会远大于其他点，因此 L1 正则化会产生稀疏的解。 L2 不会产生“顶点”，因此 J 与 L2 相交的点具有稀疏性的概率就会变得非常小。 DropoutBagging集成学习 集成方法的主要想法是分别训练不同的模型，然后让所有的模型表决最终的输出。 集成学习凑效的原因是不同模型通常不会在测试集上产生相同的误差。 集成模型能至少与它的任何一成员 表现的一样好。如果成员的误差是独立的，集成将显著提升模型的性能。 Bagging是一种集成学习的策略——具体来说，Bagging涉及构造K个不同的数据集 每个数据集从原始数据集中重复采样构成，和原始数据集具有相同的样例，——这就意味着每个数据集有大概率缺少来自原始数据集的例子，但是也包含若干重复的例子 更具体的就是，如果采样所得到的训练集与原式的数据集大小相同，那么所得到的数据集中大概有原始数据集2/3的实例。 集成算法与神经网络 神经网络能够找到足够多的不同的解，意味着他们可以从模型平均中受益——即使所有的模型都在同一数据集上训练。 神经网络中随机初始化的差异、批训练数据的随机选择、超参数的差异等非确定性实现往往足以使得继承中的不同成员有部分独立的误差。 Dropout策略 简单来说，Dropout是通过共享参数提供了一种廉价的Bagging集成近似——Drpout相当于继承了从基础网络去除部分单元后形成的子网络。 通常隐层的采样率为0.5,输入的采样率为0.8，超参数也可以这样，但其采样率一般为1 权重比例推断规则 权重比例推断规则的目的是确保在测试时一个单元的期望总输入与在训练时该单元的期望总输入大致相同。 实践时，如果使用 0.5 的采样概率，权重比例规则相当于在训练结束后将权重乘 0.5，然后像平常一样使用模型；等价的，另一种方法是在训练时将单元的状态乘 2。 Dropout 与 Bagging 的不同 在 Bagging 的情况下，所有模型都是独立的；而在 Dropout 的情况下，所有模型共享参数，其中每个模型继承父神经网络参数的不同子集。 在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。","categories":[{"name":"面试","slug":"面试","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/面试/"},{"name":"深度学习","slug":"面试/深度学习","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/面试/深度学习/"}],"tags":[]},{"title":"改进快速排序","slug":"剑指offer/快速排序","date":"2019-03-17T02:00:51.000Z","updated":"2019-05-30T14:22:25.631Z","comments":true,"path":"2019/03/17/剑指offer/快速排序/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/03/17/剑指offer/快速排序/","excerpt":"","text":"##partition算法 （荷兰国旗问题）给定一个数组arr，和一个数num，请把小于num的数放在数组的左边，等于num的数放在数组的中间，大于num的数放在数组的右边。要求额外空间复杂度O(1)，时间复杂度O(N) 思想： 大于num的数放在数组的左边，小于num的数放在数组的右边，设当前指针为cur，小于区域的最后一个数指针为less，大于区域的第一个数为more。 解题： 要比较的数x=num，cur+1； 要比较的数x&lt;num,x与小于区域的下一个数交换，并且cur+1、less+1 要比较的数x&gt;num,x与大于区域的前一个数交换，并且more+1 代码： 123456789101112131415161718192021public static int[] pattena(int[] arr,int L,int R,int num)&#123; int less = L-1; int more = R+1; int cur = L; while (cur&lt;more)&#123; if(arr[cur]&lt;num)&#123; awap(arr,++less,cur++); &#125;else if(arr[cur]&gt;num)&#123; awap(arr,cur,--more); &#125;else &#123; cur++; &#125; &#125; //return new int[]&#123;less+1,more-1&#125;; return arr; &#125;public static void awap(int[] arr,int i ,int j)&#123; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp;&#125; 基于partition改进后的快速排序 随机快速排序的细节和复杂度分析可以用荷兰国旗问题来改进快速排序时间复杂度O(N*logN)，额外空间复杂度O(logN) 解题： 将数组的最后一个数最为num 小于num的放在左边，—&gt; 递归 大于num的放在右边，—&gt; 递归 代码实现： 12345678910111213141516171819202122232425262728293031public static int[] quickSort(int arr[],int L, int R) &#123; if(L&lt;R) &#123; int num = arr[arr.length - 1]; int[] idnex = partition(arr, L, R,num); quickSort(arr, L, idnex[0]-1); quickSort(arr, idnex[1]+1, R); &#125; return arr; &#125;public static int[] partition(int[] arr,int L,int R,int num)&#123; int less = L-1; int more = R; int cur = L; while (cur &lt; more) &#123; if (arr[cur]&lt;num) &#123; exchange(arr,++less,cur++); &#125; else if (arr[cur]&gt;num) &#123; exchange(arr,--more, cur); &#125;else &#123; cur++; &#125; &#125; exchange(arr, more, R); return new int[]&#123;less + 1, more &#125;;&#125;public static void exchange(int[] arr,int i, int j) &#123; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp;&#125;","categories":[{"name":"面试","slug":"面试","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/面试/"},{"name":"剑指offer","slug":"面试/剑指offer","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/面试/剑指offer/"}],"tags":[]},{"title":"CBAM","slug":"CNN/CBAM网络","date":"2019-03-16T06:10:18.000Z","updated":"2019-05-15T12:10:01.580Z","comments":true,"path":"2019/03/16/CNN/CBAM网络/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/03/16/CNN/CBAM网络/","excerpt":"摘要 CBAM特点就是轻量级通用模块 可以很好的加入到CNN结构中 端到端可训练的CNNs","text":"摘要 CBAM特点就是轻量级通用模块 可以很好的加入到CNN结构中 端到端可训练的CNNs 作者贡献 提出了一个高效的attention模块—-CBAM，该模块能够嵌入到目前的主流CNN网络结构中。 通过额外的分离实验证明了CBAM中attention的有效性。 在多个平台上（ImageNet-1K，MS COCO和VOC 2007）上证明了CBAM的性能提升。 Convolutional Block Attention Module给定输入特征为 对于一个中间层的feature map：，CBAM将会顺序推理出1维的channel attention map 以及2维的spatial attention map ，整个过程如下所示： %20%5Cotimes%20F) %20%5Cotimes%20F%5E%7B%27%7D) 其中为element-wise multiplication，首先将channel attention map与输入的feature map相乘得到，之后计算的spatial attention map，并将两者相乘得到最终的输出。下图为CBAM的示意图： Channel attention moduleeature map 的每个channel都被视为一个feature detector，channel attention主要关注于输入图片中什么(what)是有意义的。为了高效地计算channel attention，论文使用最大池化和平均池化对feature map在空间维度上进行压缩，得到两个不同的空间背景描述：和。使用由MLP组成的共享网络对这两个不同的空间背景描述进行计算得到channel attention map：。计算过程如下： 其中，，后使用了Relu作为激活函数。 Spatial attention module. 与channel attention不同，spatial attention主要关注于位置信息(where)。为了计算spatial attention，论文首先在channel的维度上使用最大池化和平均池化得到两个不同的特征描述和，然后使用concatenation将两个特征描述合并，并使用卷积操作生成spatial attention map %20%5Cin%20%5Cmathbb%20R_%7BH*W%7D)。计算过程如下： 其中，表示7*7的卷积层 下图为channel attention和spatial attention的示意图： 部分代码： 12345678910111213141516171819202122232425262728293031323334class ChannelAttention(nn.Module): def __init__(self, in_planes, ratio=16): super(ChannelAttention, self).__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) self.max_pool = nn.AdaptiveMaxPool2d(1) self.fc1 = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False) self.relu1 = nn.ReLU() self.fc2 = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False) self.sigmoid = nn.Sigmoid() def forward(self, x): avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x)))) max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x)))) out = avg_out + max_out return self.sigmoid(out)class SpatialAttention(nn.Module): def __init__(self, kernel_size=7): super(SpatialAttention, self).__init__() assert kernel_size in (3, 7), 'kernel size must be 3 or 7' padding = 3 if kernel_size == 7 else 1 self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False) self.sigmoid = nn.Sigmoid() def forward(self, x): avg_out = torch.mean(x, dim=1, keepdim=True) max_out, _ = torch.max(x, dim=1, keepdim=True) x = torch.cat([avg_out, max_out], dim=1) x = self.conv1(x) return self.sigmoid(x)","categories":[{"name":"CNN","slug":"CNN","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/CNN/"}],"tags":[]},{"title":"hexo基本使用","slug":"工具/hexo基本使用","date":"2019-03-14T11:26:33.000Z","updated":"2019-05-17T12:26:06.798Z","comments":true,"path":"2019/03/14/工具/hexo基本使用/","link":"","permalink":"https://dreamercv.github.io/dreamercv.github.io/2019/03/14/工具/hexo基本使用/","excerpt":"","text":"创建新的文章1hexo n 文章的名字 服务器的开启123hexo clearhexo ghexo s 服务器的关闭1ctrl + c 新建分类页与标签页 分类页 1hexo n page categories 标签页 1hexo n page tags 修改完章连接的样式 打开文件：..\\themes\\source\\css_common\\components\\post\\post.styl 12345678//输入代码.post-body p a &#123; color: #345; border-bottom: none; &amp;:hover&#123; color: red; &#125;&#125; 修改文章底部带#号的标签如： 打开文件 ：..\\themes\\next\\layout_macro\\post.swig 修改代码 123456789&lt;footer class=&quot;post-footer&quot;&gt; &#123;% if post.tags and post.tags.length and not is_index %&#125; &lt;div class=&quot;post-tags&quot;&gt; &#123;% for tag in post.tags %&#125; &lt;a href=&quot;&#123;&#123; url_for(tag.path) &#125;&#125;&quot; rel=&quot;tag&quot;&gt;&lt;i class=&quot;fa fa-google&quot;&gt;&lt;/i&gt; &#123;&#123; tag.name &#125;&#125;&lt;/a&gt; &#123;% endfor %&#125; &lt;/div&gt; &#123;% endif %&#125; ... 修改后的样式： 添加Valine评论功能增加搜索功能 打开主题、站点的配置文件 打开官网，找到第三方服务集成，找到搜索服务选择搜索样式： http://theme-next.iissnan.com/third-party-services.html#local-search 在git输入npm install hexo-generator-searchdb --save 编辑主题配置文件 123# Local searchlocal_search: enable: true 添加站点配置文件 12345search: path: search.xml field: post format: html limit: 10000 效果如图： 添加不蒜子统计功能 打开主题的配置文件 打开官网，找到第三方服务集成，找到统计服务经行修改 http://theme-next.iissnan.com/third-party-services.html#analytics-busuanzi 增加分享功能 打开主题的配置文件 找到baidushare修改代码 123baidushare: type: button baidushare: true 隐藏底部的强力驱动 注释掉代码段 1234567891011121314151617&lt;!--&#123;% if theme.footer.powered.enable %&#125; &lt;div class=\"powered-by\"&gt;&#123;# #&#125;&#123;&#123; __('footer.powered', next_url('https://hexo.io', 'Hexo', &#123;class: 'theme-link'&#125;)) &#125;&#125;&#123;# #&#125;&#123;% if theme.footer.powered.version %&#125; v&#123;&#123; hexo_env('version') &#125;&#125;&#123;% endif %&#125;&#123;# #&#125;&lt;/div&gt;&#123;% endif %&#125;&#123;% if theme.footer.powered.enable and theme.footer.theme.enable %&#125; &lt;span class=\"post-meta-divider\"&gt;|&lt;/span&gt;&#123;% endif %&#125;&#123;% if theme.footer.theme.enable %&#125; &lt;div class=\"theme-info\"&gt;&#123;# #&#125;&#123;&#123; __('footer.theme') &#125;&#125; – &#123;&#123; next_url('https://theme-next.org', 'NexT.' + theme.scheme, &#123;class: 'theme-link'&#125;) &#125;&#125;&#123;# #&#125;&#123;% if theme.footer.theme.version %&#125; v&#123;&#123; version &#125;&#125;&#123;% endif %&#125;&#123;##&#125;&lt;/div&gt;&#123;% endif %&#125;--&gt; 实现字数统计与阅读时长功能","categories":[{"name":"工具","slug":"工具","permalink":"https://dreamercv.github.io/dreamercv.github.io/categories/工具/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://dreamercv.github.io/dreamercv.github.io/tags/Hexo/"}]}]}