<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 深度学习基础 · Binge.van</title><meta name="description" content="深度学习基础 - 范斌"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/Bingevan/favicon.png"><link rel="stylesheet" href="/Bingevan/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://github.com/Bingevan/atom.xml" title="Binge.van"></head><body><div class="wrap"><header><a href="/Bingevan/" class="logo-link"><img src="/Bingevan/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/Bingevan/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/Bingevan/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="http://weibo.com/sunchongsheng" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="https://github.com/pinggod" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/Bingevan/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">深度学习基础</h1><div class="post-info">2019年3月17日</div><div class="post-content"><meta name="referrer" content="no-referrer"> 

<a id="more"></a> 
<h2 id="过拟合与欠拟合"><a href="#过拟合与欠拟合" class="headerlink" title="过拟合与欠拟合"></a>过拟合与欠拟合</h2><ul>
<li>欠拟合：指模型不能在训练数据集上获得足够低的训练误差</li>
<li>过拟合：指模型的训练误差与测试误差（泛化误差）上的误差过大<ul>
<li>反映在评价指标上就是模型在训练集上表现良好，但是在测试集或者在新数据集上的表现一般（泛化能力差）</li>
</ul>
</li>
</ul>
<h3 id="降低过拟合风险的方法"><a href="#降低过拟合风险的方法" class="headerlink" title="降低过拟合风险的方法"></a>降低过拟合风险的方法</h3><blockquote>
<p>所有为了减小测试误差的策略统称为<strong>正则化方法</strong>，这些方法可能会议增大训练误差为代价。</p>
</blockquote>
<ul>
<li>数据增强<ul>
<li>图像：平移、旋转、缩放</li>
<li>利用<strong>生成对抗网络</strong>生成新数据</li>
<li>NLP：利用机器翻译生成新数据</li>
</ul>
</li>
<li>降低模型复杂度<ul>
<li>神经网络：减少网络层、神经元个数</li>
<li>决策树：降低树的深度、剪枝</li>
</ul>
</li>
<li>权值约束（添加正则化项）<ul>
<li>L1、L2正则化</li>
</ul>
</li>
<li>集成学习<ul>
<li>神经网络：Dropout</li>
<li>决策树：随机森林、GDBT</li>
</ul>
</li>
<li>提前终止</li>
</ul>
<h3 id="降低欠拟合风险的方法"><a href="#降低欠拟合风险的方法" class="headerlink" title="降低欠拟合风险的方法"></a>降低欠拟合风险的方法</h3><ul>
<li>加入新的特征<ul>
<li>交叉特征、多项式特征</li>
<li>升读学习：因子分解机、Deep-Crossing</li>
</ul>
</li>
<li>增加模型复杂度<ul>
<li>线性模型：添加高次项</li>
<li>神经网络：增加网络层数、神经元个数</li>
</ul>
</li>
<li>减小正则化项的系数<ul>
<li>添加正则化项是为了限制模型的学习能力，减小正则化项可以放宽这个限制</li>
<li>模型通常更加倾向于更大的权重，更大的权重可以使模型更好的拟合</li>
</ul>
</li>
</ul>
<h2 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h2><h3 id="反向传播的作用、目的、本质"><a href="#反向传播的作用、目的、本质" class="headerlink" title="反向传播的作用、目的、本质"></a>反向传播的作用、目的、本质</h3><ul>
<li>概述：<strong>梯度下降法</strong>中需要利用损失函数对所有参数的梯度来寻找局部最小点，而反向传播算法就是用于计算梯度的算法，其本质就是利用链式求导法则对每个参数求偏导。</li>
</ul>
<h3 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h3><p><a href="https://blog.csdn.net/lien0906/article/details/79193103" target="_blank" rel="noopener">https://blog.csdn.net/lien0906/article/details/79193103</a></p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="激活函数的作用——为什么使用非线性激活函数"><a href="#激活函数的作用——为什么使用非线性激活函数" class="headerlink" title="激活函数的作用——为什么使用非线性激活函数"></a>激活函数的作用——为什么使用非线性激活函数</h3><p>使用非线性激活函数的目的是为了像网络中加入非线性因素；增强网络的表示能力，解决线性网络存在的问题。</p>
<p><strong>为什么加入非线性网络可以增强网络的表达能力呢——神经网络的万能近似定理</strong></p>
<ul>
<li>神经网络的万能近似定理认为震惊网络具有至少一个非线性隐含层，那么只要给网络足够数量的隐藏单元，他就可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的函数。</li>
<li>如果不使用非线性激活函数，那么每一层输出都是上一层输入的线性组合；此时无论网络有多少层，其整体也将是线性的，这会导致失去万能近似的性质。</li>
<li>但是部分层是纯线性是可以接受的，有助于减少网络的参数。</li>
</ul>
<h3 id="常见的激活函数"><a href="#常见的激活函数" class="headerlink" title="常见的激活函数"></a>常见的激活函数</h3><h4 id="整流线性单元ReLU"><a href="#整流线性单元ReLU" class="headerlink" title="整流线性单元ReLU"></a>整流线性单元ReLU</h4><ul>
<li>ReLU通常是激活函数最好的默认选择</li>
</ul>
<p><img src="https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180608212808.png" alt="img"><a href="http://www.codecogs.com/eqnedit.php?latex=g(z" target="_blank" rel="noopener"><img src="https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180610213451.png" alt="img"></a>=\max(0,z))</p>
<p><strong>ReLU的拓展</strong></p>
<ul>
<li><p>ReLU及其扩展都是基于以下公式：</p>
<p><img src="https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180610214123.png" alt="img"> </p>
</li>
<li><p><strong>绝对值整流</strong>（absolute value rectification） </p>
<p>当a=-1，此时函数就是g(z)=|z|</p>
</li>
<li><p>渗漏整流线性单元</p>
<p>当a是一个很小的值，如a=0.001</p>
</li>
<li><p>参数化线性整流</p>
<p>将a作为一个可学习的参数</p>
</li>
<li><p>maxout单元</p>
<p>maxout单元进一步扩展了ReLU，他是一个可学习的K段函数，参数数量是普通全连接层的 k 倍 。</p>
</li>
</ul>
<p>####sigmoid与tanh</p>
<ul>
<li><code>sigmoid(z)</code>，常记作 <code>σ(z)</code>； </li>
<li><code>tanh(z)</code> 的图像与 <code>sigmoid(z)</code> 大致相同，区别是<strong>值域</strong>为 <code>(-1, 1)</code> </li>
</ul>
<p><a href="http://www.codecogs.com/eqnedit.php?latex=\sigma(z" target="_blank" rel="noopener"><img src="https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180610214846.png" alt="img"></a>=\frac{1}{1+\exp(-z)}) </p>
<p><img src="https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180608195851.png" alt="img"></p>
<h4 id="其他激活函数"><a href="#其他激活函数" class="headerlink" title="其他激活函数"></a>其他激活函数</h4><h4 id="ReLU（优势）与sigmoid比较"><a href="#ReLU（优势）与sigmoid比较" class="headerlink" title="ReLU（优势）与sigmoid比较"></a>ReLU（优势）与sigmoid比较</h4><p><strong>1.避免梯度消失</strong></p>
<ul>
<li>sigmoid函数在输入取绝对值非常大的正值或者负值的时候会出现饱和现象——在图像上表现的很平，此时函数会对输入的微小变化不敏感——从而造成梯度消失</li>
<li>ReLU的导数始终是一个常数——负半区为0，正半区为1，——所以不会发生梯度消失</li>
</ul>
<p><strong>2.减缓过拟合</strong></p>
<ul>
<li>ReLU在负半区输出为0，一旦神经元的激活值进入负半轴，那么该机或值就不会产生梯度/不会被训练，造成了网络的稀疏性——稀疏激活。</li>
<li>有利于减少参数的相互依赖，缓解过拟合问题的发生。</li>
</ul>
<p><strong>3.加速计算</strong></p>
<ul>
<li>ReLU的求导不涉及浮点运算</li>
</ul>
<h4 id="为什么-ReLU-不是全程可微-可导也能用于基于梯度的学习？"><a href="#为什么-ReLU-不是全程可微-可导也能用于基于梯度的学习？" class="headerlink" title="为什么 ReLU 不是全程可微/可导也能用于基于梯度的学习？"></a><strong>为什么 ReLU 不是全程可微/可导也能用于基于梯度的学习？</strong></h4><ul>
<li>在实现过程中通常返回左导数或者右导数的其中一个。</li>
</ul>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><h3 id="Batch-Normalizaton-批标准化"><a href="#Batch-Normalizaton-批标准化" class="headerlink" title="Batch Normalizaton(批标准化)"></a>Batch Normalizaton(批标准化)</h3><ul>
<li><p>BN是一种正则化方法（较少泛化误差），主要的作用有：</p>
<ul>
<li>加速网络的训练（缓解梯度消失，支持更大的学习率）</li>
<li>防止过拟合</li>
<li>降低参数初始化的要求</li>
</ul>
</li>
<li><p>动机：</p>
<ul>
<li>训练的本质就是学习数据的分布。如果训练数据与测试数据分布不同会降低模型的泛化能力，因此应该在训练前对所有输入数据做归一化处理。</li>
<li>在神经网络中，因为每个隐层的参数不同，会使下一层的输入发生变化，从而导致每一批数据的分布也发生变化，从而导致网络在每次迭代中需要拟合不同的数据分布，增加了网络的训练难度与过拟合的风险。</li>
</ul>
</li>
<li><p>基本原理</p>
<ul>
<li><p>BN方法会正对每一批数据，在网络的每一层输入之前增加归一化处理，是输入的均值为0，标准差为1，目的是将数据限制在统一的分布下。</p>
</li>
<li><p>具体就是，针对每一层的K个神经元，计算这一批数据在低K个神经元的均值与标准差，然后将归一化的值作为该神经元的激活函值。</p>
<p><img src="https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180831165546.png" alt="img"> </p>
</li>
<li><p>BN可以看作在各层之间加入了一个新的计算层，对数据进行额外的约束，从而增加模型的泛化能力</p>
</li>
<li><p>但是BN也降低了模型的拟合能力，破坏了之前学习到的特征分布。</p>
</li>
<li><p>为了恢复原式的数据，BN引入了一个重构变化来还原最优的输入数据的分布</p>
<p><img src="https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180831165516.png" alt="img"> </p>
<p>其中 <code>γ</code> 和 <code>β</code> 为可训练参数。 </p>
</li>
<li><p>小结：</p>
<p>以上过程可归纳为一个 <strong>BN(x) 函数</strong>： </p>
<p><a href="http://www.codecogs.com/eqnedit.php?latex=\large&space;\boldsymbol{y_i}=\mathrm{BN}(\boldsymbol{x_i}" target="_blank" rel="noopener"><img src="https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180903223427.png" alt="img"></a>) </p>
<p>其中 </p>
<p><a href="http://www.codecogs.com/eqnedit.php?latex=\large&space;\begin{aligned}&space;\mathrm{BN}(\boldsymbol{x_i}" target="_blank" rel="noopener"><img src="https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180903224323.png" alt="img"></a>&amp;=\gamma\boldsymbol{\hat{x}_i}+\beta\&space;&amp;=\gamma\frac{\boldsymbol{x_i}-\boldsymbol{\mathrm{E}[x_i]}}{\sqrt{\boldsymbol{\mathrm{Var}[x_i]}+\epsilon}}+\beta&space;\end{aligned}) </p>
</li>
</ul>
</li>
<li><p>完整算法：</p>
<p><img src="https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180903222433.png" alt="img"> </p>
</li>
</ul>
<h3 id="L1-L2-范数正则化"><a href="#L1-L2-范数正则化" class="headerlink" title="L1/L2 范数正则化"></a>L1/L2 范数正则化</h3><h4 id="L1-L2范数的作用、异同"><a href="#L1-L2范数的作用、异同" class="headerlink" title="L1/L2范数的作用、异同"></a>L1/L2范数的作用、异同</h4><p><strong>相同点：</strong>限制了模型的学习能力——通过限制模型参数的规模，使模型偏好于权值较小的目标函数，防止过拟合。</p>
<p><strong>不同点：</strong></p>
<ul>
<li>L1正则化可以产生稀疏的权值举证，可以用于特征选择，同时一定程度上防止过拟合；L2正则化用于防止模型过拟合</li>
<li>L1正则化适用于特征之间有关联的情况；L2正则化适用于特征之间没有关联的情况。</li>
</ul>
<h4 id="为什么L1和L2正则化可以防止过拟合"><a href="#为什么L1和L2正则化可以防止过拟合" class="headerlink" title="为什么L1和L2正则化可以防止过拟合"></a>为什么L1和L2正则化可以防止过拟合</h4><ul>
<li>L1、L2正则化回事模型偏好于更小的权重</li>
<li>更小的权重意味着更低的模型复杂度，添加L1和L2正则化相当于为模型添加了某种先验，限制参数的分布，从而降低了模型的复杂度。</li>
<li>模型的复杂度低意味着模型对于噪声与异常点的抗干扰能力增强，从而提高了模型的泛化能力——直观来说就是对训练数据的拟合刚刚号，不会过分拟合数据——奥卡姆剃刀定理</li>
</ul>
<h4 id="为什么L1正则化可以产生稀疏的权值，L2不会？"><a href="#为什么L1正则化可以产生稀疏的权值，L2不会？" class="headerlink" title="为什么L1正则化可以产生稀疏的权值，L2不会？"></a>为什么L1正则化可以产生稀疏的权值，L2不会？</h4><ul>
<li><p>对于目标函数添加范数正则化，训练时相当于在范数的约束下求目标函数<code>J</code>的最小值</p>
</li>
<li><p>带有<strong>L1 范数</strong>（左）和<strong>L2 范数</strong>（右）约束的二维图示 </p>
<p><a href="https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/blob/master/_assets/TIM%E6%88%AA%E5%9B%BE20180608171710.png"><img src="https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180608171710.png" alt="img"></a> <a href="https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/blob/master/_assets/TIM%E6%88%AA%E5%9B%BE20180608172312.png"><img src="https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180608172312.png" alt="img"></a> </p>
<ul>
<li>图中 <code>J</code> 与 <code>L1</code> 首次相交的点即是最优解。<code>L1</code> 在和每个坐标轴相交的地方都会有“<strong>顶点</strong>”出现，多维的情况下，这些顶点会更多；在顶点的位置就会产生稀疏的解。而 <code>J</code> 与这些“顶点”相交的机会远大于其他点，因此 <code>L1</code> 正则化会产生稀疏的解。</li>
<li><code>L2</code> 不会产生“<strong>顶点</strong>”，因此 <code>J</code> 与 <code>L2</code> 相交的点具有稀疏性的概率就会变得非常小。</li>
</ul>
</li>
</ul>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><h4 id="Bagging集成学习"><a href="#Bagging集成学习" class="headerlink" title="Bagging集成学习"></a>Bagging集成学习</h4><ul>
<li><p>集成方法的主要想法是分别训练不同的模型，然后让所有的模型<strong>表决</strong>最终的输出。</p>
<p>集成学习凑效的原因是不同模型通常<strong>不会</strong>在测试集上产生相同的误差。</p>
<p>集成模型能至少与它的任何一成员 表现的一样好。如果成员的<strong>误差是独立</strong>的，集成将显著<strong>提升</strong>模型的性能。</p>
</li>
<li><p>Bagging是一种集成学习的策略——具体来说，Bagging涉及构造K个不同的数据集</p>
<p>每个数据集从原始数据集中重复采样构成，和原始数据集具有相同的样例，——这就意味着每个数据集有大概率缺少来自原始数据集的例子，但是也包含若干重复的例子</p>
<blockquote>
<p>更具体的就是，如果采样所得到的训练集与原式的数据集大小相同，那么所得到的数据集中大概有原始数据集<code>2/3</code>的实例。</p>
</blockquote>
</li>
</ul>
<h4 id="集成算法与神经网络"><a href="#集成算法与神经网络" class="headerlink" title="集成算法与神经网络"></a>集成算法与神经网络</h4><ul>
<li><p>神经网络能够找到足够多的不同的解，意味着他们可以从模型平均中受益——即使所有的模型都在同一数据集上训练。</p>
<p>神经网络中随机初始化的差异、批训练数据的随机选择、超参数的差异等非确定性实现往往足以使得继承中的不同成员有部分独立的误差。</p>
</li>
</ul>
<h4 id="Dropout策略"><a href="#Dropout策略" class="headerlink" title="Dropout策略"></a>Dropout策略</h4><ul>
<li><p>简单来说，Dropout是通过共享参数提供了一种廉价的Bagging集成近似——Drpout相当于继承了从基础网络去除部分单元后形成的子网络。</p>
</li>
<li><p>通常隐层的采样率为<code>0.5</code>,输入的采样率为<code>0.8</code>，超参数也可以这样，但其采样率一般为<code>1</code></p>
<p><img src="https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180611152559.png" alt="img"> </p>
</li>
</ul>
<h4 id="权重比例推断规则"><a href="#权重比例推断规则" class="headerlink" title="权重比例推断规则"></a>权重比例推断规则</h4><ul>
<li>权重比例推断规则的目的是确保在测试时一个单元的期望总输入与在训练时该单元的期望总输入大致相同。 </li>
<li>实践时，如果使用 <code>0.5</code> 的采样概率，<strong>权重比例规则</strong>相当于在训练结束后<strong>将权重乘 0.5</strong>，然后像平常一样使用模型；等价的，另一种方法是<strong>在训练时</strong>将单元的状态乘 2。</li>
</ul>
<h4 id="Dropout-与-Bagging-的不同"><a href="#Dropout-与-Bagging-的不同" class="headerlink" title="Dropout 与 Bagging 的不同"></a>Dropout 与 Bagging 的不同</h4><ul>
<li>在 Bagging 的情况下，所有模型都是独立的；而在 Dropout 的情况下，所有模型<strong>共享参数</strong>，其中每个模型继承父神经网络参数的不同子集。</li>
<li>在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。</li>
</ul>
</div></article></div></main><footer><div class="paginator"><a href="/Bingevan/2019/03/17/01快速排序/" class="prev">上一篇</a><a href="/Bingevan/2019/03/16/CBAM网络/" class="next">下一篇</a></div><div class="copyright"><p>© 2015 - 2019 <a href="http://github.com/Bingevan">范斌</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>